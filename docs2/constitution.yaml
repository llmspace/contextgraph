# Context Graph Constitution v7.0.0
# 13-Perspective Knowledge Graph | GPU-First | 52 CFs | 55 MCP Tools | Provenance-Audited

# ═══════════════════════════════════════════════════════════════
# METADATA
# ═══════════════════════════════════════════════════════════════
metadata:
  project: context-graph
  crate_version: "0.1.0"
  constitution_version: "7.0.0"
  rust_edition: "2021"
  msrv: "1.75"
  license: "MIT OR Apache-2.0"

# ═══════════════════════════════════════════════════════════════
# TECH STACK
# ═══════════════════════════════════════════════════════════════
tech_stack:
  language: { name: Rust, edition: 2021, msrv: "1.75" }
  storage: { name: RocksDB, crate: rust-rocksdb, column_families: 50 }
  gpu_framework: { name: Candle, version: "0.9.2-alpha", features: [cuda], org: HuggingFace }
  llm_runtime: { name: llama-cpp-2, version: "0.1", format: GGUF, grammar: GBNF }
  async_runtime: { name: Tokio, version: "1.35", features: [full] }
  web_framework: { name: Axum, version: "0.8", use: "SSE transport" }
  ast_parser: { name: tree-sitter, use: "Code entity extraction (Rust)" }
  hnsw_index: { name: hnsw_rs, version: "0.3", metrics: [cosine, l2, dot] }
  serialization:
    json: { crate: serde_json, version: "1.0", use: "Provenance, SourceMetadata, audit, profiles" }
    bincode: { crate: bincode, version: "1.3", use: "Dense vectors, binary indexes ONLY" }
  errors: { crate: thiserror, version: "1.0", pattern: "Unified ContextGraphError → JSON-RPC codes" }
  concurrency: { crate: parking_lot, pattern: "Non-poisoning RwLock/Mutex" }
  validation: { crate: schemars, version: "0.8", use: "MCP tool JSON schema" }
  ids: { crate: uuid, version: "1.6", format: v4 }

# ═══════════════════════════════════════════════════════════════
# WORKSPACE (10 crates)
# ═══════════════════════════════════════════════════════════════
workspace:
  context-graph-core: "Domain types, traits, HNSW indexes, similarity"
  context-graph-storage: "RocksDB backend, 52 CFs, teleological store"
  context-graph-mcp: "MCP JSON-RPC server, 55 tools, TCP transport"
  context-graph-embeddings: "13 embedder models, ONNX/Candle, quantization"
  context-graph-cuda: "CUDA ops, Poincare ball, HDBSCAN, cones"
  context-graph-graph: "Graph structures, traversal, edge types"
  context-graph-cli: "CLI interface"
  context-graph-benchmark: "Performance benchmarks"
  context-graph-causal-agent: "LLM causal discovery (Qwen2.5 + GBNF)"
  context-graph-graph-agent: "LLM graph relationship discovery"

# ═══════════════════════════════════════════════════════════════
# DIRECTORY STRUCTURE
# ═══════════════════════════════════════════════════════════════
# contextgraph/
#   crates/           10 workspace crates (ALL source code lives here)
#   models/           Downloaded model weights (ONNX, GGUF)
#   config/           Configuration files
#   tests/            Integration tests, chaos tests
#   tests/fixtures/   Test data
#   docs/             Documentation, specs
#   scripts/          Utility scripts
#   examples/         Example code
#   benchmarks/       Benchmark configs and datasets

# ═══════════════════════════════════════════════════════════════
# CODING STANDARDS
# ═══════════════════════════════════════════════════════════════
coding_standards:
  naming:
    functions: snake_case            # create_hnsw_index, search_multi_space_sync
    types: PascalCase                # TeleologicalFingerprint, SourceMetadata
    constants: SCREAMING_SNAKE_CASE  # CF_FINGERPRINTS, TOTAL_COLUMN_FAMILIES
    modules: snake_case              # teleological, rocksdb_store, hnsw_impl
    cf_strings: lowercase_underscore # "fingerprints", "source_metadata"

  file_organization:
    - "One primary type per module file"
    - "Co-locate unit tests via #[cfg(test)] mod tests"
    - "Integration tests in crate tests/ directory"
    - "Chaos tests in tests/chaos_tests/"
    - "Keep files under 500 lines"

  error_handling:
    - "thiserror #[derive(Error)] for ALL error types"
    - "Unified ContextGraphError with From<> conversions for sub-errors"
    - "map_err() over .expect() — never panic in library code"
    - "JSON-RPC error codes: -32001 to -32009 for domain-specific errors"
    - "is_recoverable() / is_critical() classification on all errors"
    - "FAIL FAST — no silent degradation"

  serialization:
    rule: "JSON for provenance/metadata/audit. Bincode for dense vectors/indexes ONLY."
    critical: "bincode + skip_serializing_if = SILENT CORRUPTION of optional fields"
    json_cfs: [source_metadata, audit_log, merge_history, importance_history, topic_portfolio, embedding_registry, custom_weight_profiles]
    bincode_cfs: [e12_late_interaction, file_index, tool_call_index, consolidation_recommendations, "code_*"]

  concurrency:
    - "parking_lot::RwLock/Mutex — non-poisoning, ~5-10x faster than std::sync"
    - "RwLockReadGuard is !Send — MUST drop before .await in async contexts"
    - "Pattern: let data = guard.something().clone(); drop(guard); async_work(data).await"
    - "Consistent lock ordering to prevent ABBA deadlocks"
    - "JoinHandle must be awaited or aborted — never silently dropped"

# ═══════════════════════════════════════════════════════════════
# SECURITY
# ═══════════════════════════════════════════════════════════════
security:
  SEC-01: "Validate all MCP tool params at handler boundary (JSON schema + custom validation)"
  SEC-02: "Sanitize file paths — prevent directory traversal in file watcher and code indexer"
  SEC-03: "CUDA bounds checking — validate embedding dimensions before GPU kernel launch"
  SEC-04: "No secrets in code — env vars only (CONTEXT_GRAPH_STORAGE_PATH for DB path)"
  SEC-05: "PII detection at input boundary — regex for email, SSN, API keys"
  SEC-06: "Audit all destructive ops (delete, repair, reconcile) via append-only audit log"
  SEC-07: "Non-fatal audit writes — warn! on failure, never block main operation"

# ═══════════════════════════════════════════════════════════════
# TESTING
# ═══════════════════════════════════════════════════════════════
testing:
  golden_rule: "ALL MCP tests use real RocksDB + real GPU embeddings. NO STUBS."
  env_var: "CONTEXT_GRAPH_STORAGE_PATH controls test DB location"
  test_helper: "create_test_handlers() → async → (Handlers, TempDir) with RocksDB + warm GPU"
  counts:
    mcp: "~700 tests (real RocksDB + GPU)"
    core: "~2800 tests"
    storage: "~630 tests"
    chaos: "16 tests (concurrent mutation, GPU OOM, memory pressure)"
    integration: "8 end-to-end FSV tests"
  commands:
    build: "cargo build --release"
    test_all: "cargo test"
    test_release: "cargo test --release"
    test_crate: "cargo test -p context-graph-mcp"
    test_single: "cargo test -p context-graph-mcp test_name"
  rules:
    - "Run tests after every code change"
    - "Release build must succeed before committing"
    - "Tests ship with implementation, not as separate tasks"
    - "No #[ignore] without a documented reason"

# ═══════════════════════════════════════════════════════════════
# CORE PHILOSOPHY (5 principles)
# ═══════════════════════════════════════════════════════════════
# 1. 13 embedders = 13 unique lenses on every memory. Combined > any single embedder.
# 2. Every embedder provides SIGNAL, never noise. What E1 misses is additional signal.
# 3. E1 (semantic) is foundation; enhancers complement E1, never compete with it.
# 4. AI receives FULL VISIBILITY into all 13 scores for intelligent navigation.
# 5. GPU-first: all 13 embedders warm-loaded into VRAM at startup. No CPU fallback.

# ═══════════════════════════════════════════════════════════════
# EMBEDDERS (13 Perspectives) — ALL GPU-RESIDENT
# ═══════════════════════════════════════════════════════════════
embedders:
  # FOUNDATION (topic_weight: 1.0)
  E1:
    name: V_meaning
    dim: 1024
    topic_wt: 1.0
    finds: "Semantic similarity"
    when_strong: "Clear semantic content matching stored memories"
    when_weak: "Code syntax, entity names, causal relationships"
    threshold: { high: 0.75, low: 0.30 }
    matryoshka: [256, 512, 1024]

  # SEMANTIC ENHANCERS (topic_weight: 1.0)
  E5:
    name: V_causality
    dim: 768
    topic_wt: 1.0
    finds: "Causal chains (cause→effect)"
    when_strong: "'why' / 'what caused' queries"
    when_weak: "Descriptive/definitional, no causal chain"
    asymmetric: true
    direction_mod: { cause_to_effect: 1.2, effect_to_cause: 0.8 }
    threshold: { high: 0.70, low: 0.25 }
  E6:
    name: V_selectivity
    sparse: true
    topic_wt: 1.0
    finds: "Exact keyword matches"
    when_strong: "Quoted terms, error codes, jargon"
    when_weak: "Conceptual or paraphrased queries"
    blend_weight: { default: 0.3, range: [0.1, 0.5] }
    jaccard_threshold: { high: 0.60, low: 0.20 }
  E7:
    name: V_correctness
    dim: 1536
    topic_wt: 1.0
    finds: "Code patterns, function signatures"
    when_strong: "Code syntax (::, ->, fn, impl, .await)"
    when_weak: "Natural language without code"
    blend_weight: { default: 0.4, range: [0.2, 0.6] }
    threshold: { high: 0.80, low: 0.35 }
    benchmark: "+69% signature, +29% pattern vs E1-only"
  E10:
    name: V_multimodality
    dim: 768
    topic_wt: 1.0
    finds: "Same-goal work (different words)"
    when_strong: "Goal/purpose queries, paraphrased intent"
    when_weak: "Exact match, no interpretation needed"
    integration: "Multiplicative boost: E1 * (1 + boost), NOT linear blend"
    boost: { strong_e1: 0.05, medium_e1: 0.10, weak_e1: 0.15 }
    clamp: [0.8, 1.2]
  E12:
    name: V_precision
    per_token: true
    topic_wt: 1.0
    finds: "Exact phrase matches (MaxSim)"
    use: "Reranking ONLY — NEVER initial retrieval (AP-74)"
    maxsim_threshold: { high: 0.70, low: 0.30 }
  E13:
    name: V_keyword_precision
    sparse: true
    topic_wt: 1.0
    finds: "Term expansions (fast→quick, db→database)"
    use: "Stage 1 recall ONLY — NEVER final ranking (AP-75)"
    splade_threshold: { high: 0.60, low: 0.20 }
    recall_candidates: 10000

  # RELATIONAL ENHANCERS (topic_weight: 0.5)
  E8:
    name: V_connectivity
    dim: 1024
    topic_wt: 0.5
    finds: "Graph structure (imports, dependencies)"
    when_strong: "'what uses X?' / 'depends on' queries"
    when_weak: "Content queries, not structural"
    model: "e5-large-v2"
    asymmetric: true
    direction_mod: { source_to_target: 1.2, target_to_source: 0.8 }
  E11:
    name: V_factuality
    dim: 768
    topic_wt: 0.5
    finds: "Entity knowledge (Diesel=database ORM)"
    when_strong: "Named entities (frameworks, libraries, tools)"
    when_weak: "Conceptual queries without entity names"
    model: "KEPLER (RoBERTa-base + TransE)"
    transe_threshold: { valid: "> -2.0", uncertain: "-2.0 to -5.0" }

  # TEMPORAL (topic_weight: 0.0 — POST-RETRIEVAL ONLY per ARCH-25)
  E2: { name: V_freshness, dim: 512, topic_wt: 0.0, finds: "Recency", post_retrieval_only: true }
  E3: { name: V_periodicity, dim: 512, topic_wt: 0.0, finds: "Time-of-day patterns", post_retrieval_only: true }
  E4: { name: V_ordering, dim: 512, topic_wt: 0.0, finds: "Sequence (before/after)", post_retrieval_only: true }

  # STRUCTURAL (topic_weight: 0.5)
  E9:
    name: V_robustness
    dim: 1024
    topic_wt: 0.5
    finds: "Noise-robust structure (typos, variations)"
    when_strong: "Queries with typos, misspellings, noise"
    when_weak: "Clean, well-formed text"
    hamming_threshold: { high: 0.70 }

  temporal_recency_boost: { under_1h: 1.3, under_1d: 1.2, under_7d: 1.1 }

# ═══════════════════════════════════════════════════════════════
# CUSTOM WEIGHT RULES
# ═══════════════════════════════════════════════════════════════
custom_weights:
  validation: { sum_tolerance: 0.01, per_weight: [0.0, 1.0], validator: "validate_weights() in weights/mod.rs" }
  precedence: "customWeights > weightProfile > DEFAULT_SEMANTIC_WEIGHTS"
  exclusion: "Zero excluded weights, renormalize remaining. All-excluded = error. Invalid names = fail fast."
  E12_E13: "Weights apply in Pipeline only. In MultiSpace, silently treated as 0."
  E2_E3_E4: "Weights allowed. High values reduce topical relevance per AP-73."
  profiles_14: [semantic_search, causal_reasoning, code_search, fact_checking,
    graph_reasoning, temporal_navigation, sequence_navigation,
    conversation_history, category_weighted, typo_tolerant, pipeline_stage1_recall,
    pipeline_stage2_scoring, pipeline_full, balanced]

# ═══════════════════════════════════════════════════════════════
# ARCHITECTURE RULES (MUST follow)
# ═══════════════════════════════════════════════════════════════
arch_rules:
  # GPU-First
  ARCH-GPU-01: "GPU mandatory for production — no CPU fallback for embeddings"
  ARCH-GPU-02: "All 13 embedders warm-loaded into VRAM at MCP server startup"
  ARCH-GPU-03: "Embedding inference uses FP16/BF16 Tensor Cores"
  ARCH-GPU-04: "HNSW indexes via hnsw_rs (cosine/L2/dot metrics)"
  ARCH-GPU-05: "Clustering via context-graph-cuda crate (HDBSCAN)"
  ARCH-GPU-06: "Batch operations preferred — minimize kernel launches"
  ARCH-GPU-07: "[TARGET] Green Contexts partition SMs: 70% inference, 30% indexing"
  ARCH-GPU-08: "[TARGET] CUDA streams for async embedding + indexing overlap"

  # Signal Philosophy
  ARCH-SIGNAL-01: "ALL 13 embedders provide SIGNAL, never noise"
  ARCH-SIGNAL-02: "What E1 misses is ADDITIONAL signal — E7/E11/E5 capture what E1 cannot"
  ARCH-SIGNAL-03: "Parameter tuning defines signal MEANING"
  ARCH-SIGNAL-04: "Combined signal > any single embedder"
  ARCH-SIGNAL-05: "Enhancers complement E1, never compete"

  # Multi-Perspective Navigation (AI Visibility)
  ARCH-NAV-01: "AI sees all 13 embedder scores per result"
  ARCH-NAV-02: "Scores categorized: SEMANTIC, RELATIONAL, STRUCTURAL, TEMPORAL"
  ARCH-NAV-03: "Blind spot alert: enhancer >= 0.5 AND E1 < 0.3"
  ARCH-NAV-04: "Navigation hints guide AI to specialized tools by score patterns"
  ARCH-NAV-05: "Agreement metrics show embedder consensus"
  ARCH-NAV-06: "customWeights: [f32;13], validate_weights() enforces sum ~1.0, each [0.0,1.0]"
  ARCH-NAV-07: "excludeEmbedders: zero + renormalize. All-excluded = error"
  ARCH-NAV-08: "includeEmbedderBreakdown: per-embedder {score, rank, weight, rrfContribution}"

  # Core
  ARCH-01: "TeleologicalArray is atomic — all 13 embeddings or nothing"
  ARCH-02: "Apples-to-apples only — compare E1↔E1, never E1↔E5"
  ARCH-04: "Temporal (E2-E4) NEVER count toward topics"
  ARCH-05: "All 13 embedders required — missing = fatal"
  ARCH-06: "All memory ops through MCP tools only"
  ARCH-09: "Topic threshold: weighted_agreement >= 2.5"
  ARCH-10: "Divergence detection: SEMANTIC embedders only (E1,E5,E6,E7,E10,E12,E13)"

  # Retrieval Pipeline
  ARCH-12: "E1 is foundation — all retrieval starts with E1"
  ARCH-13: "Strategies: E1Only, MultiSpace (E1+E5+E7+E8+E10+E11 via RRF), Pipeline (E13→E1→E12), EmbedderFirst"
  ARCH-17: "Strong E1 (>0.8): enhancers refine. Weak E1 (<0.4): enhancers broaden"
  ARCH-18: "E5 Causal: asymmetric similarity (cause→effect direction matters)"
  ARCH-21: "Multi-space fusion: Weighted RRF, not weighted sum"
  ARCH-25: "Temporal boosts POST-retrieval only, NOT in similarity fusion"

  # E10 Multiplicative Boost
  ARCH-28: "E10 uses multiplicative boost: E1 * (1 + boost), NOT linear blending"
  ARCH-29: "E10 boost adapts: strong E1=5%, medium=10%, weak=15%"
  ARCH-30: "REMOVED — E10 alignment was non-functional (hardcoded to 1.0)"
  ARCH-33: "E10 multiplier clamped to [0.8, 1.2]"

  # Enrichment
  ARCH-ENRICH-01: "Query type detection before search"
  ARCH-ENRICH-02: "E1 always runs first (per ARCH-12)"
  ARCH-ENRICH-03: "Enhancers run in parallel via tokio::join!"
  ARCH-ENRICH-04: "Fusion uses Weighted RRF (per ARCH-21)"
  ARCH-ENRICH-05: "Blind spots: enhancer > 0.5 AND E1 < 0.3"
  ARCH-ENRICH-06: "Light mode: max 2 enhancers, Full: max 6"

  # Search Transparency
  ARCH-TRANS-01: "searchTransparency MUST be in every search_graph response"
  ARCH-TRANS-02: "activeEmbedders reflects ACTUAL embedders searched"
  ARCH-TRANS-03: "ignoredWeights explains WHY weights were unused (strategy limitation)"

  # Code Embedding
  ARCH-CODE-01: "Code entities stored separately from teleological memories"
  ARCH-CODE-02: "E7 is primary for code, not part of 13-embedder array for code pipeline"
  ARCH-CODE-03: "AST chunking preserves syntactic boundaries (functions, structs)"
  ARCH-CODE-04: "Code file watcher uses git-based change detection"
  ARCH-CODE-05: "Code search blends E7 (code) with E1 (semantic) for best results"
  ARCH-CODE-06: "tree-sitter for AST parsing, NOT regex-based extraction"

  # Provenance
  ARCH-PROV-01: "Audit writes non-fatal — warn! on failure, never block main operation"
  ARCH-PROV-02: "AuditRecord target_id = Uuid::nil() for operations on CFs/files"
  ARCH-PROV-03: "Merge history is permanent — no TTL, no expiration"
  ARCH-PROV-04: "All destructive ops (delete, repair, reconcile) MUST emit audit records"
  ARCH-PROV-05: "get_provenance_chain queries all 5 queryable CFs in a single response"

  # LLM Agents
  ARCH-LLM-01: "LLM inference local-only via llama.cpp — no external API calls"
  ARCH-LLM-02: "All LLM output constrained by GBNF grammar for structured JSON"
  ARCH-LLM-03: "LLM results include source memory provenance links"

  # Serialization (critical — learned from forensic audit)
  ARCH-SER-01: "JSON for provenance/SourceMetadata/audit. Bincode for dense vectors only."
  ARCH-SER-02: "NEVER use bincode with skip_serializing_if — silent corruption"

  # Concurrency (critical — learned from forensic audit)
  ARCH-CONC-01: "parking_lot RwLock/Mutex, not std::sync"
  ARCH-CONC-02: "Drop RwLockReadGuard before .await — it is !Send"
  ARCH-CONC-03: "Consistent lock ordering to prevent ABBA deadlocks"

# ═══════════════════════════════════════════════════════════════
# ANTI-PATTERNS (FORBIDDEN)
# ═══════════════════════════════════════════════════════════════
forbidden:
  # GPU
  AP-GPU-01: "NEVER CPU for embedding inference when GPU available"
  AP-GPU-02: "NEVER cold-load embedders per-request — warm-load at startup"
  AP-GPU-05: "NEVER transfer embeddings GPU→CPU→GPU — keep on device"
  AP-GPU-06: "NEVER FP32 for inference — use FP16/BF16 Tensor Cores"
  AP-GPU-07: "NEVER serialize embeddings per-item — batch for GPU efficiency"
  AP-GPU-08: "NEVER block on sync — use async operations"

  # Core
  AP-02: "No cross-embedder comparison (E1↔E5)"
  AP-04: "No partial TeleologicalArray"
  AP-05: "No embedding fusion into single vector"
  AP-60: "Temporal (E2-E4) MUST NOT count toward topics"
  AP-73: "Temporal MUST NOT be in similarity fusion"
  AP-74: "E12 ColBERT: reranking ONLY, not initial retrieval"
  AP-75: "E13 SPLADE: Stage 1 recall ONLY, not final ranking"
  AP-77: "E5 MUST NOT use symmetric cosine — causal is directional"
  AP-79: "MUST NOT use simple weighted sum — use Weighted RRF"
  AP-80: "E10 MUST NOT use linear blending — makes E10 compete with E1"
  AP-84: "E10 MUST NOT override E1 — when E1=0, result=0"

  # Navigation
  AP-NAV-01: "NEVER silently drop invalid embedder names — fail fast with error"
  AP-NAV-02: "NEVER allow customWeights not summing to ~1.0"
  AP-NAV-03: "NEVER exclude all 13 embedders"

  # Code
  AP-CODE-01: "NEVER chunk code by word count — use AST boundaries"
  AP-CODE-02: "NEVER store code in teleological memory graph"
  AP-CODE-03: "NEVER use E1 alone for code search — E7 is primary"
  AP-CODE-04: "NEVER parse code without tree-sitter — no regex extraction"

# ═══════════════════════════════════════════════════════════════
# COLUMN FAMILIES (50 total)
# ═══════════════════════════════════════════════════════════════
# Shared: 256MB LRU block cache, LZ4 compression (most CFs), bloom 10-bit
column_families:
  total: 50  # 11 base + 19 teleological + 13 quantized + 5 code + 2 causal

  base_11:
    nodes: "Memory nodes, UUID prefix key"
    edges: "Graph edges, source_uuid key"
    embeddings: "1536D f32 vectors, 64KB blocks"
    metadata: "Memory metadata, 256MB cache"
    temporal: "timestamp_ms:UUID index"
    tags: "tag:UUID index"
    sources: "source_uri:UUID index"
    system: "System config, no compression"
    embedder_edges: "[u8 embedder][16B uuid] = 17-byte key"
    typed_edges: "[16B src][16B tgt] = 32-byte key"
    typed_edges_by_type: "[u8 type][16B src] = 17-byte key"

  teleological_19:
    # Fingerprints & Indexes (5)
    fingerprints: "~63KB per fingerprint (all 13 embeddings), 64KB blocks"
    topic_profiles: "13D vectors, 52 bytes, no compression"
    e13_splade_inverted: "SPLADE posting lists (term_id→Vec<Uuid>)"
    e6_sparse_inverted: "E6 keyword inverted index"
    e1_matryoshka_128: "Truncated E1 128D vectors for fast ANN"
    # Content & Metadata (4)
    content: "Raw text, LZ4 compressed, up to 1MB"
    source_metadata: "Provenance info, JSON, ~100-500B"
    file_index: "file_path→FileIndexEntry"
    topic_portfolio: "Persisted topic clusters, JSON"
    # E12 & Entity (2)
    e12_late_interaction: "Token embeddings, 10-25KB per memory"
    entity_provenance: "Entity→fingerprint links"
    # Audit & Lifecycle (4)
    audit_log: "Append-only, [timestamp_nanos_be][uuid] key, JSON value"
    audit_by_target: "Secondary index: target UUID → audit keys"
    merge_history: "Permanent merge records, JSON (no TTL)"
    importance_history: "Importance change log, JSON"
    # Tool / Consolidation / Embedding (3)
    tool_call_index: "tool_use_id→Vec<Uuid>"
    consolidation_recommendations: "Analysis results"
    embedding_registry: "Model version tracking per fingerprint"
    # Profiles (1)
    custom_weight_profiles: "[f32;13] profiles, JSON"

  quantized_13:
    # One CF per embedder (emb_0 through emb_12), quantized for fast ANN
    emb_0: "E1 Semantic (1024D, PQ-8)"
    emb_1: "E2 TemporalRecent (512D, Float8E4M3)"
    emb_2: "E3 TemporalPeriodic (512D, Float8E4M3)"
    emb_3: "E4 TemporalPositional (512D, Float8E4M3)"
    emb_4: "E5 Causal (768D, PQ-8)"
    emb_5: "E6 Sparse (variable)"
    emb_6: "E7 Code (1536D, PQ-8)"
    emb_7: "E8 Graph (1024D, Float8E4M3)"
    emb_8: "E9 HDC (10000D binary)"
    emb_9: "E10 Paraphrase (768D, PQ-8)"
    emb_10: "E11 Entity (768D, Float8E4M3)"
    emb_11: "E12 LateInteraction (variable, token pruning)"
    emb_12: "E13 SPLADE (sparse)"

  code_5:
    code_entities: "AST-extracted functions/structs/traits metadata"
    code_e7_embeddings: "1536D E7 (Qodo-Embed) vectors for code"
    code_file_index: "file_path→entity ID mappings"
    code_name_index: "entity_name→Vec<Uuid>"
    code_signature_index: "signature_hash→Vec<Uuid>"

  causal_2:
    causal_relationships: "LLM-generated descriptions + E1 embeddings"
    causal_by_source: "Secondary index by source fingerprint"

# ═══════════════════════════════════════════════════════════════
# MCP TOOLS (55 total, 17 categories)
# ═══════════════════════════════════════════════════════════════
# Tool names match constants in tools/names.rs and definitions in tools/definitions/.
mcp_tools:
  core_4:              [store_memory, get_memetic_status, search_graph, trigger_consolidation]
  curation_3:          [merge_concepts, forget_concept, boost_importance]
  topic_4:             [get_topic_portfolio, get_topic_stability, detect_topics, get_divergence_alerts]
  file_watcher_4:      [list_watched_files, get_file_watcher_stats, delete_file_content, reconcile_files]
  sequence_4:          [get_conversation_context, get_session_timeline, traverse_memory_chain, compare_session_states]
  causal_4:            [search_causes, search_effects, get_causal_chain, search_causal_relationships]
  causal_discovery_2:  [trigger_causal_discovery, get_causal_discovery_status]
  keyword_1:           [search_by_keywords]
  code_1:              [search_code]
  graph_4:             [search_connections, get_graph_path, discover_graph_relationships, validate_graph_link]
  robustness_1:        [search_robust]
  entity_6:            [extract_entities, search_by_entities, infer_relationship, find_related_entities, validate_knowledge, get_entity_graph]
  embedder_search_7:   [search_by_embedder, get_embedder_clusters, compare_embedder_views, list_embedder_indexes, get_memory_fingerprint, create_weight_profile, search_cross_embedder_anomalies]
  temporal_2:          [search_recent, search_periodic]
  graph_linking_4:     [get_memory_neighbors, get_typed_edges, traverse_graph, get_unified_neighbors]
  maintenance_1:       [repair_causal_relationships]
  provenance_3:        [get_audit_trail, get_merge_history, get_provenance_chain]

  search_strategies:
    E1Only: "Fast simple semantic (default)"
    MultiSpace: "E1+E5+E7+E8+E10+E11 via RRF (6 embedders)"
    Pipeline: "E13 recall → E1 dense → E12 rerank (max precision)"
    EmbedderFirst: "Any embedder as primary perspective"
    # AdaptiveSearch removed — was non-functional (detected intent but never applied boost)

# ═══════════════════════════════════════════════════════════════
# SEARCH & NAVIGATION
# ═══════════════════════════════════════════════════════════════
search:
  # searchTransparency block (always present in search_graph responses)
  transparency:
    fields: [activeEmbedders, ignoredWeights, weightUtilization, strategyDescription]

  # Full retrieval pipeline stages
  pipeline:
    S1: "E13 SPLADE sparse recall → 10K candidates"
    S2: "E1 Matryoshka ANN → 1K candidates"
    S3: "RRF across semantic spaces → 100 candidates"
    S4: "Score-based filtering → 50 candidates"
    S5: "E12 MaxSim rerank → 10 final results"

  # Dynamic navigation params
  navigation:
    customWeights: { tools: [search_graph, get_unified_neighbors], format: '{"E1": 0.4, "E5": 0.3, "E7": 0.2, "E11": 0.1}' }
    excludeEmbedders: { tools: [search_graph, get_unified_neighbors], behavior: "Zero + renormalize remaining" }
    includeEmbedderBreakdown: { tools: [search_graph], returns: [embedderBreakdown, dominantEmbedder, agreementLevel] }

  # Auto query type detection → embedder selection
  query_type_detection:
    CAUSAL: { patterns: ["why", "caused", "because", "root cause"], enhancers: [E5, E8] }
    CODE: { patterns: ["::", "->", "fn ", "impl", ".await"], enhancers: [E7, E6] }
    ENTITY: { patterns: ["what is", "works with", capitalized_names], enhancers: [E11, E6] }
    KEYWORD: { patterns: [quoted_terms, "error:", "v2."], enhancers: [E6, E13] }
    TEMPORAL: { patterns: ["yesterday", "recently"], enhancers: "POST-RETRIEVAL only (ARCH-25)" }

  enrichment_modes:
    Off: "E1-only (legacy)"
    Light: "E1 + 1-2 enhancers, basic metrics (default, <500ms)"
    Full: "All relevant enhancers, full metrics, blind spots (<800ms)"

  # Embedder perspectives (what each finds as primary)
  perspectives:
    E1: "Dense semantic similarity"
    E2: "Temporal freshness (recent first)"
    E3: "Time-of-day patterns"
    E4: "Conversation order (before/after)"
    E5: "Cause-effect relationships"
    E6: "Exact keyword matches"
    E7: "Code patterns, AST structure"
    E8: "Structural relationships (imports, deps)"
    E9: "Noise-robust structure (typos)"
    E10: "Paraphrase detection (different words, same meaning)"
    E11: "Entity knowledge (named entities)"
    E12: "Exact phrase matches (MaxSim rerank)"
    E13: "Term expansion (SPLADE synonyms)"

# ═══════════════════════════════════════════════════════════════
# TOPIC SYSTEM
# ═══════════════════════════════════════════════════════════════
topics:
  weighted_agreement:
    formula: "Sum(topic_weight_i * is_clustered_i)"
    threshold: 2.5
    max: 8.5  # 7*1.0 (semantic) + 2*0.5 (relational) + 1*0.5 (structural)
  categories:
    SEMANTIC: { embedders: [E1,E5,E6,E7,E10,E12,E13], weight: 1.0 }
    RELATIONAL: { embedders: [E8,E11], weight: 0.5 }
    STRUCTURAL: { embedders: [E9], weight: 0.5 }
    TEMPORAL: { embedders: [E2,E3,E4], weight: 0.0 }
  divergence_detection: "SEMANTIC embedders only"

# ═══════════════════════════════════════════════════════════════
# PROVENANCE
# ═══════════════════════════════════════════════════════════════
provenance:
  rule: "Every mutation, search, and discovery audited. Append-only, non-blocking."
  cfs: [audit_log, audit_by_target, merge_history, importance_history, entity_provenance, tool_call_index, consolidation_recommendations, embedding_registry]
  operations: [MemoryCreated, MemoryDeleted, MemoryMerged, ImportanceChanged, SearchPerformed, ConsolidationAnalyzed, RelationshipDiscovered, CausalRelationshipRepaired, FileWatcherEvent]
  builder: "AuditRecord::new(op, target_id).with_operator().with_session().with_rationale().with_parameters()"
  target_id_nil: "Use Uuid::nil() for operations on CFs/files, not single entities"
  tools: [get_audit_trail, get_merge_history, get_provenance_chain]

# ═══════════════════════════════════════════════════════════════
# CODE EMBEDDING (Separate Pipeline)
# ═══════════════════════════════════════════════════════════════
# Code stored separately from teleological memories. E7 (Qodo-Embed) primary.
code_embedding:
  model: { name: "Qodo-Embed-1-1.5B", dim: 1536, precision: FP16, path: "models/code-1536" }
  ast:
    parser: tree-sitter
    target_size: 500  # non-whitespace chars per chunk
    range: [100, 1000]
    includes: [parent_context, imports]
    languages: { rust: { ext: ".rs", grammar: "tree-sitter-rust" } }
  entities: [Function, Method, Struct, Enum, Trait, Impl, Module, Const, Static, TypeAlias]
  search:
    primary: E7
    e1_blend: { default: 0.4, range: [0.2, 0.6] }
    modes: [signature, pattern, semantic]
  file_watcher:
    trigger: "git diff --name-only"
    on_change: "Parse AST → extract entities → E7 embed → store → update indexes"
    on_delete: "Lookup by path → delete entities → remove from indexes"

# ═══════════════════════════════════════════════════════════════
# LLM AGENTS
# ═══════════════════════════════════════════════════════════════
llm_agents:
  runtime: "llama-cpp-2 (llama.cpp Rust bindings)"
  model: "Qwen2.5 (GGUF format)"
  grammar: "GBNF constraints for structured JSON"
  causal_agent:
    crate: context-graph-causal-agent
    tools: [trigger_causal_discovery, get_causal_discovery_status]
    output: "CausalRelationship → CF_CAUSAL_RELATIONSHIPS"
  graph_agent:
    crate: context-graph-graph-agent
    tools: [discover_graph_relationships, validate_graph_link]
    output: "GraphEdge with LLM-validated relationship types"

# ═══════════════════════════════════════════════════════════════
# MEMORY SOURCES
# ═══════════════════════════════════════════════════════════════
memory_sources:
  HookDescription: "Claude's description of tool use"
  ClaudeResponse: "Session summaries, significant responses"
  MDFileChunk: "Markdown chunks (200 words, 50 overlap)"
  CodeEntity: "AST-parsed code entities (functions, structs, traits, impls)"

# ═══════════════════════════════════════════════════════════════
# DREAM CONSOLIDATION
# ═══════════════════════════════════════════════════════════════
dream:
  trigger: "entropy > 0.7 AND churn > 0.5"
  nrem: "Hebbian replay — strengthen high-importance edges"
  rem: "Hyperbolic random walk — discover blind spots in Poincare ball"

# ═══════════════════════════════════════════════════════════════
# THRESHOLDS
# ═══════════════════════════════════════════════════════════════
thresholds:
  topic: 2.5
  high_sim: 0.75
  low_sim: 0.30
  duplicate: 0.90
  entropy_dream: 0.70
  churn_dream: 0.50

# ═══════════════════════════════════════════════════════════════
# HOOKS (Native Claude Code)
# ═══════════════════════════════════════════════════════════════
hooks:
  config: ".claude/settings.json"
  SessionStart: "Load topic portfolio, warm GPU indexes"
  UserPromptSubmit: "GPU embed → search → detect divergence → inject context"
  PreToolUse: "Inject brief relevant context"
  PostToolUse: "Capture + GPU embed as HookDescription"
  Stop: "Capture response summary"
  SessionEnd: "Persist state, HDBSCAN clustering, check dream triggers"

# ═══════════════════════════════════════════════════════════════
# GPU HARDWARE TARGETS
# ═══════════════════════════════════════════════════════════════
# CURRENT IMPL: Candle (HuggingFace) + CUDA, hnsw_rs, context-graph-cuda HDBSCAN
# TARGET SPEC: CUDA 13.1 Green Contexts, CUDA Tile, CUDA Streams
gpu:
  hardware: { device: "RTX 5090", arch: "Blackwell GB202", vram: "32GB GDDR7", bandwidth: "1792 GB/s", tensor_cores: 680, sms: 170 }
  host: { cpu: "Ryzen 9 9950X3D", cores: "16C/32T", ram: "128GB DDR5", l3_cache: "192MB" }

  vram_budget:
    embedders: "~10GB (13 warm-loaded)"
    code_e7: "~3GB (Qodo-Embed, separate)"
    hnsw_indexes: "~8GB"
    batch_buffers: "~4GB"
    clustering: "~2GB"
    reserved: "~5GB headroom"
    total: "32GB"

  model_slot_management:
    budget: "8GB for 13 teleological models"
    e7_separate: "~3GB dedicated (not in 8GB budget)"
    eviction: "LRU when VRAM > 95%"
    pressure: { normal: "<80%", high: "80-95%", critical: ">95% (LRU eviction)" }

  performance_targets:
    all_13_embed: "<200ms (GPU batch)"
    per_space_hnsw: "<1ms"
    inject_context_p95: "<500ms"
    store_memory_p95: "<800ms"
    any_tool_p99: "<1s"
    topic_detection: "<20ms"
    warm_load_startup: "<30s"
