# Context Graph Constitution v6.4.0
# 13-Perspectives Collaboration | E1 Foundation | GPU-First Architecture
# ═══════════════════════════════════════════════════════════════

# GPU-FIRST ARCHITECTURE
# This is a GPU-first system. All compute-intensive operations use GPU over CPU.
# All 13 embedders are warm-loaded into GPU memory at MCP server startup.
# Target Hardware: RTX 5090 32GB (Blackwell GB202) + CUDA 13.1
#
# Key GPU Capabilities:
#   - 32GB GDDR7 VRAM: All 13 embedder models resident simultaneously
#   - 1,792 GB/s bandwidth: Sub-millisecond embedding generation
#   - 680 5th-gen Tensor Cores: FP16/BF16/FP8 inference
#   - 170 SMs with Green Contexts: Isolated inference partitions
#   - CUDA Tile: Portable, auto-optimized kernels

# CORE PHILOSOPHY: 13 embedders = 13 unique perspectives on every memory
# Each finds what OTHERS MISS. Combined = superior answers.
#
# Example: Query "What databases work with Rust?"
# - E1 finds: "database" or "Rust" semantically
# - E11 finds: "Diesel" (knows Diesel IS a database ORM - E1 missed this)
# - E7 finds: code using sqlx, diesel crates
# - Combined: Better answer than any single embedder

# ═══════════════════════════════════════════════════════════════
# EMBEDDERS (13 Perspectives) - ALL GPU-RESIDENT
# ═══════════════════════════════════════════════════════════════
# All 13 embedders are warm-loaded into GPU VRAM at startup.
# Total VRAM footprint: ~8-12GB (leaves 20GB+ for batch processing)
# Inference precision: FP16/BF16 (Tensor Core accelerated)
# No CPU fallback - GPU is mandatory for this system.

embedders:
  # FOUNDATION (GPU: ~2GB VRAM)
  E1: { name: V_meaning, dim: 1024, finds: "Semantic similarity", misses: "Entities, code patterns, causal links", gpu: "warm-loaded" }

  # SEMANTIC ENHANCERS (topic_weight: 1.0, GPU: ~4GB total)
  E5: { name: V_causality, dim: 768, finds: "Causal chains (why X caused Y)", misses_by_E1: "Direction lost", gpu: "warm-loaded" }
  E6: { name: V_selectivity, sparse: true, finds: "Exact keyword matches", misses_by_E1: "Diluted by averaging", gpu: "warm-loaded" }
  E7: { name: V_correctness, dim: 1536, finds: "Code patterns, function signatures", misses_by_E1: "Treats code as NL", gpu: "warm-loaded" }
  E10: { name: V_multimodality, dim: 768, finds: "Same-goal work (different words)", integration: "Multiplicative boost on E1", gpu: "warm-loaded" }
  E12: { name: V_precision, per_token: true, finds: "Exact phrase matches", use: "Reranking only", gpu: "warm-loaded" }
  E13: { name: V_keyword_precision, sparse: true, finds: "Term expansions (fast→quick)", use: "Stage 1 recall", gpu: "warm-loaded" }

  # RELATIONAL ENHANCERS (topic_weight: 0.5, GPU: ~2GB total)
  E8: { name: V_connectivity, dim: 384, finds: "Graph structure (X imports Y)", gpu: "warm-loaded" }
  E11: { name: V_factuality, dim: 768, finds: "Entity knowledge (Diesel=database ORM)", model: "KEPLER (RoBERTa-base + TransE)", gpu: "warm-loaded" }

  # TEMPORAL CONTEXT (topic_weight: 0.0, POST-RETRIEVAL ONLY, GPU: ~1.5GB total)
  E2: { name: V_freshness, dim: 512, finds: "Recency", gpu: "warm-loaded" }
  E3: { name: V_periodicity, dim: 512, finds: "Time-of-day patterns", gpu: "warm-loaded" }
  E4: { name: V_ordering, dim: 512, finds: "Sequence (before/after)", gpu: "warm-loaded" }

  # STRUCTURAL (topic_weight: 0.5, GPU: ~1GB)
  E9: { name: V_robustness, dim: 1024, finds: "Noise-robust structure", gpu: "warm-loaded" }

# ═══════════════════════════════════════════════════════════════
# ARCHITECTURAL RULES (MUST follow)
# ═══════════════════════════════════════════════════════════════
arch_rules:
  # GPU-First (MANDATORY)
  ARCH-GPU-01: "GPU is mandatory - no CPU fallback for embeddings"
  ARCH-GPU-02: "All 13 embedders warm-loaded into VRAM at MCP server startup"
  ARCH-GPU-03: "Embedding inference uses FP16/BF16 Tensor Cores"
  ARCH-GPU-04: "FAISS indexes use GPU (faiss-gpu) not CPU"
  ARCH-GPU-05: "HDBSCAN clustering runs on GPU via cuML"
  ARCH-GPU-06: "Batch operations preferred - minimize kernel launches"
  ARCH-GPU-07: "Green Contexts partition SMs: 70% inference, 30% indexing"
  ARCH-GPU-08: "CUDA streams for async embedding + indexing overlap"

  # Core
  ARCH-01: "TeleologicalArray is atomic - all 13 embeddings or nothing"
  ARCH-02: "Apples-to-apples only - compare E1↔E1, never E1↔E5"
  ARCH-04: "Temporal (E2-E4) NEVER count toward topics"
  ARCH-05: "All 13 embedders required - missing = fatal"
  ARCH-06: "All memory ops through MCP tools only"
  ARCH-09: "Topic threshold: weighted_agreement >= 2.5"
  ARCH-10: "Divergence detection: SEMANTIC embedders only (E1,E5,E6,E7,E10,E12,E13)"

  # Retrieval Pipeline
  ARCH-12: "E1 is foundation - all retrieval starts with E1"
  ARCH-13: "Strategies: E1Only (default), MultiSpace (E1+enhancers), Pipeline (E13→E1→E12)"
  ARCH-17: "Strong E1 (>0.8): enhancers refine. Weak E1 (<0.4): enhancers broaden"
  ARCH-18: "E5 Causal: asymmetric similarity (cause→effect direction matters)"
  ARCH-21: "Multi-space fusion: use Weighted RRF, not weighted sum"

  # E10 Multiplicative Boost
  ARCH-28: "E10 uses multiplicative boost: E1 * (1 + boost), NOT linear blending"
  ARCH-29: "E10 boost adapts: strong E1=5%, medium=10%, weak=15%"
  ARCH-30: "E10 alignment: >0.5=boost, <0.5=reduce, =0.5=neutral"
  ARCH-33: "E10 multiplier clamped to [0.8, 1.2]"

  # Temporal (POST-RETRIEVAL ONLY)
  ARCH-25: "Temporal boosts POST-retrieval only, NOT in similarity fusion"

# ═══════════════════════════════════════════════════════════════
# ANTI-PATTERNS (FORBIDDEN)
# ═══════════════════════════════════════════════════════════════
forbidden:
  # GPU Anti-Patterns
  AP-GPU-01: "NEVER use CPU for embedding inference when GPU available"
  AP-GPU-02: "NEVER cold-load embedders per-request - warm-load at startup"
  AP-GPU-03: "NEVER use CPU FAISS when GPU FAISS available"
  AP-GPU-04: "NEVER use sklearn HDBSCAN - use cuML GPU implementation"
  AP-GPU-05: "NEVER transfer embeddings GPU→CPU→GPU - keep on GPU"
  AP-GPU-06: "NEVER use FP32 for inference - use FP16/BF16 Tensor Cores"
  AP-GPU-07: "NEVER serialize embeddings per-item - batch for GPU efficiency"
  AP-GPU-08: "NEVER block on sync - use CUDA streams for async ops"

  # Core Anti-Patterns
  AP-02: "No cross-embedder comparison (E1↔E5)"
  AP-04: "No partial TeleologicalArray"
  AP-05: "No embedding fusion into single vector"
  AP-60: "Temporal (E2-E4) MUST NOT count toward topics"
  AP-73: "Temporal MUST NOT be used in similarity fusion"
  AP-74: "E12 ColBERT: reranking ONLY, not initial retrieval"
  AP-75: "E13 SPLADE: Stage 1 recall ONLY, not final ranking"
  AP-77: "E5 MUST NOT use symmetric cosine - causal is directional"
  AP-79: "MUST NOT use simple weighted sum - use Weighted RRF"
  AP-80: "E10 MUST NOT use linear blending - makes E10 compete with E1"
  AP-84: "E10 MUST NOT override E1 - when E1=0, result=0"

# ═══════════════════════════════════════════════════════════════
# TOPIC SYSTEM
# ═══════════════════════════════════════════════════════════════
topics:
  weighted_agreement:
    formula: "Sum(topic_weight_i × is_clustered_i)"
    threshold: 2.5
    max: 8.5  # 7×1.0 (semantic) + 2×0.5 (relational) + 1×0.5 (structural)
    temporal_contribution: 0.0  # ALWAYS

  categories:
    SEMANTIC: { embedders: [E1,E5,E6,E7,E10,E12,E13], weight: 1.0 }
    RELATIONAL: { embedders: [E8,E11], weight: 0.5 }
    STRUCTURAL: { embedders: [E9], weight: 0.5 }
    TEMPORAL: { embedders: [E2,E3,E4], weight: 0.0 }  # Never for topics

  divergence_detection: "SEMANTIC embedders only"

# ═══════════════════════════════════════════════════════════════
# MCP TOOLS
# ═══════════════════════════════════════════════════════════════
mcp_tools:
  search: [search_graph, search_causes, search_connections, search_by_intent]
  memory: [store_memory, inject_context, get_memetic_status]
  sequence: [get_conversation_context, get_session_timeline, traverse_memory_chain]
  topic: [get_topic_portfolio, get_topic_stability, detect_topics]
  causal: [get_causal_chain, search_causes]
  graph: [get_graph_path, search_connections]
  intent: [search_by_intent, find_contextual_matches, detect_intent_drift]
  entity: [extract_entities, search_by_entities, infer_relationship, find_related_entities, validate_knowledge, get_entity_graph]
  maintenance: [trigger_consolidation, trigger_dream, merge_concepts, forget_concept]

  search_strategies:
    E1Only: "Fast, simple semantic queries"
    MultiSpace: "E1 + enhancers via RRF - use when E1 blind spots matter"
    Pipeline: "E13 recall → E1 dense → E12 rerank - maximum precision"

# ═══════════════════════════════════════════════════════════════
# RETRIEVAL PIPELINE
# ═══════════════════════════════════════════════════════════════
retrieval:
  stages:
    S1: "E13 SPLADE sparse recall → 10K candidates"
    S2: "E1 Matryoshka ANN → 1K candidates"
    S3: "RRF across semantic spaces → 100 candidates"
    S4: "Topic alignment (weighted_agreement >= 2.5) → 50"
    S5: "E12 MaxSim rerank → 10 final results"

  # When to use which enhancer
  use_E5: "Causal queries (why, what caused)"
  use_E7: "Code queries (implementations, functions)"
  use_E10: "Intent queries (same goal, similar purpose)"
  use_E11: "Entity queries (specific named things)"
  use_E6_E13: "Keyword queries (exact terms, jargon)"

# ═══════════════════════════════════════════════════════════════
# MEMORY SOURCES
# ═══════════════════════════════════════════════════════════════
memory_sources:
  HookDescription: "Claude's description of tool use"
  ClaudeResponse: "Session summaries, significant responses"
  MDFileChunk: "Markdown file chunks (200 words, 50 overlap)"

# ═══════════════════════════════════════════════════════════════
# DREAM CONSOLIDATION
# ═══════════════════════════════════════════════════════════════
dream:
  trigger: "entropy > 0.7 AND churn > 0.5"
  nrem: "Hebbian replay - strengthen high-importance edges"
  rem: "Hyperbolic random walk - discover blind spots"

# ═══════════════════════════════════════════════════════════════
# KEY THRESHOLDS
# ═══════════════════════════════════════════════════════════════
thresholds:
  topic: 2.5           # weighted_agreement for topic detection
  high_sim: 0.75       # High similarity
  low_sim: 0.30        # Divergence threshold
  duplicate: 0.90      # Duplicate detection
  entropy_dream: 0.70  # Dream trigger
  churn_dream: 0.50    # Dream trigger

# ═══════════════════════════════════════════════════════════════
# HOOKS (Native Claude Code)
# ═══════════════════════════════════════════════════════════════
hooks:
  config: ".claude/settings.json"
  SessionStart: "Load topic portfolio, warm GPU indexes"
  UserPromptSubmit: "GPU embed prompt, search, detect divergence, inject context"
  PreToolUse: "Inject brief relevant context"
  PostToolUse: "Capture + GPU embed as HookDescription"
  Stop: "Capture response summary"
  SessionEnd: "Persist state, GPU HDBSCAN, check dream triggers"

# ═══════════════════════════════════════════════════════════════
# GPU INFRASTRUCTURE (RTX 5090 32GB + CUDA 13.1)
# ═══════════════════════════════════════════════════════════════
gpu:
  hardware:
    device: "NVIDIA GeForce RTX 5090"
    architecture: "Blackwell (GB202)"
    vram: "32GB GDDR7"
    bandwidth: "1,792 GB/s"
    cuda_cores: 21760
    tensor_cores: 680
    sms: 170
    compute_capability: "12.0"
    driver: "R580+ (CUDA 13.1)"

  host:
    cpu: "AMD Ryzen 9 9950X3D"
    cores: "16 cores / 32 threads"
    ram: "128GB DDR5"
    l3_cache: "192MB (96MB x2)"

  vram_allocation:
    embedders: "~10GB (all 13 warm-loaded)"
    faiss_indexes: "~8GB (HNSW per-space)"
    batch_buffers: "~4GB (inference batches)"
    cuml_workspace: "~2GB (HDBSCAN, clustering)"
    reserved: "~8GB (headroom for spikes)"
    total_budget: "32GB"

  performance_targets:
    all_13_embed: "<200ms (GPU batch)"
    per_space_hnsw: "<1ms (faiss-gpu)"
    inject_context_p95: "<500ms"
    store_memory_p95: "<800ms"
    any_tool_p99: "<1000ms"
    topic_detection: "<20ms (cuML HDBSCAN)"
    warm_load_startup: "<30s (all 13 models)"

  cuda_features:
    green_contexts: true  # SM partitioning for QoS
    cuda_tile: true       # Portable kernel programming
    cuda_streams: true    # Async overlap
    tensor_cores: "FP16/BF16/FP8"
    mps_clients: 60       # Multi-Process Service

  testing:
    all_tests_use_gpu: true
    benchmarks_require_gpu: true
    no_cpu_fallback_tests: true
    gpu_memory_profiling: true
