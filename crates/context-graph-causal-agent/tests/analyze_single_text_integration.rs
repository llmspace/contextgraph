//! Integration tests for CausalDiscoveryLLM.analyze_single_text()
//!
//! Test Suite 2 from CAUSAL_HINT_INTEGRATION_TESTING.md
//! These tests require the Hermes 2 Pro model to be loaded.

use context_graph_causal_agent::{CausalDiscoveryLLM, CausalDirectionHint, LlmConfig};
use std::path::PathBuf;

/// Get the absolute path to the models directory
fn models_dir() -> PathBuf {
    // Check workspace root first (for cargo test from any directory)
    let workspace_root = std::env::var("CARGO_MANIFEST_DIR")
        .map(PathBuf::from)
        .map(|p| p.parent().unwrap().parent().unwrap().to_path_buf())
        .unwrap_or_else(|_| PathBuf::from("/home/cabdru/contextgraph"));

    workspace_root.join("models/hermes-2-pro")
}

/// Create LlmConfig with proper absolute paths
fn create_llm_config() -> LlmConfig {
    let models = models_dir();
    LlmConfig {
        model_path: models.join("Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf"),
        causal_grammar_path: models.join("causal_analysis.gbnf"),
        graph_grammar_path: models.join("graph_relationship.gbnf"),
        validation_grammar_path: models.join("validation.gbnf"),
        ..Default::default()
    }
}

/// Test 2.1: Causal Content Analysis
///
/// Input: "High cortisol levels cause memory impairment in the hippocampus."
/// Expected: is_causal=true, direction=Cause, confidence>=0.7
#[tokio::test]
async fn test_2_1_causal_content_analysis() {
    println!("\n=== Test 2.1: Causal Content Analysis ===");

    let config = create_llm_config();
    println!("Model path: {:?}", config.model_path);

    let llm = CausalDiscoveryLLM::with_config(config)
        .expect("FATAL: Failed to create LLM - model files missing?");

    // Load the model (this will fail if model is not available)
    println!("Loading LLM model...");
    llm.load()
        .await
        .expect("FATAL: Failed to load LLM model - check VRAM and model file");

    assert!(llm.is_loaded(), "LLM should be loaded after load()");

    // Synthetic test input
    let content = "High cortisol levels cause memory impairment in the hippocampus.";

    println!("Analyzing: {}", content);
    let hint = llm.analyze_single_text(content)
        .await
        .expect("FATAL: analyze_single_text failed");

    println!("Result:");
    println!("  is_causal: {}", hint.is_causal);
    println!("  direction_hint: {:?}", hint.direction_hint);
    println!("  confidence: {}", hint.confidence);
    println!("  key_phrases: {:?}", hint.key_phrases);

    // Verification per test document
    assert!(hint.is_causal, "Expected is_causal=true for causal content");
    assert_eq!(hint.direction_hint, CausalDirectionHint::Cause,
        "Expected direction=Cause for content about what CAUSES something");
    assert!(hint.confidence >= 0.5,
        "Expected confidence >= 0.5, got {}", hint.confidence);

    println!("[PASS] Test 2.1: Causal content analyzed correctly");
}

/// Test 2.2: Non-Causal Content Analysis
///
/// Input: "The weather in Paris is pleasant during spring."
/// Expected: is_causal=false, confidence < 0.5
#[tokio::test]
async fn test_2_2_non_causal_content_analysis() {
    println!("\n=== Test 2.2: Non-Causal Content Analysis ===");

    let config = create_llm_config();
    let llm = CausalDiscoveryLLM::with_config(config)
        .expect("FATAL: Failed to create LLM");

    println!("Loading LLM model...");
    llm.load()
        .await
        .expect("FATAL: Failed to load LLM model");

    // Non-causal synthetic input
    let content = "The weather in Paris is pleasant during spring.";

    println!("Analyzing: {}", content);
    let hint = llm.analyze_single_text(content)
        .await
        .expect("FATAL: analyze_single_text failed");

    println!("Result:");
    println!("  is_causal: {}", hint.is_causal);
    println!("  direction_hint: {:?}", hint.direction_hint);
    println!("  confidence: {}", hint.confidence);
    println!("  key_phrases: {:?}", hint.key_phrases);

    // For non-causal content, we expect either:
    // 1. is_causal=false, OR
    // 2. is_causal=true with low confidence (<0.5)
    let is_correctly_classified = !hint.is_causal || hint.confidence < 0.5;
    assert!(is_correctly_classified,
        "Non-causal content should be detected as non-causal or low confidence. \
        Got is_causal={}, confidence={}", hint.is_causal, hint.confidence);

    println!("[PASS] Test 2.2: Non-causal content analyzed correctly");
}

/// Test 2.3: Effect-Direction Content
///
/// Input: "Memory impairment results from prolonged stress exposure."
/// Expected: is_causal=true, direction=Effect, confidence>=0.6
#[tokio::test]
async fn test_2_3_effect_direction_content() {
    println!("\n=== Test 2.3: Effect-Direction Content ===");

    let config = create_llm_config();
    let llm = CausalDiscoveryLLM::with_config(config)
        .expect("FATAL: Failed to create LLM");

    println!("Loading LLM model...");
    llm.load()
        .await
        .expect("FATAL: Failed to load LLM model");

    // Effect-direction synthetic input
    let content = "Memory impairment results from prolonged stress exposure.";

    println!("Analyzing: {}", content);
    let hint = llm.analyze_single_text(content)
        .await
        .expect("FATAL: analyze_single_text failed");

    println!("Result:");
    println!("  is_causal: {}", hint.is_causal);
    println!("  direction_hint: {:?}", hint.direction_hint);
    println!("  confidence: {}", hint.confidence);
    println!("  key_phrases: {:?}", hint.key_phrases);

    // Verification per test document
    assert!(hint.is_causal, "Expected is_causal=true for causal content");
    // "results from" describes what the EFFECT is, so direction should be Effect
    assert_eq!(hint.direction_hint, CausalDirectionHint::Effect,
        "Expected direction=Effect for content describing what results FROM something");
    assert!(hint.confidence >= 0.5,
        "Expected confidence >= 0.5, got {}", hint.confidence);

    println!("[PASS] Test 2.3: Effect-direction content analyzed correctly");
}

/// Test 2.4: Bias Factors Verification
///
/// Verify that bias_factors() returns correct values for each direction
#[test]
fn test_2_4_bias_factors() {
    println!("\n=== Test 2.4: Bias Factors Verification ===");

    // Cause direction: boost cause (1.3), dampen effect (0.8)
    let (cause_bias, effect_bias) = CausalDirectionHint::Cause.bias_factors();
    assert!((cause_bias - 1.3).abs() < 0.01, "Cause bias should be 1.3, got {}", cause_bias);
    assert!((effect_bias - 0.8).abs() < 0.01, "Effect bias should be 0.8, got {}", effect_bias);
    println!("  Cause: cause_bias={}, effect_bias={}", cause_bias, effect_bias);

    // Effect direction: dampen cause (0.8), boost effect (1.3)
    let (cause_bias, effect_bias) = CausalDirectionHint::Effect.bias_factors();
    assert!((cause_bias - 0.8).abs() < 0.01, "Cause bias should be 0.8, got {}", cause_bias);
    assert!((effect_bias - 1.3).abs() < 0.01, "Effect bias should be 1.3, got {}", effect_bias);
    println!("  Effect: cause_bias={}, effect_bias={}", cause_bias, effect_bias);

    // Neutral direction: no bias (1.0, 1.0)
    let (cause_bias, effect_bias) = CausalDirectionHint::Neutral.bias_factors();
    assert!((cause_bias - 1.0).abs() < 0.01, "Cause bias should be 1.0, got {}", cause_bias);
    assert!((effect_bias - 1.0).abs() < 0.01, "Effect bias should be 1.0, got {}", effect_bias);
    println!("  Neutral: cause_bias={}, effect_bias={}", cause_bias, effect_bias);

    println!("[PASS] Test 2.4: All bias factors correct");
}

/// Test 2.5: LLM Not Loaded Error
///
/// Verify proper error when LLM is not loaded
#[tokio::test]
async fn test_2_5_llm_not_loaded_error() {
    println!("\n=== Test 2.5: LLM Not Loaded Error ===");

    let config = create_llm_config();
    let llm = CausalDiscoveryLLM::with_config(config)
        .expect("FATAL: Failed to create LLM");

    // DO NOT load the model
    assert!(!llm.is_loaded(), "LLM should not be loaded before load()");

    // Attempt to analyze without loading
    let result = llm.analyze_single_text("test content").await;

    assert!(result.is_err(), "Should error when LLM not loaded");
    let err = result.unwrap_err();
    println!("Error when LLM not loaded: {}", err);

    // Verify error type
    assert!(
        format!("{}", err).contains("not initialized") ||
        format!("{:?}", err).contains("LlmNotInitialized"),
        "Error should indicate LLM not initialized"
    );

    println!("[PASS] Test 2.5: Proper error when LLM not loaded");
}
