//! E11 Entity Embedder Benchmark Runner.
//!
//! This module provides the benchmark runner for evaluating the E11 entity
//! embedder (KEPLER, RoBERTa-base + TransE on Wikidata5M, 768D) and its MCP tool integrations.
//!
//! ## Benchmarks
//!
//! - **Benchmark A: Entity Extraction** - Tests extract_entities tool accuracy
//! - **Benchmark B: Entity Retrieval** - Compares E1-only vs E11-only vs E1+E11 hybrid
//! - **Benchmark C: TransE Relationship Inference** - Tests infer_relationship tool (KEPLER)
//! - **Benchmark D: Knowledge Validation** - Tests validate_knowledge tool scores (KEPLER)
//! - **Benchmark E: Entity Graph** - Tests get_entity_graph construction
//!
//! ## Constitution Compliance
//!
//! - ARCH-12: E1 is THE semantic foundation, E11 enhances with entity facts
//! - ARCH-20: E11 SHOULD use entity linking for disambiguation
//! - E11 is RELATIONAL_ENHANCER with topic_weight 0.5

use std::collections::HashMap;

use serde::{Deserialize, Serialize};
use tracing::info;
use uuid::Uuid;

use context_graph_core::entity::EntityMetadata;

#[cfg(feature = "real-embeddings")]
use std::sync::Arc;

#[cfg(feature = "real-embeddings")]
use std::time::Instant;

#[cfg(feature = "real-embeddings")]
use tracing::warn;

#[cfg(feature = "real-embeddings")]
use context_graph_core::entity::entity_jaccard_similarity;

#[cfg(feature = "real-embeddings")]
use context_graph_core::similarity::cosine_similarity;

#[cfg(feature = "real-embeddings")]
use context_graph_core::traits::{MultiArrayEmbeddingOutput, MultiArrayEmbeddingProvider};

#[cfg(feature = "real-embeddings")]
use context_graph_core::types::fingerprint::SemanticFingerprint;

#[cfg(feature = "real-embeddings")]
use context_graph_embeddings::models::KeplerModel;

#[cfg(feature = "real-embeddings")]
use context_graph_embeddings::{get_warm_provider, initialize_global_warm_provider};

use crate::datasets::e11_entity::{
    E11EntityBenchmarkDataset, E11EntityDatasetConfig, E11EntityDatasetLoader,
    extract_entity_mentions,
};

#[cfg(feature = "real-embeddings")]
use crate::datasets::e11_entity::KnowledgeTriple;

use crate::metrics::e11_entity::{
    E11EntityMetrics, EntityGraphMetrics, EntityRetrievalMetrics, ExtractionMetrics, TransEMetrics,
};
use crate::realdata::{DatasetLoader, RealDataset};

// ============================================================================
// Configuration
// ============================================================================

/// Configuration for E11 entity benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct E11EntityBenchmarkConfig {
    /// Dataset configuration.
    pub dataset: E11EntityDatasetConfig,
    /// Maximum chunks to load.
    pub max_chunks: usize,
    /// Number of queries for retrieval benchmark.
    pub num_queries: usize,
    /// Random seed for reproducibility.
    pub seed: u64,
    /// Whether to run all benchmarks.
    pub run_all: bool,
    /// Run entity extraction benchmark.
    pub run_extraction: bool,
    /// Run entity retrieval benchmark.
    pub run_retrieval: bool,
    /// Run TransE benchmark.
    pub run_transe: bool,
    /// Run knowledge validation benchmark.
    pub run_validation: bool,
    /// Run entity graph benchmark.
    pub run_graph: bool,
    /// Output path for results.
    pub output_path: Option<String>,
}

impl Default for E11EntityBenchmarkConfig {
    fn default() -> Self {
        Self {
            dataset: E11EntityDatasetConfig::default(),
            max_chunks: 1000,
            num_queries: 100,
            seed: 42,
            run_all: true,
            run_extraction: true,
            run_retrieval: true,
            run_transe: true,
            run_validation: true,
            run_graph: true,
            output_path: None,
        }
    }
}

// ============================================================================
// Benchmark Results
// ============================================================================

/// Complete results from E11 entity benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct E11EntityBenchmarkResults {
    /// Combined metrics.
    pub metrics: E11EntityMetrics,
    /// Timing information.
    pub timings: E11EntityBenchmarkTimings,
    /// Dataset statistics.
    pub dataset_stats: E11EntityDatasetStats,
    /// Individual benchmark results.
    pub extraction_results: Option<ExtractionBenchmarkResults>,
    pub retrieval_results: Option<RetrievalBenchmarkResults>,
    pub transe_results: Option<TransEBenchmarkResults>,
    pub validation_results: Option<ValidationBenchmarkResults>,
    pub graph_results: Option<GraphBenchmarkResults>,
    /// Configuration used.
    pub config: E11EntityBenchmarkConfig,
    /// Whether all targets were met.
    pub all_targets_met: bool,
}

/// Dataset statistics wrapper.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct E11EntityDatasetStats {
    pub num_documents: usize,
    pub docs_with_entities: usize,
    pub total_entities: usize,
    pub unique_entities: usize,
    pub avg_entities_per_doc: f64,
    pub num_valid_triples: usize,
    pub num_invalid_triples: usize,
    pub num_entity_pairs: usize,
    pub entity_type_distribution: HashMap<String, usize>,
}

/// Timing information for benchmarks.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct E11EntityBenchmarkTimings {
    pub total_ms: u64,
    pub dataset_load_ms: u64,
    pub embedding_ms: u64,
    pub extraction_benchmark_ms: u64,
    pub retrieval_benchmark_ms: u64,
    pub transe_benchmark_ms: u64,
    pub validation_benchmark_ms: u64,
    pub graph_benchmark_ms: u64,
}

/// Results from extraction benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExtractionBenchmarkResults {
    pub metrics: ExtractionMetrics,
    pub sample_extractions: Vec<SampleExtraction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SampleExtraction {
    pub doc_id: Uuid,
    pub text_snippet: String,
    pub predicted_entities: Vec<String>,
    pub ground_truth_entities: Vec<String>,
    pub precision: f64,
    pub recall: f64,
}

/// Results from retrieval benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RetrievalBenchmarkResults {
    pub metrics: EntityRetrievalMetrics,
    pub query_results: Vec<QueryResult>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResult {
    pub query_id: Uuid,
    pub query_text: String,
    pub query_entities: Vec<String>,
    pub e1_mrr: f64,
    pub e11_mrr: f64,
    pub hybrid_mrr: f64,
}

/// Results from TransE benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TransEBenchmarkResults {
    pub metrics: TransEMetrics,
    pub valid_triple_scores: Vec<TripleScoreResult>,
    pub invalid_triple_scores: Vec<TripleScoreResult>,
    pub relationship_inferences: Vec<RelationshipInferenceResult>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TripleScoreResult {
    pub subject: String,
    pub predicate: String,
    pub object: String,
    pub score: f32,
    pub is_valid_ground_truth: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RelationshipInferenceResult {
    pub head: String,
    pub tail: String,
    pub predicted_relation: String,
    pub expected_relation: Option<String>,
    pub score: f32,
    pub rank: usize,
}

/// Results from validation benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationBenchmarkResults {
    pub accuracy: f64,
    pub optimal_threshold: f32,
    pub confusion_matrix: ConfusionMatrix,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ConfusionMatrix {
    pub true_positives: usize,
    pub true_negatives: usize,
    pub false_positives: usize,
    pub false_negatives: usize,
}

/// Results from graph benchmark.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphBenchmarkResults {
    pub metrics: EntityGraphMetrics,
    pub top_entities: Vec<(String, usize)>,
    pub top_relationships: Vec<(String, String, String, f32)>,
}

// ============================================================================
// Benchmark Runner
// ============================================================================

/// E11 Entity Benchmark Runner.
pub struct E11EntityBenchmarkRunner {
    #[allow(dead_code)] // Used by bin targets, not lib tests
    config: E11EntityBenchmarkConfig,
    #[cfg(feature = "real-embeddings")]
    provider: Option<Arc<dyn MultiArrayEmbeddingProvider>>,
}

impl E11EntityBenchmarkRunner {
    /// Create a new benchmark runner.
    pub fn new(config: E11EntityBenchmarkConfig) -> Self {
        Self {
            config,
            #[cfg(feature = "real-embeddings")]
            provider: None,
        }
    }

    /// Run all configured benchmarks.
    #[cfg(feature = "real-embeddings")]
    pub async fn run(&mut self, data_dir: &str) -> Result<E11EntityBenchmarkResults, String> {
        let total_start = Instant::now();
        info!("E11EntityBenchmarkRunner: Starting E11 entity benchmark");

        // Load dataset
        let dataset_start = Instant::now();
        let dataset = self.load_dataset(data_dir)?;
        let dataset_load_ms = dataset_start.elapsed().as_millis() as u64;

        info!(
            num_documents = dataset.documents.len(),
            total_entities = dataset.stats.total_entities,
            unique_entities = dataset.stats.unique_entities,
            valid_triples = dataset.ground_truth.valid_triples.len(),
            invalid_triples = dataset.ground_truth.invalid_triples.len(),
            "E11EntityBenchmarkRunner: Dataset loaded"
        );

        // Initialize warm embedding provider
        let embedding_start = Instant::now();
        self.initialize_provider().await?;
        let embedding_ms = embedding_start.elapsed().as_millis() as u64;

        // Run benchmarks
        let mut metrics = E11EntityMetrics::default();
        let mut timings = E11EntityBenchmarkTimings {
            dataset_load_ms,
            embedding_ms,
            ..Default::default()
        };

        let mut extraction_results = None;
        let mut retrieval_results = None;
        let mut transe_results = None;
        let mut validation_results = None;
        let mut graph_results = None;

        // Benchmark A: Entity Extraction
        if self.config.run_all || self.config.run_extraction {
            let start = Instant::now();
            let results = self.run_extraction_benchmark(&dataset)?;
            timings.extraction_benchmark_ms = start.elapsed().as_millis() as u64;
            metrics.extraction = results.metrics.clone();
            extraction_results = Some(results);
        }

        // Benchmark B: Entity Retrieval
        if self.config.run_all || self.config.run_retrieval {
            let start = Instant::now();
            let results = self.run_retrieval_benchmark(&dataset).await?;
            timings.retrieval_benchmark_ms = start.elapsed().as_millis() as u64;
            metrics.retrieval = results.metrics.clone();
            retrieval_results = Some(results);
        }

        // Benchmark C: TransE Relationship Inference
        if self.config.run_all || self.config.run_transe {
            let start = Instant::now();
            let results = self.run_transe_benchmark(&dataset).await?;
            timings.transe_benchmark_ms = start.elapsed().as_millis() as u64;
            metrics.transe = results.metrics.clone();
            transe_results = Some(results);
        }

        // Benchmark D: Knowledge Validation
        if self.config.run_all || self.config.run_validation {
            let start = Instant::now();
            let results = self.run_validation_benchmark(&dataset).await?;
            timings.validation_benchmark_ms = start.elapsed().as_millis() as u64;
            metrics.transe.knowledge_validation_accuracy = results.accuracy;
            validation_results = Some(results);
        }

        // Benchmark E: Entity Graph
        if self.config.run_all || self.config.run_graph {
            let start = Instant::now();
            let results = self.run_graph_benchmark(&dataset).await?;
            timings.graph_benchmark_ms = start.elapsed().as_millis() as u64;
            metrics.graph = results.metrics.clone();
            graph_results = Some(results);
        }

        timings.total_ms = total_start.elapsed().as_millis() as u64;

        // Build dataset stats
        let dataset_stats = E11EntityDatasetStats {
            num_documents: dataset.stats.num_documents,
            docs_with_entities: dataset.stats.docs_with_entities,
            total_entities: dataset.stats.total_entities,
            unique_entities: dataset.stats.unique_entities,
            avg_entities_per_doc: dataset.stats.avg_entities_per_doc,
            num_valid_triples: dataset.stats.num_valid_triples,
            num_invalid_triples: dataset.stats.num_invalid_triples,
            num_entity_pairs: dataset.stats.num_entity_pairs,
            entity_type_distribution: dataset.stats.entity_type_counts.clone(),
        };

        let all_targets_met = metrics.meets_all_targets();

        info!(
            total_ms = timings.total_ms,
            all_targets_met = all_targets_met,
            overall_score = metrics.overall_quality_score(),
            "E11EntityBenchmarkRunner: Benchmark complete"
        );

        Ok(E11EntityBenchmarkResults {
            metrics,
            timings,
            dataset_stats,
            extraction_results,
            retrieval_results,
            transe_results,
            validation_results,
            graph_results,
            config: self.config.clone(),
            all_targets_met,
        })
    }

    /// Load dataset from real data.
    #[allow(dead_code)] // Called from run() which is used by bin targets
    fn load_dataset(&self, data_dir: &str) -> Result<E11EntityBenchmarkDataset, String> {
        let loader = DatasetLoader::new().with_max_chunks(self.config.max_chunks);

        let real_dataset: RealDataset = loader
            .load_from_dir(data_dir)
            .map_err(|e| format!("Failed to load dataset: {}", e))?;

        if real_dataset.chunks.is_empty() {
            return Err("Dataset has no chunks".to_string());
        }

        let config = E11EntityDatasetConfig {
            max_chunks: self.config.max_chunks,
            seed: self.config.seed,
            min_entities_per_doc: 0, // Include all documents
            num_valid_triples: 100,
            num_invalid_triples: 100,
            num_entity_pairs: self.config.num_queries,
        };

        let entity_loader = E11EntityDatasetLoader::new(config);
        let dataset = entity_loader.load_from_chunks(&real_dataset.chunks);

        dataset.validate()?;

        Ok(dataset)
    }

    /// Initialize the warm embedding provider.
    #[cfg(feature = "real-embeddings")]
    async fn initialize_provider(&mut self) -> Result<(), String> {
        info!("E11EntityBenchmarkRunner: Initializing warm embedding provider");

        // Initialize the warm provider (loads all 13 models to GPU)
        initialize_global_warm_provider()
            .await
            .map_err(|e| format!("Failed to initialize warm provider: {}", e))?;

        let provider = get_warm_provider()
            .map_err(|e| format!("Failed to get warm provider: {}", e))?;

        self.provider = Some(provider);
        Ok(())
    }

    /// Get provider reference.
    #[cfg(feature = "real-embeddings")]
    fn provider(&self) -> Result<&Arc<dyn MultiArrayEmbeddingProvider>, String> {
        self.provider
            .as_ref()
            .ok_or_else(|| "Warm provider not initialized".to_string())
    }

    /// Embed a single text and return its fingerprint.
    #[cfg(feature = "real-embeddings")]
    async fn embed_text(&self, text: &str) -> Result<SemanticFingerprint, String> {
        let provider = self.provider()?;
        let output: MultiArrayEmbeddingOutput = provider
            .embed_all(text)
            .await
            .map_err(|e| format!("Failed to embed text: {}", e))?;
        Ok(output.fingerprint)
    }

    /// Embed multiple texts and return their fingerprints.
    ///
    /// Uses concurrent tokio tasks for GPU parallelism.
    /// Each task runs embed_all() which processes all 13 embedders in parallel.
    #[cfg(feature = "real-embeddings")]
    async fn embed_texts(&self, texts: &[&str]) -> Result<Vec<SemanticFingerprint>, String> {
        use futures::future::join_all;

        let provider = self.provider()?.clone();

        // Spawn concurrent tasks for each text
        let tasks: Vec<_> = texts
            .iter()
            .map(|text| {
                let provider = provider.clone();
                let text = text.to_string();
                tokio::spawn(async move {
                    provider
                        .embed_all(&text)
                        .await
                        .map(|output| output.fingerprint)
                        .map_err(|e| format!("Failed to embed text: {}", e))
                })
            })
            .collect();

        // Wait for all tasks to complete
        let results = join_all(tasks).await;

        // Collect results, propagating any errors
        let mut fingerprints = Vec::with_capacity(texts.len());
        for result in results {
            match result {
                Ok(Ok(fp)) => fingerprints.push(fp),
                Ok(Err(e)) => return Err(e),
                Err(e) => return Err(format!("Task join error: {}", e)),
            }
        }

        Ok(fingerprints)
    }

    // ========================================================================
    // Benchmark A: Entity Extraction
    // ========================================================================

    #[allow(dead_code)] // Called from run() which is used by bin targets
    fn run_extraction_benchmark(
        &self,
        dataset: &E11EntityBenchmarkDataset,
    ) -> Result<ExtractionBenchmarkResults, String> {
        info!("E11EntityBenchmarkRunner: Running extraction benchmark");

        let mut all_metrics: Vec<ExtractionMetrics> = Vec::new();
        let mut sample_extractions: Vec<SampleExtraction> = Vec::new();

        for doc in &dataset.documents {
            // Re-extract entities from text
            let extracted: EntityMetadata = extract_entity_mentions(&doc.text);
            let predicted: Vec<String> = extracted
                .entities
                .iter()
                .map(|e| e.canonical_id.clone())
                .collect();

            // Ground truth from dataset
            let ground_truth: Vec<String> = doc
                .entities
                .iter()
                .map(|e| e.canonical_id.clone())
                .collect();

            let metrics = ExtractionMetrics::compute(&predicted, &ground_truth);
            all_metrics.push(metrics.clone());

            // Collect sample extractions
            if sample_extractions.len() < 10 {
                let snippet = if doc.text.len() > 100 {
                    format!("{}...", &doc.text[..100])
                } else {
                    doc.text.clone()
                };

                sample_extractions.push(SampleExtraction {
                    doc_id: doc.id,
                    text_snippet: snippet,
                    predicted_entities: predicted,
                    ground_truth_entities: ground_truth,
                    precision: metrics.precision,
                    recall: metrics.recall,
                });
            }
        }

        let aggregated = ExtractionMetrics::aggregate(&all_metrics);

        // Compute canonicalization accuracy
        let mut correct_canonicalizations = 0;
        let mut total_canonicalizations = 0;

        for (surface, expected_canonical) in &dataset.ground_truth.canonicalization {
            let detected = extract_entity_mentions(surface);
            if let Some(entity) = detected.entities.first() {
                total_canonicalizations += 1;
                if &entity.canonical_id == expected_canonical {
                    correct_canonicalizations += 1;
                }
            }
        }

        let mut final_metrics = aggregated;
        if total_canonicalizations > 0 {
            final_metrics.canonicalization_accuracy =
                correct_canonicalizations as f64 / total_canonicalizations as f64;
        }

        info!(
            f1 = final_metrics.f1_score,
            precision = final_metrics.precision,
            recall = final_metrics.recall,
            canonicalization = final_metrics.canonicalization_accuracy,
            "E11EntityBenchmarkRunner: Extraction benchmark complete"
        );

        Ok(ExtractionBenchmarkResults {
            metrics: final_metrics,
            sample_extractions,
        })
    }

    // ========================================================================
    // Benchmark B: Entity Retrieval
    // ========================================================================

    #[cfg(feature = "real-embeddings")]
    async fn run_retrieval_benchmark(
        &self,
        dataset: &E11EntityBenchmarkDataset,
    ) -> Result<RetrievalBenchmarkResults, String> {
        info!("E11EntityBenchmarkRunner: Running retrieval benchmark");

        let mut query_results: Vec<QueryResult> = Vec::new();
        let mut e1_ranks: Vec<usize> = Vec::new();
        let mut e11_ranks: Vec<usize> = Vec::new();
        let mut hybrid_ranks: Vec<usize> = Vec::new();
        let mut latencies: Vec<f64> = Vec::new();

        // Sample queries from documents with entities
        let docs_with_entities: Vec<_> = dataset
            .documents
            .iter()
            .filter(|d| !d.entities.is_empty())
            .take(self.config.num_queries)
            .collect();

        // Embed all documents first
        let doc_texts: Vec<&str> = dataset.documents.iter().map(|d| d.text.as_str()).collect();

        info!(
            num_docs = doc_texts.len(),
            "E11EntityBenchmarkRunner: Embedding documents for retrieval"
        );

        let doc_fingerprints = self.embed_texts(&doc_texts).await?;

        // For each query document, find similar documents
        for query_doc in docs_with_entities {
            let query_start = Instant::now();

            // Get query embedding
            let query_fingerprint = self.embed_text(&query_doc.text).await?;

            // Compute E1-only similarities
            let mut e1_scores: Vec<(usize, f32)> = doc_fingerprints
                .iter()
                .enumerate()
                .filter(|(i, _)| dataset.documents[*i].id != query_doc.id)
                .map(|(i, fp)| {
                    let sim = cosine_similarity(&query_fingerprint.e1_semantic, &fp.e1_semantic)
                        .unwrap_or(0.0);
                    (i, sim)
                })
                .collect();
            e1_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            // Compute E11-only similarities
            let mut e11_scores: Vec<(usize, f32)> = doc_fingerprints
                .iter()
                .enumerate()
                .filter(|(i, _)| dataset.documents[*i].id != query_doc.id)
                .map(|(i, fp)| {
                    let sim = cosine_similarity(&query_fingerprint.e11_entity, &fp.e11_entity)
                        .unwrap_or(0.0);
                    (i, sim)
                })
                .collect();
            e11_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            // Compute hybrid scores (E1 + E11 + entity Jaccard)
            let query_entities = extract_entity_mentions(&query_doc.text);
            let mut hybrid_scores: Vec<(usize, f32)> = doc_fingerprints
                .iter()
                .enumerate()
                .filter(|(i, _)| dataset.documents[*i].id != query_doc.id)
                .map(|(i, fp)| {
                    let e1_sim = cosine_similarity(&query_fingerprint.e1_semantic, &fp.e1_semantic)
                        .unwrap_or(0.0);
                    let e11_sim = cosine_similarity(&query_fingerprint.e11_entity, &fp.e11_entity)
                        .unwrap_or(0.0);
                    let doc_entities = extract_entity_mentions(&dataset.documents[i].text);
                    let jaccard = entity_jaccard_similarity(&query_entities, &doc_entities);

                    // Hybrid: 0.5 * E1 + 0.3 * E11 + 0.2 * Jaccard
                    let hybrid = 0.5 * e1_sim + 0.3 * e11_sim + 0.2 * jaccard;
                    (i, hybrid)
                })
                .collect();
            hybrid_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            latencies.push(query_start.elapsed().as_secs_f64() * 1000.0);

            // Find rank of a relevant document (one with shared entities)
            let query_entity_ids: std::collections::HashSet<_> = query_doc
                .entities
                .iter()
                .map(|e| e.canonical_id.clone())
                .collect();

            // Find first document with shared entities
            let find_rank = |scores: &[(usize, f32)]| -> usize {
                for (rank, (idx, _)) in scores.iter().enumerate() {
                    let doc_entity_ids: std::collections::HashSet<_> = dataset.documents[*idx]
                        .entities
                        .iter()
                        .map(|e| e.canonical_id.clone())
                        .collect();

                    if !query_entity_ids.is_disjoint(&doc_entity_ids) {
                        return rank + 1;
                    }
                }
                0 // Not found
            };

            let e1_rank = find_rank(&e1_scores);
            let e11_rank = find_rank(&e11_scores);
            let hybrid_rank = find_rank(&hybrid_scores);

            e1_ranks.push(e1_rank);
            e11_ranks.push(e11_rank);
            hybrid_ranks.push(hybrid_rank);

            let e1_mrr = if e1_rank > 0 { 1.0 / e1_rank as f64 } else { 0.0 };
            let e11_mrr = if e11_rank > 0 { 1.0 / e11_rank as f64 } else { 0.0 };
            let hybrid_mrr = if hybrid_rank > 0 { 1.0 / hybrid_rank as f64 } else { 0.0 };

            query_results.push(QueryResult {
                query_id: query_doc.id,
                query_text: if query_doc.text.len() > 50 {
                    format!("{}...", &query_doc.text[..50])
                } else {
                    query_doc.text.clone()
                },
                query_entities: query_doc.entities.iter().map(|e| e.canonical_id.clone()).collect(),
                e1_mrr,
                e11_mrr,
                hybrid_mrr,
            });
        }

        // Compute overall MRR
        let mrr_e1_only = crate::metrics::e11_entity::compute_mrr(&e1_ranks);
        let mrr_e11_only = crate::metrics::e11_entity::compute_mrr(&e11_ranks);
        let mrr_e1_e11_hybrid = crate::metrics::e11_entity::compute_mrr(&hybrid_ranks);

        let e11_contribution_pct =
            EntityRetrievalMetrics::compute_contribution(mrr_e1_only, mrr_e1_e11_hybrid);

        let metrics = EntityRetrievalMetrics {
            mrr_e1_only,
            mrr_e11_only,
            mrr_e1_e11_hybrid,
            e11_contribution_pct,
            entity_overlap_correlation: 0.0, // Would need more computation
            precision_at_k: HashMap::new(),
            recall_at_k: HashMap::new(),
            queries_evaluated: query_results.len(),
        };

        info!(
            mrr_e1 = mrr_e1_only,
            mrr_e11 = mrr_e11_only,
            mrr_hybrid = mrr_e1_e11_hybrid,
            e11_contribution_pct = e11_contribution_pct,
            "E11EntityBenchmarkRunner: Retrieval benchmark complete"
        );

        Ok(RetrievalBenchmarkResults {
            metrics,
            query_results,
        })
    }

    // ========================================================================
    // Benchmark C: TransE Relationship Inference
    // ========================================================================

    #[cfg(feature = "real-embeddings")]
    async fn run_transe_benchmark(
        &self,
        dataset: &E11EntityBenchmarkDataset,
    ) -> Result<TransEBenchmarkResults, String> {
        info!("E11EntityBenchmarkRunner: Running TransE benchmark");

        let mut valid_scores: Vec<f32> = Vec::new();
        let mut invalid_scores: Vec<f32> = Vec::new();
        let mut valid_triple_results: Vec<TripleScoreResult> = Vec::new();
        let mut invalid_triple_results: Vec<TripleScoreResult> = Vec::new();
        let mut inference_results: Vec<RelationshipInferenceResult> = Vec::new();

        // Known relations for inference
        let known_relations = vec![
            "depends_on", "uses", "extends", "part_of", "created_by",
            "alternative_to", "implements", "is_type",
        ];

        // Score valid triples
        for triple in &dataset.ground_truth.valid_triples {
            if let Ok(score) = self.score_triple(triple).await {
                valid_scores.push(score);
                valid_triple_results.push(TripleScoreResult {
                    subject: triple.subject.clone(),
                    predicate: triple.predicate.clone(),
                    object: triple.object.clone(),
                    score,
                    is_valid_ground_truth: true,
                });
            }
        }

        // Score invalid triples
        for triple in &dataset.ground_truth.invalid_triples {
            if let Ok(score) = self.score_triple(triple).await {
                invalid_scores.push(score);
                invalid_triple_results.push(TripleScoreResult {
                    subject: triple.subject.clone(),
                    predicate: triple.predicate.clone(),
                    object: triple.object.clone(),
                    score,
                    is_valid_ground_truth: false,
                });
            }
        }

        // Relationship inference on entity pairs
        let mut inference_ranks: Vec<usize> = Vec::new();
        for pair in dataset.ground_truth.entity_pairs.iter().take(50) {
            // Embed head and tail
            let head_fp = match self.embed_text(&pair.head).await {
                Ok(fp) => fp,
                Err(e) => {
                    warn!(error = %e, head = %pair.head, "Failed to embed head entity");
                    continue;
                }
            };

            let tail_fp = match self.embed_text(&pair.tail).await {
                Ok(fp) => fp,
                Err(e) => {
                    warn!(error = %e, tail = %pair.tail, "Failed to embed tail entity");
                    continue;
                }
            };

            // Score each known relation
            let mut relation_scores: Vec<(&str, f32)> = Vec::new();
            for relation in &known_relations {
                let relation_fp = match self.embed_text(relation).await {
                    Ok(fp) => fp,
                    Err(_) => continue,
                };

                let score = KeplerModel::transe_score(
                    &head_fp.e11_entity,
                    &relation_fp.e11_entity,
                    &tail_fp.e11_entity,
                );
                relation_scores.push((relation, score));
            }

            relation_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            if let Some((predicted_relation, score)) = relation_scores.first() {
                let rank = if let Some(expected) = &pair.expected_relation {
                    relation_scores
                        .iter()
                        .position(|(r, _)| r == expected)
                        .map(|p| p + 1)
                        .unwrap_or(0)
                } else {
                    1 // No expected relation, count as rank 1
                };

                inference_ranks.push(rank);

                inference_results.push(RelationshipInferenceResult {
                    head: pair.head.clone(),
                    tail: pair.tail.clone(),
                    predicted_relation: predicted_relation.to_string(),
                    expected_relation: pair.expected_relation.clone(),
                    score: *score,
                    rank,
                });
            }
        }

        let mut metrics = TransEMetrics::compute(&valid_scores, &invalid_scores);
        metrics.relationship_inference_mrr = crate::metrics::e11_entity::compute_mrr(&inference_ranks);

        info!(
            valid_avg = metrics.valid_triple_avg_score,
            invalid_avg = metrics.invalid_triple_avg_score,
            separation = metrics.separation_score,
            inference_mrr = metrics.relationship_inference_mrr,
            "E11EntityBenchmarkRunner: TransE benchmark complete"
        );

        Ok(TransEBenchmarkResults {
            metrics,
            valid_triple_scores: valid_triple_results,
            invalid_triple_scores: invalid_triple_results,
            relationship_inferences: inference_results,
        })
    }

    /// Score a single triple using TransE.
    #[cfg(feature = "real-embeddings")]
    async fn score_triple(
        &self,
        triple: &KnowledgeTriple,
    ) -> Result<f32, String> {
        let h = self.embed_text(&triple.subject).await?;
        let r = self.embed_text(&triple.predicate).await?;
        let t = self.embed_text(&triple.object).await?;

        let score = KeplerModel::transe_score(&h.e11_entity, &r.e11_entity, &t.e11_entity);
        Ok(score)
    }

    // ========================================================================
    // Benchmark D: Knowledge Validation
    // ========================================================================

    #[cfg(feature = "real-embeddings")]
    async fn run_validation_benchmark(
        &self,
        dataset: &E11EntityBenchmarkDataset,
    ) -> Result<ValidationBenchmarkResults, String> {
        info!("E11EntityBenchmarkRunner: Running validation benchmark");

        let mut valid_scores: Vec<f32> = Vec::new();
        let mut invalid_scores: Vec<f32> = Vec::new();

        // Score all triples
        for triple in &dataset.ground_truth.valid_triples {
            if let Ok(score) = self.score_triple(triple).await {
                valid_scores.push(score);
            }
        }

        for triple in &dataset.ground_truth.invalid_triples {
            if let Ok(score) = self.score_triple(triple).await {
                invalid_scores.push(score);
            }
        }

        // Find optimal threshold adaptively based on score distributions
        let (best_threshold, best_accuracy) = find_optimal_threshold(&valid_scores, &invalid_scores);

        // Compute confusion matrix at optimal threshold
        let confusion_matrix = ConfusionMatrix {
            true_positives: valid_scores.iter().filter(|&&s| s > best_threshold).count(),
            false_negatives: valid_scores.iter().filter(|&&s| s <= best_threshold).count(),
            true_negatives: invalid_scores.iter().filter(|&&s| s <= best_threshold).count(),
            false_positives: invalid_scores.iter().filter(|&&s| s > best_threshold).count(),
        };

        info!(
            accuracy = best_accuracy,
            threshold = best_threshold,
            tp = confusion_matrix.true_positives,
            tn = confusion_matrix.true_negatives,
            fp = confusion_matrix.false_positives,
            fn_ = confusion_matrix.false_negatives,
            "E11EntityBenchmarkRunner: Validation benchmark complete"
        );

        Ok(ValidationBenchmarkResults {
            accuracy: best_accuracy,
            optimal_threshold: best_threshold,
            confusion_matrix,
        })
    }

    // ========================================================================
    // Benchmark E: Entity Graph
    // ========================================================================

    #[cfg(feature = "real-embeddings")]
    async fn run_graph_benchmark(
        &self,
        dataset: &E11EntityBenchmarkDataset,
    ) -> Result<GraphBenchmarkResults, String> {
        info!("E11EntityBenchmarkRunner: Running graph benchmark");

        // Build entity co-occurrence graph from documents
        let mut entity_counts: HashMap<String, usize> = HashMap::new();
        let mut cooccurrences: HashMap<(String, String), usize> = HashMap::new();

        for doc in &dataset.documents {
            for entity in &doc.entities {
                *entity_counts.entry(entity.canonical_id.clone()).or_insert(0) += 1;
            }

            // Record co-occurrences
            let entities: Vec<_> = doc.entities.iter().map(|e| e.canonical_id.clone()).collect();
            for i in 0..entities.len() {
                for j in (i + 1)..entities.len() {
                    let key = if entities[i] < entities[j] {
                        (entities[i].clone(), entities[j].clone())
                    } else {
                        (entities[j].clone(), entities[i].clone())
                    };
                    *cooccurrences.entry(key).or_insert(0) += 1;
                }
            }
        }

        // Build nodes (top entities by count)
        let mut entity_list: Vec<_> = entity_counts.into_iter().collect();
        entity_list.sort_by(|a, b| b.1.cmp(&a.1));

        let top_entities: Vec<(String, usize)> = entity_list.iter().take(50).cloned().collect();
        let top_entity_set: std::collections::HashSet<_> = top_entities.iter().map(|(e, _)| e.clone()).collect();

        // Build edges (co-occurrences between top entities)
        let mut edges: Vec<(String, String, String, f32)> = Vec::new();
        let max_cooccur = cooccurrences.values().max().copied().unwrap_or(1) as f32;

        for ((e1, e2), count) in &cooccurrences {
            if top_entity_set.contains(e1) && top_entity_set.contains(e2) {
                let weight = *count as f32 / max_cooccur;
                let relation = cooccurrence_relation(*count);
                edges.push((e1.clone(), e2.clone(), relation.to_string(), weight));
            }
        }

        // Sort edges by weight
        edges.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        let top_relationships: Vec<_> = edges.iter().take(20).cloned().collect();

        // Build nodes for metrics
        let nodes: Vec<(String, String)> = top_entities
            .iter()
            .map(|(id, _)| {
                // Find entity type from dataset
                let entity_type = dataset.ground_truth.entity_types.get(id)
                    .cloned()
                    .unwrap_or_else(|| "Unknown".to_string());
                (id.clone(), entity_type)
            })
            .collect();

        let metrics = EntityGraphMetrics::compute(&nodes, &edges);

        info!(
            nodes = metrics.nodes_discovered,
            edges = metrics.edges_inferred,
            density = metrics.graph_density,
            "E11EntityBenchmarkRunner: Graph benchmark complete"
        );

        Ok(GraphBenchmarkResults {
            metrics,
            top_entities,
            top_relationships,
        })
    }
}

// ============================================================================
// Helper Functions
// ============================================================================

/// Find optimal classification threshold by searching around the midpoint of distributions.
#[cfg(feature = "real-embeddings")]
fn find_optimal_threshold(valid_scores: &[f32], invalid_scores: &[f32]) -> (f32, f64) {
    let avg = |scores: &[f32]| -> f32 {
        if scores.is_empty() { 0.0 } else { scores.iter().sum::<f32>() / scores.len() as f32 }
    };

    let valid_avg = avg(valid_scores);
    let invalid_avg = avg(invalid_scores);
    let midpoint = (valid_avg + invalid_avg) / 2.0;
    let step = (valid_avg - invalid_avg).abs().max(0.1) / 10.0;

    (-10..=10)
        .map(|i| {
            let threshold = midpoint + (i as f32) * step;
            let accuracy = TransEMetrics::compute_validation_accuracy(valid_scores, invalid_scores, threshold);
            (threshold, accuracy)
        })
        .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal))
        .unwrap_or((midpoint, 0.0))
}

/// Determine relationship strength label based on co-occurrence count.
#[allow(dead_code)] // Called from benchmark infrastructure used by bin targets
fn cooccurrence_relation(count: usize) -> &'static str {
    if count >= 5 {
        "strongly_related_to"
    } else if count >= 2 {
        "related_to"
    } else {
        "co_occurs_with"
    }
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(all(test, feature = "benchmark-tests"))]
mod tests {
    use super::*;

    #[test]
    fn test_config_defaults() {
        let config = E11EntityBenchmarkConfig::default();
        assert!(config.run_all);
        assert_eq!(config.seed, 42);
        assert_eq!(config.max_chunks, 1000);
    }

    #[test]
    fn test_dataset_stats_default() {
        let stats = E11EntityDatasetStats::default();
        assert_eq!(stats.num_documents, 0);
        assert_eq!(stats.total_entities, 0);
    }
}
