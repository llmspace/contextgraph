# M03-L16: BatchQueue and Request Types

```xml
<task_spec id="M03-L16" version="1.0">
<metadata>
  <title>Implement BatchQueue and BatchRequest Types</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>16</sequence>
  <implements>PRD batch processing infrastructure</implements>
  <depends_on>M03-F06, M03-F13</depends_on>
  <estimated_hours>2</estimated_hours>
</metadata>

<context>
Implement batch queue and request types for asynchronous batching.
The batch system allows multiple embedding requests to be collected
and processed together for improved GPU utilization and throughput.

Key components:
- BatchRequest: Individual embedding request with response channel
- BatchQueue: Collection of pending requests per model
- Batch: Assembled batch ready for processing

These types form the foundation for the BatchProcessor (M03-L17).
</context>

<definition_of_done>
  <signatures>
```rust
use tokio::sync::oneshot;
use uuid::Uuid;
use std::time::Instant;

/// Individual embedding request submitted to the batch system
#[derive(Debug)]
pub struct BatchRequest {
    /// Unique request identifier
    pub id: Uuid,
    /// Input to embed
    pub input: ModelInput,
    /// Target model for embedding
    pub model_id: ModelId,
    /// Channel for returning result
    pub response_tx: oneshot::Sender<EmbeddingResult<ModelEmbedding>>,
    /// Timestamp when request was submitted
    pub submitted_at: Instant,
    /// Priority level (higher = more urgent)
    pub priority: u8,
}

impl BatchRequest {
    pub fn new(
        input: ModelInput,
        model_id: ModelId,
    ) -> (Self, oneshot::Receiver<EmbeddingResult<ModelEmbedding>>);

    /// Time elapsed since submission
    pub fn elapsed(&self) -> std::time::Duration;

    /// Estimated token count for batching decisions
    pub fn estimated_tokens(&self) -> usize;
}

/// Queue of pending requests for a single model
#[derive(Debug)]
pub struct BatchQueue {
    /// Pending requests ordered by submission time
    requests: VecDeque<BatchRequest>,
    /// Configuration for batching behavior
    config: BatchConfig,
    /// Model this queue serves
    model_id: ModelId,
    /// Statistics
    stats: BatchQueueStats,
}

impl BatchQueue {
    pub fn new(model_id: ModelId, config: BatchConfig) -> Self;

    /// Add request to queue
    pub fn push(&mut self, request: BatchRequest);

    /// Check if queue should be flushed (batch ready)
    pub fn should_flush(&self) -> bool;

    /// Extract a batch of requests for processing
    pub fn drain_batch(&mut self) -> Option<Batch>;

    /// Number of pending requests
    pub fn len(&self) -> usize;

    /// Check if queue is empty
    pub fn is_empty(&self) -> bool;

    /// Oldest request wait time
    pub fn oldest_wait_time(&self) -> Option<std::time::Duration>;

    /// Clear all pending requests with error
    pub fn cancel_all(&mut self, error: EmbeddingError);
}

/// Assembled batch ready for processing
#[derive(Debug)]
pub struct Batch {
    /// Batch identifier
    pub id: Uuid,
    /// Model to use
    pub model_id: ModelId,
    /// Inputs in this batch
    pub inputs: Vec<ModelInput>,
    /// Response channels (same order as inputs)
    pub response_txs: Vec<oneshot::Sender<EmbeddingResult<ModelEmbedding>>>,
    /// Original request IDs
    pub request_ids: Vec<Uuid>,
    /// When batch was assembled
    pub assembled_at: Instant,
    /// Total tokens in batch (for padding estimation)
    pub total_tokens: usize,
}

impl Batch {
    pub fn new(model_id: ModelId) -> Self;

    /// Add a request to the batch
    pub fn add(&mut self, request: BatchRequest);

    /// Number of items in batch
    pub fn len(&self) -> usize;

    /// Send results back to requesters
    pub fn complete(self, results: Vec<EmbeddingResult<ModelEmbedding>>);

    /// Send error to all requesters
    pub fn fail(self, error: EmbeddingError);
}

/// Queue statistics for monitoring
#[derive(Debug, Default, Clone)]
pub struct BatchQueueStats {
    pub requests_received: AtomicU64,
    pub batches_processed: AtomicU64,
    pub requests_completed: AtomicU64,
    pub requests_failed: AtomicU64,
    pub total_wait_time_us: AtomicU64,
    pub avg_batch_size: AtomicU64,
}

impl BatchQueueStats {
    pub fn record_request(&self);
    pub fn record_batch(&self, size: usize, wait_time_us: u64);
    pub fn record_completion(&self, success: bool);
    pub fn summary(&self) -> BatchQueueSummary;
}
```
  </signatures>

  <constraints>
    <constraint>BatchRequest uses oneshot channel for async response</constraint>
    <constraint>UUID for unique request identification</constraint>
    <constraint>Priority support for request ordering</constraint>
    <constraint>should_flush() respects max_batch_size and max_wait_ms</constraint>
    <constraint>drain_batch() optionally sorts by sequence length</constraint>
    <constraint>Batch::complete() sends results in correct order</constraint>
    <constraint>cancel_all() properly cleans up on shutdown</constraint>
    <constraint>Thread-safe statistics with atomics</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/batch/types.rs</file>
  <file>crates/context-graph-embeddings/src/batch/mod.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>BatchRequest::new() returns request and receiver</criterion>
  <criterion>BatchQueue::push() adds request correctly</criterion>
  <criterion>should_flush() returns true at max_batch_size</criterion>
  <criterion>should_flush() returns true after max_wait_ms</criterion>
  <criterion>drain_batch() returns None when empty</criterion>
  <criterion>Batch::complete() sends to all receivers</criterion>
  <criterion>Statistics are correctly updated</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Request Creation Pattern
```rust
impl BatchRequest {
    pub fn new(
        input: ModelInput,
        model_id: ModelId,
    ) -> (Self, oneshot::Receiver<EmbeddingResult<ModelEmbedding>>) {
        let (tx, rx) = oneshot::channel();
        let request = Self {
            id: Uuid::new_v4(),
            input,
            model_id,
            response_tx: tx,
            submitted_at: Instant::now(),
            priority: 0,
        };
        (request, rx)
    }

    pub fn estimated_tokens(&self) -> usize {
        match &self.input {
            ModelInput::Text { content, .. } => {
                // Rough estimate: 4 chars per token
                content.len() / 4 + 1
            }
            ModelInput::Code { content, .. } => {
                // Code is often more token-dense
                content.len() / 3 + 1
            }
            _ => 100,  // Default for non-text
        }
    }
}
```

### Batch Flush Logic
```rust
impl BatchQueue {
    pub fn should_flush(&self) -> bool {
        if self.requests.is_empty() {
            return false;
        }

        // Flush if reached max batch size
        if self.requests.len() >= self.config.max_batch_size {
            return true;
        }

        // Flush if oldest request waited too long
        if let Some(oldest) = self.requests.front() {
            if oldest.elapsed().as_millis() as u64 >= self.config.max_wait_ms {
                return true;
            }
        }

        false
    }
}
```

### Batch Assembly with Length Sorting
```rust
impl BatchQueue {
    pub fn drain_batch(&mut self) -> Option<Batch> {
        if self.requests.is_empty() {
            return None;
        }

        let batch_size = self.requests.len().min(self.config.max_batch_size);
        let mut batch = Batch::new(self.model_id);

        // Drain requests
        let mut requests: Vec<BatchRequest> = self.requests
            .drain(..batch_size)
            .collect();

        // Optionally sort by length for padding efficiency
        if self.config.sort_by_length {
            requests.sort_by_key(|r| r.estimated_tokens());
        }

        for request in requests {
            batch.add(request);
        }

        // Update statistics
        let avg_wait = batch.request_ids.iter()
            .map(|_| batch.assembled_at.elapsed())
            .sum::<std::time::Duration>() / batch.len() as u32;
        self.stats.record_batch(batch.len(), avg_wait.as_micros() as u64);

        Some(batch)
    }
}
```

### Result Distribution
```rust
impl Batch {
    pub fn complete(self, results: Vec<EmbeddingResult<ModelEmbedding>>) {
        assert_eq!(self.response_txs.len(), results.len());

        for (tx, result) in self.response_txs.into_iter().zip(results.into_iter()) {
            // Ignore send errors (receiver may have dropped)
            let _ = tx.send(result);
        }
    }

    pub fn fail(self, error: EmbeddingError) {
        for tx in self.response_txs {
            let _ = tx.send(Err(error.clone()));
        }
    }
}
```
