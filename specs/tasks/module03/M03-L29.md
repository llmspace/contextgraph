# Task Specification: M03-L29

```xml
<task_spec id="M03-L29" version="1.0">
<metadata>
  <title>Shared Tokenizer Cache (TokenizationManager)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>29</sequence>
  <implements>
    - PRD: <200ms P95 latency requirement
    - Performance: Eliminate redundant tokenization across models
    - Architecture: Shared tokenizer families (BERT, MiniLM, etc.)
  </implements>
  <depends_on>M03-F06, M03-F01, M03-L01</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Many of the 12 embedding models share common base architectures and tokenizers:
- E8 (Graph/MiniLM), E11 (Entity/MiniLM) share paraphrase-MiniLM tokenizer
- E10 (Multimodal/CLIP), E7 (Code/CodeBERT) share similar BERT-base tokenizers
- E1 (Semantic/E5), E5 (Causal/Longformer) share similar tokenization patterns

If we tokenize the same input string 12 times independently, we waste 20-40ms of
the 200ms budget on CPU work before any GPU tensor operations begin.

The TokenizationManager:
1. Groups models by "Tokenizer Family" (shared tokenizer architecture)
2. Tokenizes input once per family
3. Caches the Encoding object for reuse across models in that family
4. Handles model-specific max_length and padding requirements
5. Avoids re-tokenization for cache hits on repeated inputs

Performance targets:
- First tokenization: <5ms per unique input
- Cache hit: <100μs
- Memory overhead: <10MB for tokenizer cache
- Deduplication: 12 models → 5-6 unique tokenizations
</context>

<definition_of_done>
  <signatures>
```rust
use tokenizers::{Encoding, Tokenizer};
use std::sync::Arc;

/// Tokenizer family grouping models with compatible tokenizers
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum TokenizerFamily {
    /// E5-large tokenizer (E1: Semantic)
    E5Large,
    /// Longformer tokenizer (E5: Causal)
    Longformer,
    /// SPLADE tokenizer (E6: Sparse)
    Splade,
    /// CodeBERT tokenizer (E7: Code)
    CodeBert,
    /// MiniLM tokenizers (E8: Graph, E11: Entity)
    MiniLM,
    /// CLIP tokenizer (E10: Multimodal)
    Clip,
    /// ColBERT tokenizer (E12: Late-Interaction)
    ColBert,
    /// No tokenizer needed (E2, E3, E4: Temporal, E9: HDC)
    None,
}

impl TokenizerFamily {
    /// Get family for a model ID
    pub fn from_model_id(model_id: ModelId) -> Self;

    /// Get all models that use this tokenizer family
    pub fn models(&self) -> Vec<ModelId>;

    /// Get HuggingFace tokenizer identifier
    pub fn tokenizer_id(&self) -> Option<&'static str>;
}

/// Model-specific tokenization requirements
#[derive(Debug, Clone)]
pub struct TokenizationConfig {
    /// Maximum sequence length for this model
    pub max_length: usize,
    /// Padding strategy
    pub padding: PaddingStrategy,
    /// Truncation strategy
    pub truncation: TruncationStrategy,
    /// Add special tokens ([CLS], [SEP], etc.)
    pub add_special_tokens: bool,
}

/// Padding strategies
#[derive(Debug, Clone, Copy)]
pub enum PaddingStrategy {
    /// No padding
    None,
    /// Pad to max_length
    MaxLength,
    /// Pad to longest in batch
    Longest,
}

/// Truncation strategies
#[derive(Debug, Clone, Copy)]
pub enum TruncationStrategy {
    /// No truncation (error if too long)
    None,
    /// Truncate from end
    TruncateEnd,
    /// Truncate from beginning
    TruncateStart,
}

/// Cached tokenization result
#[derive(Debug, Clone)]
pub struct CachedEncoding {
    /// The raw encoding from tokenizer
    pub encoding: Arc<Encoding>,
    /// Family this encoding belongs to
    pub family: TokenizerFamily,
    /// Input hash for cache key
    pub input_hash: u64,
    /// Token count
    pub token_count: usize,
}

/// Manager for shared tokenization across models
pub struct TokenizationManager {
    /// Loaded tokenizers by family
    tokenizers: HashMap<TokenizerFamily, Arc<Tokenizer>>,
    /// Model-specific configs
    model_configs: HashMap<ModelId, TokenizationConfig>,
    /// LRU cache for recent encodings
    cache: LruCache<(TokenizerFamily, u64), CachedEncoding>,
    /// Metrics
    metrics: TokenizationMetrics,
}

impl TokenizationManager {
    /// Create manager and load all required tokenizers
    pub async fn new(cache_size: usize) -> Result<Self, TokenizerError>;

    /// Load a specific tokenizer family
    pub async fn load_tokenizer(&mut self, family: TokenizerFamily) -> Result<(), TokenizerError>;

    /// Tokenize input for a specific model
    pub fn tokenize(
        &self,
        input: &str,
        model_id: ModelId,
    ) -> Result<CachedEncoding, TokenizerError>;

    /// Tokenize batch for a specific model
    pub fn tokenize_batch(
        &self,
        inputs: &[&str],
        model_id: ModelId,
    ) -> Result<Vec<CachedEncoding>, TokenizerError>;

    /// Tokenize once for all models in a family
    pub fn tokenize_for_family(
        &self,
        input: &str,
        family: TokenizerFamily,
    ) -> Result<CachedEncoding, TokenizerError>;

    /// Get pre-tokenized encoding for model (from family cache)
    pub fn get_encoding(
        &self,
        input_hash: u64,
        model_id: ModelId,
    ) -> Option<CachedEncoding>;

    /// Prepare tokenized input for specific model's requirements
    pub fn prepare_for_model(
        &self,
        encoding: &CachedEncoding,
        model_id: ModelId,
    ) -> Result<PreparedInput, TokenizerError>;

    /// Get tokenization metrics
    pub fn metrics(&self) -> &TokenizationMetrics;

    /// Clear cache
    pub fn clear_cache(&mut self);
}

/// Input prepared for a specific model
#[derive(Debug, Clone)]
pub struct PreparedInput {
    /// Input IDs tensor
    pub input_ids: Vec<i64>,
    /// Attention mask
    pub attention_mask: Vec<i64>,
    /// Token type IDs (for BERT-style models)
    pub token_type_ids: Option<Vec<i64>>,
    /// Actual sequence length (before padding)
    pub sequence_length: usize,
}

/// Tokenization metrics
#[derive(Debug, Default)]
pub struct TokenizationMetrics {
    /// Total tokenization requests
    pub total_requests: AtomicU64,
    /// Cache hits
    pub cache_hits: AtomicU64,
    /// Cache misses (actual tokenization)
    pub cache_misses: AtomicU64,
    /// Family deduplication hits
    pub family_reuse: AtomicU64,
    /// Total tokenization time (microseconds)
    pub total_time_us: AtomicU64,
    /// Average tokens per input
    pub avg_tokens: AtomicU64,
}

/// Tokenizer-related errors
#[derive(Debug, thiserror::Error)]
pub enum TokenizerError {
    #[error("Failed to load tokenizer for {0:?}: {1}")]
    LoadFailed(TokenizerFamily, String),

    #[error("Input exceeds max length {max} for model {model:?}: got {got}")]
    InputTooLong { model: ModelId, max: usize, got: usize },

    #[error("Tokenizer not loaded for family {0:?}")]
    NotLoaded(TokenizerFamily),

    #[error("Tokenization failed: {0}")]
    TokenizationFailed(String),
}
```
  </signatures>

  <constraints>
    <constraint>Tokenizers loaded lazily on first use</constraint>
    <constraint>Cache size configurable (default 1000 entries)</constraint>
    <constraint>Thread-safe for concurrent tokenization</constraint>
    <constraint>Memory-mapped tokenizer files where possible</constraint>
    <constraint>No tokenization for temporal/HDC models (None family)</constraint>
    <constraint>Model-specific max_length respected</constraint>
    <constraint>Hash-based cache keys for O(1) lookup</constraint>
    <constraint>Metrics updated atomically</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/tokenization/mod.rs</file>
  <file>crates/context-graph-embeddings/src/tokenization/manager.rs</file>
  <file>crates/context-graph-embeddings/src/tokenization/families.rs</file>
  <file>crates/context-graph-embeddings/src/tokenization/cache.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>All tokenizers load successfully</criterion>
  <criterion>Cache hits return in <100μs</criterion>
  <criterion>First tokenization completes in <5ms</criterion>
  <criterion>Same input reuses encoding across family</criterion>
  <criterion>Model-specific max_length enforced</criterion>
  <criterion>Metrics accurately track hit rates</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Tokenizer Family Mapping
```rust
impl TokenizerFamily {
    pub fn from_model_id(model_id: ModelId) -> Self {
        match model_id {
            ModelId::Semantic => Self::E5Large,           // E1
            ModelId::TemporalRecent => Self::None,        // E2 - custom
            ModelId::TemporalPeriodic => Self::None,      // E3 - custom
            ModelId::TemporalPositional => Self::None,    // E4 - custom
            ModelId::Causal => Self::Longformer,          // E5
            ModelId::Sparse => Self::Splade,              // E6
            ModelId::Code => Self::CodeBert,              // E7
            ModelId::Graph => Self::MiniLM,               // E8
            ModelId::Hdc => Self::None,                   // E9 - custom
            ModelId::Multimodal => Self::Clip,            // E10
            ModelId::Entity => Self::MiniLM,              // E11 - shares with E8
            ModelId::LateInteraction => Self::ColBert,    // E12
        }
    }

    pub fn models(&self) -> Vec<ModelId> {
        match self {
            Self::E5Large => vec![ModelId::Semantic],
            Self::Longformer => vec![ModelId::Causal],
            Self::Splade => vec![ModelId::Sparse],
            Self::CodeBert => vec![ModelId::Code],
            Self::MiniLM => vec![ModelId::Graph, ModelId::Entity],
            Self::Clip => vec![ModelId::Multimodal],
            Self::ColBert => vec![ModelId::LateInteraction],
            Self::None => vec![
                ModelId::TemporalRecent,
                ModelId::TemporalPeriodic,
                ModelId::TemporalPositional,
                ModelId::Hdc,
            ],
        }
    }

    pub fn tokenizer_id(&self) -> Option<&'static str> {
        match self {
            Self::E5Large => Some("intfloat/e5-large-v2"),
            Self::Longformer => Some("allenai/longformer-base-4096"),
            Self::Splade => Some("naver/splade-cocondenser-ensembledistil"),
            Self::CodeBert => Some("microsoft/codebert-base"),
            Self::MiniLM => Some("sentence-transformers/paraphrase-MiniLM-L6-v2"),
            Self::Clip => Some("openai/clip-vit-large-patch14"),
            Self::ColBert => Some("colbert-ir/colbertv2.0"),
            Self::None => None,
        }
    }
}
```

### Model-Specific Configurations
```rust
impl TokenizationManager {
    fn default_model_configs() -> HashMap<ModelId, TokenizationConfig> {
        let mut configs = HashMap::new();

        // E1: Semantic (E5-large)
        configs.insert(ModelId::Semantic, TokenizationConfig {
            max_length: 512,
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        // E5: Causal (Longformer)
        configs.insert(ModelId::Causal, TokenizationConfig {
            max_length: 4096,
            padding: PaddingStrategy::Longest,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        // E6: Sparse (SPLADE)
        configs.insert(ModelId::Sparse, TokenizationConfig {
            max_length: 256,
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        // E7: Code (CodeBERT)
        configs.insert(ModelId::Code, TokenizationConfig {
            max_length: 512,
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        // E8, E11: Graph, Entity (MiniLM)
        let minilm_config = TokenizationConfig {
            max_length: 128,
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        };
        configs.insert(ModelId::Graph, minilm_config.clone());
        configs.insert(ModelId::Entity, minilm_config);

        // E10: Multimodal (CLIP)
        configs.insert(ModelId::Multimodal, TokenizationConfig {
            max_length: 77,  // CLIP's fixed context length
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        // E12: Late-Interaction (ColBERT)
        configs.insert(ModelId::LateInteraction, TokenizationConfig {
            max_length: 256,
            padding: PaddingStrategy::MaxLength,
            truncation: TruncationStrategy::TruncateEnd,
            add_special_tokens: true,
        });

        configs
    }
}
```

### Cache Key Generation
```rust
use std::hash::{Hash, Hasher};
use std::collections::hash_map::DefaultHasher;

fn compute_input_hash(input: &str) -> u64 {
    let mut hasher = DefaultHasher::new();
    input.hash(&mut hasher);
    hasher.finish()
}

impl TokenizationManager {
    pub fn tokenize_for_family(
        &self,
        input: &str,
        family: TokenizerFamily,
    ) -> Result<CachedEncoding, TokenizerError> {
        let input_hash = compute_input_hash(input);
        let cache_key = (family, input_hash);

        // Check cache first
        if let Some(cached) = self.cache.get(&cache_key) {
            self.metrics.cache_hits.fetch_add(1, Ordering::Relaxed);
            return Ok(cached.clone());
        }
        self.metrics.cache_misses.fetch_add(1, Ordering::Relaxed);

        // Get tokenizer for family
        let tokenizer = self.tokenizers.get(&family)
            .ok_or(TokenizerError::NotLoaded(family))?;

        // Tokenize
        let start = Instant::now();
        let encoding = tokenizer.encode(input, true)
            .map_err(|e| TokenizerError::TokenizationFailed(e.to_string()))?;
        let elapsed = start.elapsed();

        self.metrics.total_time_us.fetch_add(
            elapsed.as_micros() as u64,
            Ordering::Relaxed
        );

        let cached = CachedEncoding {
            encoding: Arc::new(encoding),
            family,
            input_hash,
            token_count: encoding.len(),
        };

        // Store in cache
        self.cache.put(cache_key, cached.clone());

        Ok(cached)
    }
}
```

### Preparation for Model-Specific Requirements
```rust
impl TokenizationManager {
    pub fn prepare_for_model(
        &self,
        encoding: &CachedEncoding,
        model_id: ModelId,
    ) -> Result<PreparedInput, TokenizerError> {
        let config = self.model_configs.get(&model_id)
            .unwrap_or(&TokenizationConfig::default());

        let ids = encoding.encoding.get_ids();
        let attention = encoding.encoding.get_attention_mask();
        let type_ids = encoding.encoding.get_type_ids();

        // Apply truncation
        let len = ids.len().min(config.max_length);
        let mut input_ids: Vec<i64> = ids[..len].iter().map(|&x| x as i64).collect();
        let mut attention_mask: Vec<i64> = attention[..len].iter().map(|&x| x as i64).collect();
        let sequence_length = len;

        // Apply padding
        match config.padding {
            PaddingStrategy::MaxLength => {
                input_ids.resize(config.max_length, 0);
                attention_mask.resize(config.max_length, 0);
            }
            PaddingStrategy::Longest => {
                // No-op for single input; batch handling would find max
            }
            PaddingStrategy::None => {}
        }

        // Token type IDs (optional)
        let token_type_ids = if type_ids.is_empty() {
            None
        } else {
            let mut tids: Vec<i64> = type_ids[..len].iter().map(|&x| x as i64).collect();
            if matches!(config.padding, PaddingStrategy::MaxLength) {
                tids.resize(config.max_length, 0);
            }
            Some(tids)
        };

        Ok(PreparedInput {
            input_ids,
            attention_mask,
            token_type_ids,
            sequence_length,
        })
    }
}
```

### Integration with Models
```rust
impl EmbeddingPipeline {
    pub async fn embed(&self, input: &str) -> EmbeddingResult<FusedEmbedding> {
        // Pre-tokenize for all families
        let mut family_encodings: HashMap<TokenizerFamily, CachedEncoding> = HashMap::new();

        for family in TokenizerFamily::all_needed() {
            let encoding = self.tokenization_manager.tokenize_for_family(input, family)?;
            family_encodings.insert(family, encoding);
        }

        // Spawn model tasks with pre-tokenized input
        let mut join_set = JoinSet::new();
        for model_id in ModelId::iter() {
            let family = TokenizerFamily::from_model_id(model_id);

            // Get prepared input for this model
            let prepared = match family_encodings.get(&family) {
                Some(encoding) => {
                    self.metrics.family_reuse.fetch_add(1, Ordering::Relaxed);
                    Some(self.tokenization_manager.prepare_for_model(encoding, model_id)?)
                }
                None => None, // Custom models (temporal, HDC)
            };

            let model = self.registry.get(model_id)?;
            join_set.spawn(async move {
                model.embed_prepared(prepared.as_ref()).await
            });
        }

        // ... rest of pipeline
    }
}
```

---
*Task ID: M03-L29*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
