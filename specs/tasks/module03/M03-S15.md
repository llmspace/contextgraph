# Task Specification: M03-S15

```xml
<task_spec id="M03-S15" version="1.0">
<metadata>
  <title>GPU Direct Storage (GDS) Integration</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>45</sequence>
  <implements>
    - RTX 5090 Technical Report Section 3.3: GPU Direct Storage
    - PRD Roadmap Phase 7: NVMe→GPU DMA bypass
    - Constitution: cuda.memory.gds
  </implements>
  <depends_on>M03-S04, M03-S05</depends_on>
  <estimated_hours>6</estimated_hours>
</metadata>

<context>
Implement GPU Direct Storage (GDS) for high-bandwidth NVMe→GPU transfers that
bypass the CPU. This is critical for loading the 32GB of model weights onto the
RTX 5090 at peak speed during initialization and for hydrating vector caches.

Key features:
1. cuFile API integration for DMA bypass
2. Asynchronous model loading directly to GPU VRAM
3. Vector cache hydration from NVMe storage
4. O_DIRECT file access for maximum bandwidth
5. Batched prefetch for predictive loading

Performance targets:
- Model loading: 25+ GB/s sustained (vs ~6 GB/s via CPU)
- Vector hydration: 20+ GB/s for cache warm-up
- Latency: <100μs per 4KB block
</context>

<definition_of_done>
  <signatures>
```rust
/// GPU Direct Storage configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GdsConfig {
    /// Enable GDS (requires compatible NVMe + GPU)
    pub enabled: bool,
    /// Maximum concurrent DMA operations
    pub max_concurrent_ops: usize,
    /// Buffer alignment (typically 4KB for O_DIRECT)
    pub alignment: usize,
    /// Prefetch batch size in bytes
    pub prefetch_batch_size: usize,
    /// Thread pool size for async operations
    pub thread_pool_size: usize,
}

impl Default for GdsConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            max_concurrent_ops: 32,
            alignment: 4096,
            prefetch_batch_size: 64 * 1024 * 1024, // 64MB
            thread_pool_size: 4,
        }
    }
}

/// GPU Direct Storage manager
pub struct GpuDirectStorage {
    config: GdsConfig,
    device: Device,
    driver_handle: Option<CuFileDriverHandle>,
    active_ops: AtomicUsize,
    stats: GdsStats,
}

impl GpuDirectStorage {
    /// Initialize GDS with device and config
    pub fn new(device: &Device, config: GdsConfig) -> CudaResult<Self>;

    /// Check if GDS is available on this system
    pub fn is_available() -> bool;

    /// Open file for direct GPU access
    pub fn open_file(&self, path: impl AsRef<Path>) -> CudaResult<GdsFile>;

    /// Load model weights directly to GPU buffer
    pub async fn load_to_gpu(
        &self,
        file: &GdsFile,
        offset: u64,
        gpu_buffer: &mut GpuBuffer,
    ) -> CudaResult<usize>;

    /// Batch load multiple model files to GPU
    pub async fn load_models_batch(
        &self,
        model_paths: &[PathBuf],
        gpu_memory: &GpuMemoryPool,
    ) -> CudaResult<HashMap<PathBuf, GpuBuffer>>;

    /// Prefetch vectors from storage to GPU cache
    pub async fn prefetch_vectors(
        &self,
        vector_file: &GdsFile,
        indices: &[u64],
        vector_dim: usize,
        gpu_buffer: &mut GpuBuffer,
    ) -> CudaResult<usize>;

    /// Read vectors by index using GDS
    pub async fn read_vectors(
        &self,
        vector_file: &GdsFile,
        start_idx: u64,
        count: usize,
        vector_dim: usize,
    ) -> CudaResult<GpuBuffer>;

    /// Get transfer statistics
    pub fn stats(&self) -> GdsStatsSnapshot;

    /// Reset statistics
    pub fn reset_stats(&self);
}

/// File handle for GPU Direct access
pub struct GdsFile {
    path: PathBuf,
    file: File,
    cu_file: CuFileHandle,
    size: u64,
}

impl GdsFile {
    /// Get file size in bytes
    pub fn size(&self) -> u64;

    /// Check if file is properly aligned for GDS
    pub fn is_aligned(&self) -> bool;
}

/// GDS transfer statistics
#[derive(Debug, Clone, Default)]
pub struct GdsStats {
    pub bytes_transferred: AtomicU64,
    pub operations_completed: AtomicU64,
    pub operations_failed: AtomicU64,
    pub peak_bandwidth_gbps: AtomicU64,
    pub avg_latency_us: AtomicU64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GdsStatsSnapshot {
    pub bytes_transferred: u64,
    pub operations_completed: u64,
    pub operations_failed: u64,
    pub peak_bandwidth_gbps: f64,
    pub avg_latency_us: u64,
    pub effective_bandwidth_gbps: f64,
}
```
  </signatures>

  <constraints>
    <constraint>Requires NVIDIA cuFile driver (GDS 1.0+)</constraint>
    <constraint>Requires compatible NVMe SSD with GDS support</constraint>
    <constraint>Buffer alignment must be 4KB (O_DIRECT requirement)</constraint>
    <constraint>GPU buffer must be registered with cuFileBufRegister</constraint>
    <constraint>File must be opened with O_DIRECT flag</constraint>
    <constraint>Thread-safe for concurrent DMA operations</constraint>
    <constraint>Graceful fallback to mmap if GDS unavailable</constraint>
    <constraint>Memory pool integration for buffer management</constraint>
    <constraint>Async/await compatible for pipeline integration</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-cuda/src/gds/mod.rs</file>
  <file>crates/context-graph-cuda/src/gds/config.rs</file>
  <file>crates/context-graph-cuda/src/gds/driver.rs</file>
  <file>crates/context-graph-cuda/src/gds/file.rs</file>
  <file>crates/context-graph-cuda/src/gds/stats.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>GpuDirectStorage::is_available() correctly detects GDS support</criterion>
  <criterion>Fallback to standard I/O when GDS unavailable</criterion>
  <criterion>Buffer alignment enforced (4KB boundary)</criterion>
  <criterion>load_to_gpu transfers data to correct GPU buffer</criterion>
  <criterion>Statistics accurately track bandwidth and latency</criterion>
  <criterion>Thread-safe concurrent operations</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### cuFile API Integration
```rust
// FFI bindings to NVIDIA cuFile
#[repr(C)]
pub struct CuFileHandle {
    handle: *mut std::ffi::c_void,
}

#[repr(C)]
pub struct CuFileDriverHandle {
    handle: *mut std::ffi::c_void,
}

extern "C" {
    fn cuFileDriverOpen() -> i32;
    fn cuFileDriverClose() -> i32;
    fn cuFileHandleRegister(
        fh: *mut CuFileHandle,
        descr: *const CuFileDescr,
    ) -> i32;
    fn cuFileHandleDeregister(fh: CuFileHandle) -> ();
    fn cuFileBufRegister(
        devPtr: *mut std::ffi::c_void,
        size: usize,
        flags: u32,
    ) -> i32;
    fn cuFileBufDeregister(devPtr: *mut std::ffi::c_void) -> i32;
    fn cuFileRead(
        fh: CuFileHandle,
        devPtr: *mut std::ffi::c_void,
        size: usize,
        file_offset: i64,
        devPtr_offset: i64,
    ) -> isize;
    fn cuFileWrite(
        fh: CuFileHandle,
        devPtr: *const std::ffi::c_void,
        size: usize,
        file_offset: i64,
        devPtr_offset: i64,
    ) -> isize;
}
```

### GDS Initialization Flow
```rust
impl GpuDirectStorage {
    pub fn new(device: &Device, config: GdsConfig) -> CudaResult<Self> {
        // Check GDS availability
        if !Self::is_available() {
            return Ok(Self {
                config,
                device: device.clone(),
                driver_handle: None,
                active_ops: AtomicUsize::new(0),
                stats: GdsStats::default(),
            });
        }

        // Initialize cuFile driver
        unsafe {
            let status = cuFileDriverOpen();
            if status != 0 {
                warn!("cuFileDriverOpen failed: {}, falling back to standard I/O", status);
                return Ok(Self::without_gds(device.clone(), config));
            }
        }

        Ok(Self {
            config,
            device: device.clone(),
            driver_handle: Some(CuFileDriverHandle { handle: std::ptr::null_mut() }),
            active_ops: AtomicUsize::new(0),
            stats: GdsStats::default(),
        })
    }

    pub fn is_available() -> bool {
        // Check for cuFile library
        #[cfg(target_os = "linux")]
        {
            std::path::Path::new("/usr/lib/x86_64-linux-gnu/libcufile.so").exists()
                || std::path::Path::new("/usr/local/cuda/lib64/libcufile.so").exists()
        }
        #[cfg(not(target_os = "linux"))]
        {
            false // GDS only available on Linux
        }
    }
}
```

### Async Model Loading
```rust
impl GpuDirectStorage {
    pub async fn load_to_gpu(
        &self,
        file: &GdsFile,
        offset: u64,
        gpu_buffer: &mut GpuBuffer,
    ) -> CudaResult<usize> {
        let start = Instant::now();

        // Register GPU buffer with cuFile
        unsafe {
            let status = cuFileBufRegister(
                gpu_buffer.as_ptr(),
                gpu_buffer.size(),
                0,
            );
            if status != 0 {
                return Err(CudaError::GdsError(format!(
                    "cuFileBufRegister failed: {}", status
                )));
            }
        }

        // Perform DMA transfer
        let bytes_read = unsafe {
            cuFileRead(
                file.cu_file,
                gpu_buffer.as_ptr(),
                gpu_buffer.size(),
                offset as i64,
                0,
            )
        };

        // Deregister buffer
        unsafe {
            cuFileBufDeregister(gpu_buffer.as_ptr());
        }

        if bytes_read < 0 {
            self.stats.operations_failed.fetch_add(1, Ordering::Relaxed);
            return Err(CudaError::GdsError(format!(
                "cuFileRead failed: {}", bytes_read
            )));
        }

        // Update statistics
        let elapsed = start.elapsed();
        let bytes = bytes_read as u64;
        self.stats.bytes_transferred.fetch_add(bytes, Ordering::Relaxed);
        self.stats.operations_completed.fetch_add(1, Ordering::Relaxed);

        let bandwidth_gbps = (bytes as f64 * 8.0) / elapsed.as_secs_f64() / 1e9;
        let current_peak = self.stats.peak_bandwidth_gbps.load(Ordering::Relaxed);
        if (bandwidth_gbps * 1000.0) as u64 > current_peak {
            self.stats.peak_bandwidth_gbps.store(
                (bandwidth_gbps * 1000.0) as u64,
                Ordering::Relaxed,
            );
        }

        Ok(bytes_read as usize)
    }
}
```

### Batch Model Loading
```rust
impl GpuDirectStorage {
    pub async fn load_models_batch(
        &self,
        model_paths: &[PathBuf],
        gpu_memory: &GpuMemoryPool,
    ) -> CudaResult<HashMap<PathBuf, GpuBuffer>> {
        let mut results = HashMap::new();
        let mut handles = Vec::new();

        // Open all files and prepare transfers
        for path in model_paths {
            let file = self.open_file(path)?;
            let buffer = gpu_memory.allocate(file.size() as usize)?;
            handles.push((path.clone(), file, buffer));
        }

        // Execute transfers concurrently (up to max_concurrent_ops)
        let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent_ops));

        let tasks: Vec<_> = handles.into_iter().map(|(path, file, mut buffer)| {
            let sem = semaphore.clone();
            let gds = self.clone();

            tokio::spawn(async move {
                let _permit = sem.acquire().await.unwrap();
                gds.load_to_gpu(&file, 0, &mut buffer).await?;
                Ok::<_, CudaError>((path, buffer))
            })
        }).collect();

        for task in tasks {
            let (path, buffer) = task.await.map_err(|e| CudaError::AsyncError(e.to_string()))??;
            results.insert(path, buffer);
        }

        Ok(results)
    }
}
```

### Vector Prefetch for Cache Hydration
```rust
impl GpuDirectStorage {
    pub async fn prefetch_vectors(
        &self,
        vector_file: &GdsFile,
        indices: &[u64],
        vector_dim: usize,
        gpu_buffer: &mut GpuBuffer,
    ) -> CudaResult<usize> {
        let vector_size = vector_dim * std::mem::size_of::<f32>();
        let mut total_bytes = 0;

        // Sort indices for sequential access pattern
        let mut sorted_indices = indices.to_vec();
        sorted_indices.sort();

        // Batch contiguous reads
        let mut batch_start = sorted_indices[0];
        let mut batch_count = 1;
        let mut offset_in_buffer = 0;

        for i in 1..sorted_indices.len() {
            if sorted_indices[i] == sorted_indices[i-1] + 1 {
                batch_count += 1;
            } else {
                // Read current batch
                let file_offset = batch_start * vector_size as u64;
                let read_size = batch_count * vector_size;

                total_bytes += self.read_chunk(
                    vector_file,
                    file_offset,
                    gpu_buffer,
                    offset_in_buffer,
                    read_size,
                ).await?;

                offset_in_buffer += read_size;
                batch_start = sorted_indices[i];
                batch_count = 1;
            }
        }

        // Read final batch
        let file_offset = batch_start * vector_size as u64;
        let read_size = batch_count * vector_size;
        total_bytes += self.read_chunk(
            vector_file,
            file_offset,
            gpu_buffer,
            offset_in_buffer,
            read_size,
        ).await?;

        Ok(total_bytes)
    }
}
```

---
*Task ID: M03-S15*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
