# M03-L12: Multimodal Model (E10 - CLIP)

```xml
<task_spec id="M03-L12" version="1.0">
<metadata>
  <title>Implement Multimodal Model using CLIP</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>12</sequence>
  <implements>PRD E10 - Multimodal embedding with cross-attention</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement multimodal embedding using openai/clip-vit-large-patch14.
CLIP (Contrastive Language-Image Pre-training) provides a shared embedding
space for both text and images, enabling cross-modal similarity search.

Supports both Text and Image input types:
- Text: Uses CLIP text encoder (transformer)
- Image: Uses CLIP vision encoder (ViT-L/14)

Output: 768D dense vector.
Max tokens: 77 (CLIP's context length limit).
</context>

<definition_of_done>
  <signatures>
```rust
pub struct MultimodalModel {
    text_encoder: ClipTextModel,
    vision_encoder: ClipVisionModel,
    text_tokenizer: Tokenizer,
    image_processor: ImageProcessor,
    device: Device,
    loaded: AtomicBool,
    config: SingleModelConfig,
}

pub struct ImageProcessor {
    /// Target size for CLIP ViT-L/14
    target_size: (u32, u32),  // (224, 224)
    /// Normalization mean (ImageNet stats)
    mean: [f32; 3],
    /// Normalization std
    std: [f32; 3],
}

impl ImageProcessor {
    pub fn new() -> Self;
    pub fn process(&self, bytes: &[u8], format: ImageFormat) -> EmbeddingResult<Tensor>;
    pub fn resize_and_center_crop(&self, img: &DynamicImage) -> DynamicImage;
    pub fn normalize(&self, tensor: Tensor) -> Tensor;
}

impl MultimodalModel {
    pub fn new(models_dir: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Embed text using CLIP text encoder
    pub async fn embed_text(&self, text: &str) -> EmbeddingResult<Vec<f32>>;

    /// Embed image using CLIP vision encoder
    pub async fn embed_image(&self, bytes: &[u8], format: ImageFormat) -> EmbeddingResult<Vec<f32>>;

    /// Compute cosine similarity between text and image embeddings
    pub fn cross_modal_similarity(&self, text_emb: &[f32], image_emb: &[f32]) -> f32;
}

#[async_trait]
impl EmbeddingModel for MultimodalModel {
    fn model_id(&self) -> ModelId { ModelId::Multimodal }
    fn dimension(&self) -> usize { 768 }
    fn max_tokens(&self) -> usize { 77 }  // CLIP limit
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text, InputType::Image] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for CLIP inference</constraint>
    <constraint>Model loaded from models/multimodal/ directory</constraint>
    <constraint>Supports both Text and Image input types</constraint>
    <constraint>Uses CLIP text encoder for text inputs</constraint>
    <constraint>Uses CLIP vision encoder for image inputs</constraint>
    <constraint>dimension() returns 768</constraint>
    <constraint>max_tokens() returns 77 (CLIP context limit)</constraint>
    <constraint>Image preprocessing: resize to 224x224, center crop, normalize</constraint>
    <constraint>Text and image embeddings are in shared latent space</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/multimodal.rs</file>
  <file>crates/context-graph-embeddings/src/models/pretrained/image_processor.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>MultimodalModel::new() creates valid instance</criterion>
  <criterion>Model loads both text and vision encoder weights</criterion>
  <criterion>embed_text() produces 768D vector</criterion>
  <criterion>embed_image() produces 768D vector</criterion>
  <criterion>Text and image embeddings are L2 normalized</criterion>
  <criterion>Cross-modal similarity works correctly</criterion>
  <criterion>Supports PNG, JPEG, WebP image formats</criterion>
  <criterion>Memory usage includes both encoders</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `openai/clip-vit-large-patch14`
- **Text Encoder**: Transformer (12 layers, 768 hidden)
- **Vision Encoder**: ViT-L/14 (24 layers, 1024 hidden, projects to 768)
- **Output Dimension**: 768
- **Image Size**: 224x224 pixels
- **Patch Size**: 14x14

### Image Preprocessing Pipeline
```rust
impl ImageProcessor {
    pub fn new() -> Self {
        Self {
            target_size: (224, 224),
            mean: [0.48145466, 0.4578275, 0.40821073],  // CLIP stats
            std: [0.26862954, 0.26130258, 0.27577711],
        }
    }

    pub fn process(&self, bytes: &[u8], format: ImageFormat) -> EmbeddingResult<Tensor> {
        // 1. Decode image
        let img = image::load_from_memory(bytes)?;

        // 2. Resize (shortest side to 224)
        let resized = self.resize_shortest_side(&img, 224);

        // 3. Center crop to 224x224
        let cropped = self.center_crop(&resized, 224, 224);

        // 4. Convert to RGB tensor [C, H, W] in range [0, 1]
        let tensor = self.image_to_tensor(&cropped);

        // 5. Normalize with CLIP statistics
        self.normalize(tensor)
    }
}
```

### Embedding Dispatch
```rust
impl MultimodalModel {
    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding> {
        let start = Instant::now();

        let vector = match input {
            ModelInput::Text { content, .. } => {
                self.embed_text(content).await?
            }
            ModelInput::Image { bytes, format } => {
                self.embed_image(bytes, *format).await?
            }
            _ => return Err(EmbeddingError::UnsupportedModality {
                model_id: ModelId::Multimodal,
                input_type: InputType::from(input),
            }),
        };

        Ok(ModelEmbedding {
            model_id: ModelId::Multimodal,
            vector,
            latency_us: start.elapsed().as_micros() as u64,
            attention_weights: None,
        })
    }
}
```

### CLIP Text Encoding
Text is tokenized with special handling:
- Prepend [SOS] token
- Append [EOS] token
- Pad/truncate to 77 tokens
- Use final [EOS] token's hidden state as embedding
