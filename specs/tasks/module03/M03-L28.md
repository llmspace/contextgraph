# Task Specification: M03-L28

```xml
<task_spec id="M03-L28" version="1.0">
<metadata>
  <title>Blackwell-Specific Quantization Kernels (FP8/FP4)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>28</sequence>
  <implements>
    - RTX 5090 Technical Report: FP4 (NVFP4) Blackwell-exclusive optimization
    - PRD: <5ms latency targets for E5-Large and heavy models
    - Constitution: cuda.quantization.blackwell
  </implements>
  <depends_on>M03-L23, M03-S04</depends_on>
  <estimated_hours>5</estimated_hours>
</metadata>

<context>
Implement Blackwell-specific FP8 and FP4 quantization kernels for maximum throughput
on RTX 5090. Standard FP16 is insufficient to hit <5ms latency targets for heavier
models like E5-Large. FP8/FP4 requires specialized block-scaling logic using
E4M3/E5M2 formats with per-block scaling factors.

Key features:
1. FP8 (E4M3/E5M2) quantization with block scaling
2. FP4 (NVFP4) Blackwell-exclusive path for max throughput
3. Mixed-precision inference paths
4. Runtime format selection based on model and device capability
5. Dequantization for output compatibility

Performance targets:
- FP8: 2x throughput vs FP16
- FP4: 4x throughput vs FP16 (Blackwell only)
- Quantization overhead: <0.5ms per batch
</context>

<definition_of_done>
  <signatures>
```rust
/// Quantization format for Blackwell tensor cores
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum QuantFormat {
    /// Standard FP16 (baseline)
    FP16,
    /// FP8 E4M3 format (4-bit exponent, 3-bit mantissa)
    FP8E4M3,
    /// FP8 E5M2 format (5-bit exponent, 2-bit mantissa)
    FP8E5M2,
    /// FP4 NVFP4 (Blackwell exclusive, max throughput)
    FP4,
}

/// Block scaling metadata for quantized tensors
#[derive(Debug, Clone)]
pub struct BlockScale {
    /// Scaling factors per block
    pub scales: Vec<f32>,
    /// Block size used for quantization
    pub block_size: usize,
    /// Original tensor shape
    pub shape: Vec<usize>,
}

/// Quantized tensor representation
#[derive(Debug, Clone)]
pub struct QuantizedTensor {
    /// Quantized data bytes
    pub data: Vec<u8>,
    /// Format used
    pub format: QuantFormat,
    /// Block scaling metadata
    pub scale: BlockScale,
    /// Device placement
    pub device: Device,
}

pub struct BlackwellQuantizer {
    /// Supported formats for current device
    supported_formats: Vec<QuantFormat>,
    /// Default format to use
    default_format: QuantFormat,
    /// Device capability info
    device_caps: DeviceCapabilities,
}

impl BlackwellQuantizer {
    /// Create quantizer with device capability detection
    pub fn new(device: &Device) -> CudaResult<Self>;

    /// Check if Blackwell FP4 is available
    pub fn supports_fp4(&self) -> bool;

    /// Check if FP8 is available
    pub fn supports_fp8(&self) -> bool;

    /// Select optimal format for model and batch size
    pub fn select_format(&self, model_id: ModelId, batch_size: usize) -> QuantFormat;

    /// Quantize FP32 tensor to specified format
    pub fn quantize(
        &self,
        tensor: &Tensor,
        format: QuantFormat,
        block_size: usize,
    ) -> CudaResult<QuantizedTensor>;

    /// Quantize with automatic format selection
    pub fn quantize_auto(&self, tensor: &Tensor) -> CudaResult<QuantizedTensor>;

    /// Dequantize back to FP32
    pub fn dequantize(&self, quantized: &QuantizedTensor) -> CudaResult<Tensor>;

    /// Quantize model weights for inference
    pub fn quantize_model_weights(
        &self,
        weights: &HashMap<String, Tensor>,
        format: QuantFormat,
    ) -> CudaResult<HashMap<String, QuantizedTensor>>;

    /// Run quantized matrix multiplication
    pub fn quantized_matmul(
        &self,
        a: &QuantizedTensor,
        b: &QuantizedTensor,
    ) -> CudaResult<Tensor>;
}
```
  </signatures>

  <constraints>
    <constraint>FP4 only available on Blackwell (SM 10.0+)</constraint>
    <constraint>FP8 available on Ada Lovelace+ (SM 8.9+)</constraint>
    <constraint>Block size must be power of 2 (typically 32, 64, or 128)</constraint>
    <constraint>Scaling factors stored in FP32 for precision</constraint>
    <constraint>Quantization must be numerically stable (no overflow)</constraint>
    <constraint>Dequantization must preserve semantic meaning</constraint>
    <constraint>Thread-safe for concurrent model inference</constraint>
    <constraint>Graceful fallback to FP16 if FP8/FP4 unavailable</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-cuda/src/quantization/mod.rs</file>
  <file>crates/context-graph-cuda/src/quantization/formats.rs</file>
  <file>crates/context-graph-cuda/src/quantization/blackwell.rs</file>
  <file>crates/context-graph-cuda/src/quantization/kernels.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>BlackwellQuantizer::new() detects device capabilities</criterion>
  <criterion>FP8 quantize/dequantize round-trip preserves values within tolerance</criterion>
  <criterion>FP4 path only activates on Blackwell hardware</criterion>
  <criterion>Block scaling produces valid scale factors</criterion>
  <criterion>quantized_matmul produces correct output dimensions</criterion>
  <criterion>Graceful fallback on unsupported hardware</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### FP8 E4M3 Format
```
| S | EEEE | MMM |
 1    4       3    = 8 bits

- Sign: 1 bit
- Exponent: 4 bits (bias = 7)
- Mantissa: 3 bits
- Range: ±448
- Precision: ~0.1% relative error
```

### FP8 E5M2 Format
```
| S | EEEEE | MM |
 1    5        2   = 8 bits

- Sign: 1 bit
- Exponent: 5 bits (bias = 15)
- Mantissa: 2 bits
- Range: ±57344
- Precision: ~0.4% relative error
```

### FP4 NVFP4 Format (Blackwell)
```
| S | EE | M |
 1   2    1   = 4 bits

- Sign: 1 bit
- Exponent: 2 bits
- Mantissa: 1 bit
- Requires larger block sizes for accuracy
- 4x memory reduction, 4x throughput increase
```

### Block Scaling Algorithm
```rust
fn compute_block_scale(
    tensor: &[f32],
    block_size: usize,
    format: QuantFormat,
) -> (Vec<u8>, Vec<f32>) {
    let max_representable = match format {
        QuantFormat::FP8E4M3 => 448.0,
        QuantFormat::FP8E5M2 => 57344.0,
        QuantFormat::FP4 => 6.0,
        QuantFormat::FP16 => 65504.0,
    };

    let mut scales = Vec::new();
    let mut quantized = Vec::new();

    for block in tensor.chunks(block_size) {
        // Find max absolute value in block
        let max_abs = block.iter()
            .map(|x| x.abs())
            .fold(0.0f32, f32::max);

        // Compute scale factor
        let scale = if max_abs > 0.0 {
            max_representable / max_abs
        } else {
            1.0
        };
        scales.push(1.0 / scale);

        // Quantize each value
        for &val in block {
            let scaled = val * scale;
            let quantized_val = quantize_to_format(scaled, format);
            quantized.push(quantized_val);
        }
    }

    (quantized, scales)
}
```

### Device Capability Detection
```rust
impl BlackwellQuantizer {
    pub fn new(device: &Device) -> CudaResult<Self> {
        let caps = device.capabilities()?;

        let (supported_formats, default_format) = match caps.compute_capability {
            (10, _) => {
                // Blackwell: Full support
                (vec![QuantFormat::FP16, QuantFormat::FP8E4M3,
                      QuantFormat::FP8E5M2, QuantFormat::FP4],
                 QuantFormat::FP4)
            }
            (8, 9) | (9, _) => {
                // Ada Lovelace / Hopper: FP8 support
                (vec![QuantFormat::FP16, QuantFormat::FP8E4M3,
                      QuantFormat::FP8E5M2],
                 QuantFormat::FP8E4M3)
            }
            _ => {
                // Older GPUs: FP16 only
                (vec![QuantFormat::FP16], QuantFormat::FP16)
            }
        };

        Ok(Self {
            supported_formats,
            default_format,
            device_caps: caps,
        })
    }
}
```

### Model-Specific Format Selection
```rust
impl BlackwellQuantizer {
    pub fn select_format(&self, model_id: ModelId, batch_size: usize) -> QuantFormat {
        // Heavy models benefit most from aggressive quantization
        let prefer_fp4 = matches!(model_id,
            ModelId::Semantic |    // E5-Large (1024D)
            ModelId::Causal |      // Longformer (768D)
            ModelId::Sparse |      // SPLADE (large vocab)
            ModelId::Multimodal    // CLIP (vision encoder)
        );

        if prefer_fp4 && self.supports_fp4() && batch_size >= 16 {
            QuantFormat::FP4
        } else if self.supports_fp8() {
            // Use E4M3 for weights (more precision), E5M2 for activations (more range)
            QuantFormat::FP8E4M3
        } else {
            QuantFormat::FP16
        }
    }
}
```

---
*Task ID: M03-L28*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
