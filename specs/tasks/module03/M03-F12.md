# M03-F12: ModelRegistryConfig Struct

```xml
<task_spec id="M03-F12" version="1.0">
<metadata>
  <title>Define ModelRegistryConfig for Model Loading</title>
  <status>ready</status>
  <layer>foundation</layer>
  <sequence>12</sequence>
  <implements>constitution.yaml:embeddings.registry, PRD Section 4.3</implements>
  <depends_on>M03-F01</depends_on>
  <estimated_hours>1.5</estimated_hours>
</metadata>

<context>
Configuration for the model registry including:
- Path to downloaded models directory
- Lazy loading behavior
- List of models to preload at startup
- Auto-download settings
- Per-model configuration overrides (device, quantization, batch size)
</context>

<definition_of_done>
  <signatures>
```rust
use std::path::PathBuf;
use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use crate::types::ModelId;

/// Configuration for the model registry.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelRegistryConfig {
    /// Directory containing downloaded model weights.
    /// Default: "./models"
    pub models_dir: PathBuf,

    /// Whether to load models on first use (lazy) or at startup.
    /// Default: true
    pub lazy_loading: bool,

    /// Models to preload at initialization (even if lazy_loading=true).
    /// Default: [] (empty)
    pub preload_models: Vec<ModelId>,

    /// Whether to auto-download missing models from HuggingFace.
    /// Default: false
    pub auto_download: bool,

    /// Maximum concurrent model loads.
    /// Default: 2
    pub max_concurrent_loads: usize,

    /// Per-model configuration overrides.
    pub model_configs: HashMap<ModelId, SingleModelConfig>,
}

/// Configuration for a single model.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SingleModelConfig {
    /// Device placement for this model.
    pub device: DevicePlacement,

    /// Quantization mode for reduced memory.
    pub quantization: QuantizationMode,

    /// Maximum batch size for this model.
    /// Default: 32
    pub max_batch_size: usize,

    /// Whether to use flash attention if available.
    /// Default: true
    pub use_flash_attention: bool,
}

/// Device placement options.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, Default)]
#[serde(rename_all = "snake_case")]
pub enum DevicePlacement {
    /// CPU-only inference
    Cpu,
    /// Specific CUDA device by ID
    Cuda(u32),
    /// Auto-select based on availability
    #[default]
    Auto,
}

/// Quantization modes for memory reduction.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, Default)]
#[serde(rename_all = "snake_case")]
pub enum QuantizationMode {
    /// No quantization (full precision)
    #[default]
    None,
    /// 8-bit integer quantization
    Int8,
    /// 16-bit floating point
    Fp16,
    /// Brain floating point 16-bit
    Bf16,
}

impl Default for ModelRegistryConfig {
    fn default() -> Self {
        Self {
            models_dir: PathBuf::from("./models"),
            lazy_loading: true,
            preload_models: Vec::new(),
            auto_download: false,
            max_concurrent_loads: 2,
            model_configs: HashMap::new(),
        }
    }
}

impl Default for SingleModelConfig {
    fn default() -> Self {
        Self {
            device: DevicePlacement::Auto,
            quantization: QuantizationMode::None,
            max_batch_size: 32,
            use_flash_attention: true,
        }
    }
}
```
  </signatures>

  <constraints>
    - DevicePlacement and QuantizationMode must be Copy for efficiency
    - snake_case serde for TOML compatibility
    - HashMap keyed by ModelId for O(1) config lookup
    - Default preload_models is empty (lazy load all)
    - max_concurrent_loads prevents OOM from parallel loads
    - model_configs HashMap allows per-model overrides
  </constraints>

  <verification>
    - Default::default() creates valid configuration
    - Serde roundtrip works for all structs/enums
    - DevicePlacement::Cuda(0) serializes correctly
    - QuantizationMode variants serialize to snake_case
    - Empty model_configs HashMap is valid
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/config.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check --package context-graph-embeddings passes</criterion>
  <criterion>cargo test --package context-graph-embeddings passes</criterion>
  <criterion>cargo clippy --package context-graph-embeddings -- -D warnings shows 0 warnings</criterion>
  <criterion>All enums have Default implementations</criterion>
  <criterion>TOML serialization produces valid output</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Example TOML for model_configs
```toml
[models.model_configs.semantic]
device = "cuda"  # Note: Cuda(0) needs custom serde
quantization = "fp16"
max_batch_size = 16
use_flash_attention = true

[models.model_configs.code]
device = "cpu"
quantization = "int8"
max_batch_size = 8
use_flash_attention = false
```

### Custom Serde for DevicePlacement::Cuda
May need custom deserializer to parse:
- "cpu" -> DevicePlacement::Cpu
- "auto" -> DevicePlacement::Auto
- "cuda:0" -> DevicePlacement::Cuda(0)
- "cuda:1" -> DevicePlacement::Cuda(1)

### Test Cases
1. `test_default_models_dir` - "./models"
2. `test_default_lazy_loading` - true
3. `test_device_placement_variants` - All 3 variants
4. `test_quantization_mode_variants` - All 4 variants
5. `test_serde_cuda_device` - Cuda(0) roundtrip
6. `test_model_configs_hashmap` - Lookup by ModelId

---

*Task ID: M03-F12*
*Module: 03 - 12-Model Embedding Pipeline*
*Layer: Foundation*
*Created: 2026-01-01*
