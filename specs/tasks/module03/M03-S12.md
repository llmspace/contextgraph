<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-S12" version="1.0">
<metadata>
  <title>Documentation and Examples</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>48</sequence>
  <implements>
    - PRD: Developer documentation requirements
    - Quality gate: cargo doc generates clean docs
  </implements>
  <depends_on>
    - M03-S01 (EmbeddingPipeline Core)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Create comprehensive API documentation using rustdoc and usage examples
that demonstrate common embedding pipeline operations. Documentation should
enable developers to:
- Understand the 12-model architecture and FuseMoE fusion
- Configure the pipeline for different environments
- Integrate embeddings into their applications
- Troubleshoot common issues

Examples should be runnable and cover the most common use cases.
</context>

<definition_of_done>
  <signatures>
    <signature name="lib_rs_docs">
//! # Context Graph Embeddings
//!
//! 12-Model Embedding Pipeline for Context Graph.
//!
//! This crate provides text-to-embedding conversion using 12 specialized models
//! fused via FuseMoE (Mixture of Experts) into unified 1536-dimensional representations.
//!
//! ## Architecture
//!
//! The pipeline consists of three layers:
//!
//! 1. **Model Layer**: 12 specialized embedding models
//!    - E1: Semantic (e5-large-v2, 1024D)
//!    - E2-E4: Temporal (custom, 512D each)
//!    - E5: Causal (longformer, 768D)
//!    - E6: Sparse (SPLADE, projected to 1536D)
//!    - E7: Code (CodeBERT, 768D)
//!    - E8: Graph (paraphrase-MiniLM, 384D)
//!    - E9: HDC (custom hyperdimensional, 1024D)
//!    - E10: Multimodal (CLIP, 768D)
//!    - E11: Entity (all-MiniLM, 384D)
//!    - E12: Late-Interaction (ColBERT, 128D)
//!
//! 2. **Fusion Layer**: FuseMoE with 8 experts, top-2 routing
//!
//! 3. **Output**: Unified 1536D embedding
//!
//! ## Quick Start
//!
//! ```rust,no_run
//! use context_graph_embeddings::{EmbeddingPipeline, EmbeddingConfig};
//!
//! #[tokio::main]
//! async fn main() -> Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
//!     // Load default configuration
//!     let config = EmbeddingConfig::default();
//!
//!     // Create and initialize pipeline
//!     let pipeline = EmbeddingPipeline::new(config).await?;
//!     pipeline.initialize().await?;
//!
//!     // Embed text
//!     let embedding = pipeline.embed("Hello, world!").await?;
//!     println!("Embedding dimension: {}", embedding.vector.len()); // 1536
//!
//!     Ok(())
//! }
//! ```
//!
//! ## Performance
//!
//! | Operation | Target | Actual |
//! |-----------|--------|--------|
//! | Single embed | &lt;200ms | P95 |
//! | Batch (32) | &gt;100/sec | Throughput |
//! | Cache hit | &lt;100us | Latency |
//!
//! ## Feature Flags
//!
//! - `cuda`: Enable CUDA GPU acceleration (default)
//! - `cpu-only`: CPU-only mode for development
//! - `cache-disk`: Enable disk-backed cache persistence
    </signature>
    <signature name="example_basic">
// examples/basic_embed.rs
//! Basic text embedding example.
//!
//! Demonstrates simple text-to-embedding conversion.
//!
//! Run with: `cargo run --example basic_embed`

use context_graph_embeddings::{EmbeddingPipeline, EmbeddingConfig, EmbeddingResult};

#[tokio::main]
async fn main() -> EmbeddingResult&lt;()&gt; {
    // Initialize pipeline with defaults
    let pipeline = EmbeddingPipeline::new(EmbeddingConfig::default()).await?;
    pipeline.initialize().await?;

    // Embed a simple text
    let text = "The quick brown fox jumps over the lazy dog.";
    let embedding = pipeline.embed(text).await?;

    println!("Input: {}", text);
    println!("Embedding dimension: {}", embedding.vector.len());
    println!("Pipeline latency: {}us", embedding.pipeline_latency_us);
    println!("Top experts: {:?}", embedding.selected_experts);

    // Compute similarity between two texts
    let embedding2 = pipeline.embed("A fast auburn fox leaps over a sleepy hound.").await?;
    let similarity = embedding.cosine_similarity(&amp;embedding2);
    println!("Cosine similarity: {:.4}", similarity);

    pipeline.shutdown().await?;
    Ok(())
}
    </signature>
    <signature name="example_batch">
// examples/batch_embed.rs
//! Batch embedding example.
//!
//! Demonstrates efficient batch processing for multiple texts.
//!
//! Run with: `cargo run --example batch_embed`

use context_graph_embeddings::{EmbeddingPipeline, EmbeddingConfig, EmbeddingResult};

#[tokio::main]
async fn main() -> EmbeddingResult&lt;()&gt; {
    let pipeline = EmbeddingPipeline::new(EmbeddingConfig::default()).await?;
    pipeline.initialize().await?;

    let texts = vec![
        "Machine learning models process data.",
        "Natural language processing understands text.",
        "Deep learning uses neural networks.",
        "Embeddings represent meaning as vectors.",
    ];

    let start = std::time::Instant::now();
    let embeddings = pipeline.embed_batch(&amp;texts.iter().map(|s| *s).collect::&lt;Vec&lt;_&gt;&gt;()).await?;
    let elapsed = start.elapsed();

    println!("Embedded {} texts in {:?}", embeddings.len(), elapsed);
    println!("Throughput: {:.1} texts/sec", embeddings.len() as f64 / elapsed.as_secs_f64());

    pipeline.shutdown().await?;
    Ok(())
}
    </signature>
    <signature name="example_multimodal">
// examples/multimodal_embed.rs
//! Multimodal embedding example.
//!
//! Demonstrates embedding different input types.
//!
//! Run with: `cargo run --example multimodal_embed`

use context_graph_embeddings::{
    EmbeddingPipeline, EmbeddingConfig, ModelInput, EmbeddingResult
};

#[tokio::main]
async fn main() -> EmbeddingResult&lt;()&gt; {
    let pipeline = EmbeddingPipeline::new(EmbeddingConfig::default()).await?;
    pipeline.initialize().await?;

    // Text input
    let text_input = ModelInput::text("Natural language text");
    let text_emb = pipeline.embed_input(text_input).await?;
    println!("Text embedding: {}D", text_emb.vector.len());

    // Code input (uses CodeBERT)
    let code_input = ModelInput::code(
        "fn hello() { println!(\"Hello!\"); }",
        "rust"
    );
    let code_emb = pipeline.embed_input(code_input).await?;
    println!("Code embedding: {}D", code_emb.vector.len());

    pipeline.shutdown().await?;
    Ok(())
}
    </signature>
    <signature name="example_config">
// examples/custom_config.rs
//! Custom configuration example.
//!
//! Demonstrates configuring the pipeline for specific needs.
//!
//! Run with: `cargo run --example custom_config`

use context_graph_embeddings::{
    EmbeddingPipeline, EmbeddingConfig, EmbeddingResult,
    config::{BatchConfig, CacheConfig, GpuConfig, DevicePlacement},
};

#[tokio::main]
async fn main() -> EmbeddingResult&lt;()&gt; {
    // Custom configuration
    let config = EmbeddingConfig {
        batch: BatchConfig {
            max_batch_size: 64,
            max_wait_ms: 100,
            dynamic_batching: true,
            ..Default::default()
        },
        cache: CacheConfig {
            enabled: true,
            max_entries: 50_000,
            ttl_seconds: Some(3600),
            ..Default::default()
        },
        gpu: GpuConfig {
            enabled: true,
            device_ids: vec![0],
            memory_fraction: 0.8,
            ..Default::default()
        },
        ..Default::default()
    };

    let pipeline = EmbeddingPipeline::new(config).await?;
    pipeline.initialize().await?;

    println!("Pipeline configured and ready");
    println!("Health: {:?}", pipeline.health_check().await);
    println!("Metrics: {:?}", pipeline.metrics());

    pipeline.shutdown().await?;
    Ok(())
}
    </signature>
  </signatures>

  <constraints>
    <constraint>cargo doc generates clean documentation (no warnings)</constraint>
    <constraint>All public APIs fully documented with examples</constraint>
    <constraint>Examples must compile and run successfully</constraint>
    <constraint>Include troubleshooting section for common issues</constraint>
    <constraint>Document performance implications of configuration choices</constraint>
    <constraint>No emoji in documentation (per project guidelines)</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/examples/basic_embed.rs</file>
  <file>crates/context-graph-embeddings/examples/batch_embed.rs</file>
  <file>crates/context-graph-embeddings/examples/multimodal_embed.rs</file>
  <file>crates/context-graph-embeddings/examples/custom_config.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo doc --package context-graph-embeddings passes with no warnings</criterion>
  <criterion>All examples compile: cargo build --examples</criterion>
  <criterion>All examples run successfully with models loaded</criterion>
  <criterion>Documentation covers all public types and methods</criterion>
  <criterion>Quick start example works for new users</criterion>
</validation_criteria>

<documentation_sections>
  <section name="Architecture Overview">
    Explain 12-model design, FuseMoE fusion, and 1536D output
  </section>
  <section name="Getting Started">
    Quick start guide with minimal code example
  </section>
  <section name="Configuration">
    All configuration options with defaults and recommendations
  </section>
  <section name="API Reference">
    Auto-generated from rustdoc, comprehensive method docs
  </section>
  <section name="Performance Tuning">
    Batch size, cache settings, GPU memory optimization
  </section>
  <section name="Troubleshooting">
    Common errors and solutions
  </section>
</documentation_sections>

<example_descriptions>
  <example name="basic_embed.rs">
    Simple text embedding showing core API usage
  </example>
  <example name="batch_embed.rs">
    Batch processing for high throughput
  </example>
  <example name="multimodal_embed.rs">
    Different input types (text, code, images)
  </example>
  <example name="custom_config.rs">
    Configuration customization for different environments
  </example>
</example_descriptions>

<notes>
Use `#![doc = include_str!("../README.md")]` if a README exists.
Include doc-tests that run as part of cargo test.
Consider adding a CHANGELOG.md for version history.
</notes>
</task_spec>
