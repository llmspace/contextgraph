<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L07" version="1.0">
<metadata>
  <title>Causal Model (E5 - allenai/longformer-base-4096)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>7</sequence>
  <implements>PRD: E5 Causal embedding, 768D output, Longformer</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement causal embedding model using allenai/longformer-base-4096 for
processing longer sequences with efficient attention. This model supports
up to 4096 tokens and produces 768D vectors for causal reasoning tasks.

Model specifications:
- HuggingFace repo: allenai/longformer-base-4096
- Output dimension: 768D
- Max tokens: 4096
- Latency target: less than 8ms
- Attention: Sliding window with global tokens

The Longformer model uses a combination of sliding window attention and
global attention to efficiently process long documents while maintaining
the ability to capture long-range dependencies important for causal reasoning.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct CausalModel {
    model: LongformerModel,
    tokenizer: Tokenizer,
    device: Device,
    config: SingleModelConfig,
    loaded: AtomicBool,
    memory_size: usize,
}

impl CausalModel {
    /// Create new causal model instance
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Configure global attention tokens
    pub fn set_global_attention_tokens(&mut self, tokens: &[usize]);

    /// Get window size for sliding attention
    pub fn attention_window(&self) -> usize;
}

#[async_trait]
impl EmbeddingModel for CausalModel {
    fn model_id(&self) -> ModelId { ModelId::Causal }
    fn dimension(&self) -> usize { 768 }
    fn max_tokens(&self) -> usize { 4096 }
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses Longformer's sliding window attention for efficiency</constraint>
    <constraint>max_tokens() returns 4096</constraint>
    <constraint>dimension() returns 768</constraint>
    <constraint>Model loaded from models/causal/ directory</constraint>
    <constraint>Global attention on [CLS] token by default</constraint>
    <constraint>Mean pooling over token embeddings</constraint>
    <constraint>Output normalized to unit vector</constraint>
    <constraint>Latency target: less than 8ms for moderate length input</constraint>
  </constraints>

  <verification>
    <step>Model loads from local path</step>
    <step>embed() returns 768D vector</step>
    <step>Handles input up to 4096 tokens</step>
    <step>Long sequences processed efficiently (subquadratic)</step>
    <step>Output is L2 normalized</step>
    <step>Memory usage reported accurately</step>
    <step>Latency under 8ms for 512 token input</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/causal.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test passes</criterion>
  <criterion>Integration test with real model weights</criterion>
  <criterion>Long sequence handling verified</criterion>
  <criterion>Latency benchmark under 8ms P95</criterion>
</validation_criteria>
</task_spec>
