<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-S11" version="1.0">
<metadata>
  <title>Benchmarks</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>47</sequence>
  <implements>
    - PRD: Performance validation and regression detection
    - Performance targets table from specification
  </implements>
  <depends_on>
    - M03-S01 (EmbeddingPipeline Core)
  </depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Create comprehensive performance benchmarks using the criterion crate to:
- Validate all latency and throughput targets from PRD
- Detect performance regressions in CI/CD pipeline
- Generate detailed HTML reports for analysis
- Compare performance across different configurations

Benchmarks must cover the critical path: single embedding latency, batch
throughput, FuseMoE fusion time, cache lookup, and per-model latencies.
</context>

<definition_of_done>
  <signatures>
    <signature name="benchmark_module_structure">
// crates/context-graph-embeddings/benches/embedding_benchmarks.rs
use criterion::{criterion_group, criterion_main, Criterion, BenchmarkId, Throughput};

fn single_embed_latency(c: &amp;mut Criterion);
fn batch_embed_throughput(c: &amp;mut Criterion);
fn fusemoe_fusion(c: &amp;mut Criterion);
fn cache_lookup(c: &amp;mut Criterion);
fn per_model_latency(c: &amp;mut Criterion);

criterion_group!(
    benches,
    single_embed_latency,
    batch_embed_throughput,
    fusemoe_fusion,
    cache_lookup,
    per_model_latency,
);
criterion_main!(benches);
    </signature>
    <signature name="single_embed_latency">
/// Benchmark single text embedding end-to-end latency.
/// Target: &lt;200ms P95
fn single_embed_latency(c: &amp;mut Criterion) {
    let rt = tokio::runtime::Runtime::new().unwrap();
    let pipeline = rt.block_on(setup_pipeline());

    c.bench_function("single_embed_e2e", |b| {
        b.to_async(&amp;rt).iter(|| async {
            pipeline.embed("benchmark input text").await.unwrap()
        })
    });
}
    </signature>
    <signature name="batch_embed_throughput">
/// Benchmark batch embedding throughput at various batch sizes.
/// Target: &gt;100 items/sec at batch 32
fn batch_embed_throughput(c: &amp;mut Criterion) {
    let rt = tokio::runtime::Runtime::new().unwrap();
    let pipeline = rt.block_on(setup_pipeline());

    let mut group = c.benchmark_group("batch_throughput");
    for batch_size in [8, 16, 32, 64].iter() {
        let inputs: Vec&lt;&amp;str&gt; = (0..*batch_size)
            .map(|_| "sample text for throughput benchmark")
            .collect();

        group.throughput(Throughput::Elements(*batch_size as u64));
        group.bench_with_input(
            BenchmarkId::from_parameter(batch_size),
            &amp;inputs,
            |b, inputs| {
                b.to_async(&amp;rt).iter(|| async {
                    pipeline.embed_batch(inputs).await.unwrap()
                })
            },
        );
    }
    group.finish();
}
    </signature>
    <signature name="fusemoe_fusion">
/// Benchmark FuseMoE fusion layer in isolation.
/// Target: &lt;3ms per fusion
fn fusemoe_fusion(c: &amp;mut Criterion) {
    let fusemoe = setup_fusemoe();
    let concatenated = create_test_concatenated_embedding();

    c.bench_function("fusemoe_fusion", |b| {
        b.iter(|| fusemoe.fuse(&amp;concatenated).unwrap())
    });
}
    </signature>
    <signature name="cache_lookup">
/// Benchmark cache lookup latency.
/// Target: &lt;100us
fn cache_lookup(c: &amp;mut Criterion) {
    let cache = setup_cache_with_entries();
    let key = CacheKey { content_hash: 12345 };

    c.bench_function("cache_lookup", |b| {
        b.iter(|| cache.get(&amp;key))
    });
}
    </signature>
    <signature name="per_model_latency">
/// Benchmark individual model latencies.
/// Targets vary by model (see PRD performance table)
fn per_model_latency(c: &amp;mut Criterion) {
    let rt = tokio::runtime::Runtime::new().unwrap();
    let registry = rt.block_on(setup_registry());

    let mut group = c.benchmark_group("per_model");
    for model_id in ModelId::all() {
        let model = rt.block_on(registry.get_model(*model_id)).unwrap();
        let input = ModelInput::text("benchmark text");

        group.bench_with_input(
            BenchmarkId::from_parameter(format!("{:?}", model_id)),
            &amp;input,
            |b, input| {
                b.to_async(&amp;rt).iter(|| async {
                    model.embed(input).await.unwrap()
                })
            },
        );
    }
    group.finish();
}
    </signature>
  </signatures>

  <constraints>
    <constraint>Use criterion crate for statistical rigor</constraint>
    <constraint>Generate HTML reports in target/criterion/</constraint>
    <constraint>Support CI job for regression detection (10% threshold)</constraint>
    <constraint>Warm up models before benchmarking</constraint>
    <constraint>Use consistent input data for reproducibility</constraint>
    <constraint>Document expected performance targets in code</constraint>
    <constraint>Add baseline comparison capability</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/benches/embedding_benchmarks.rs</file>
  <file>crates/context-graph-embeddings/benches/model_benchmarks.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo bench compiles and runs</criterion>
  <criterion>HTML reports generated in target/criterion/</criterion>
  <criterion>single_embed_latency &lt; 200ms at P95</criterion>
  <criterion>batch_embed_throughput &gt; 100 items/sec at batch 32</criterion>
  <criterion>fusemoe_fusion &lt; 3ms mean</criterion>
  <criterion>cache_lookup &lt; 100us mean</criterion>
  <criterion>Semantic model &lt; 5ms mean</criterion>
  <criterion>Code model &lt; 10ms mean</criterion>
</validation_criteria>

<performance_targets>
  <target component="Single embed E2E" value="&lt;200ms" metric="P95 latency"/>
  <target component="Batch throughput" value="&gt;100 items/sec" metric="At batch 32"/>
  <target component="FuseMoE fusion" value="&lt;3ms" metric="Per fusion"/>
  <target component="Cache hit" value="&lt;100us" metric="Lookup latency"/>
  <target component="Cache hit rate" value="&gt;80%" metric="Under normal load"/>
  <target component="Semantic model (E1)" value="&lt;5ms" metric="Latency"/>
  <target component="Code model (E7)" value="&lt;10ms" metric="Latency"/>
  <target component="Late-interaction (E12)" value="&lt;8ms" metric="Latency"/>
</performance_targets>

<cargo_toml_addition>
[[bench]]
name = "embedding_benchmarks"
harness = false

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports", "async_tokio"] }
</cargo_toml_addition>

<ci_integration>
  <github_action>
# .github/workflows/benchmarks.yml
name: Performance Benchmarks
on:
  push:
    branches: [main]
  pull_request:

jobs:
  benchmark:
    runs-on: [self-hosted, gpu]
    steps:
      - uses: actions/checkout@v4
      - name: Run benchmarks
        run: cargo bench -- --save-baseline pr-${{ github.event.number }}
      - name: Compare with main
        if: github.event_name == 'pull_request'
        run: |
          cargo bench -- --baseline main --save-baseline pr-${{ github.event.number }}
          # Fail if regression > 10%
      - name: Upload reports
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-reports
          path: target/criterion/
  </github_action>
</ci_integration>

<notes>
Consider adding memory benchmarks for peak GPU/CPU usage.
Use criterion's async support with tokio runtime.
Set measurement time appropriately for GPU warm-up.
Add custom plots comparing batch sizes.
</notes>
</task_spec>
