<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L06" version="1.0">
<metadata>
  <title>Temporal-Positional Model (E4 - Sinusoidal PE)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>6</sequence>
  <implements>PRD: E4 Temporal-Positional embedding, 512D output, custom computed</implements>
  <depends_on>M03-F09</depends_on>
  <estimated_hours>2</estimated_hours>
</metadata>

<context>
Implement sinusoidal positional encoding for absolute timestamps, similar to
transformer positional encodings. This model produces 512D vectors that encode
the absolute position of content in time.

Model specifications:
- Type: Custom computed (no pretrained weights)
- Output dimension: 512D
- Encoding: Transformer-style sinusoidal positional encoding
- Formula: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
          PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

The temporal-positional model provides absolute time positioning that helps
distinguish content from different eras and enables temporal ordering.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct TemporalPositionalModel {
    max_position: u64,
    d_model: usize,
    base: f32, // 10000.0 by default
}

impl TemporalPositionalModel {
    /// Create new temporal-positional model
    pub fn new() -> Self;

    /// Create with custom parameters
    pub fn with_params(max_position: u64, d_model: usize, base: f32) -> Self;

    /// Compute sinusoidal positional encoding for timestamp
    fn compute_positional_encoding(&self, timestamp: DateTime<Utc>) -> Vec<f32>;

    /// Convert timestamp to position value
    fn timestamp_to_position(&self, timestamp: DateTime<Utc>) -> u64;

    /// Extract timestamp from input
    fn extract_timestamp(&self, input: &ModelInput) -> Option<DateTime<Utc>>;
}

#[async_trait]
impl EmbeddingModel for TemporalPositionalModel {
    fn model_id(&self) -> ModelId { ModelId::TemporalPositional }
    fn dimension(&self) -> usize { 512 }
    fn max_tokens(&self) -> usize { 0 } // Not token-based
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool { true }
    async fn load(&self) -> EmbeddingResult<()> { Ok(()) }
    async fn unload(&self) -> EmbeddingResult<()> { Ok(()) }

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize { std::mem::size_of::<Self>() }
    fn warmup_complete(&self) -> bool { true }
}
```
  </signatures>

  <constraints>
    <constraint>Uses transformer-style sinusoidal encoding</constraint>
    <constraint>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</constraint>
    <constraint>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</constraint>
    <constraint>Position derived from Unix timestamp (seconds since epoch)</constraint>
    <constraint>dimension() returns 512</constraint>
    <constraint>is_custom() returns true</constraint>
    <constraint>Base frequency 10000.0 (configurable)</constraint>
    <constraint>Supports timestamps from 1970 to far future</constraint>
  </constraints>

  <verification>
    <step>embed() returns 512D vector</step>
    <step>Different timestamps produce different embeddings</step>
    <step>Close timestamps produce similar embeddings</step>
    <step>Output values in range [-1, 1]</step>
    <step>Encoding follows transformer PE formula exactly</step>
    <step>Deterministic output for same timestamp</step>
    <step>Latency under 100 microseconds</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/custom/temporal_positional.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test passes</criterion>
  <criterion>Matches transformer positional encoding formula</criterion>
  <criterion>Numerical precision maintained</criterion>
</validation_criteria>
</task_spec>
