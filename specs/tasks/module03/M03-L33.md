# Task Specification: M03-L33

```xml
<task_spec id="M03-L33" version="1.0">
<metadata>
  <title>Sparse Projection Matrix Management (SPLADE)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>33</sequence>
  <implements>
    - PRD Section 7.3: E6 SPLADE sparse embedding projection
    - Constitution: reliability.projection_determinism
    - Technical Engine: High-dimensional projection stability
  </implements>
  <depends_on>
    - M03-L08 (Sparse Model - SPLADE E6)
    - M03-L31 (WeightRegistry)
    - M03-S13 (ModelArtifactManager)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
SPLADE outputs ~30,000 sparse dimensions (one per vocabulary token). These must be
projected to 1536D for fusion with other models. Since this projection layer is NOT
part of the pretrained SPLADE model, it requires explicit management.

Two approaches:
1. Random Projection (Johnson-Lindenstrauss lemma): Fast, no training, but projection
   matrix MUST be deterministic and persisted to ensure embedding stability.
2. Learned Projection: Trained linear layer, managed via WeightRegistry (M03-L31).

This task implements the random projection approach with deterministic persistence,
providing a fallback when no learned projection is available.

Key responsibilities:
- Generate deterministic projection matrix from seed
- Persist projection matrix for embedding stability
- Verify matrix integrity on load
- Integrate with WeightRegistry for learned projection alternative
- Apply projection efficiently (sparse × dense matrix multiply)
</context>

<definition_of_done>
  <signatures>
```rust
use std::path::PathBuf;

/// Sparse projection configuration
#[derive(Debug, Clone)]
pub struct SparseProjectionConfig {
    pub input_dim: usize,       // ~30,000 (SPLADE vocab)
    pub output_dim: usize,      // 1536 (target fusion dimension)
    pub projection_type: ProjectionType,
    pub storage_path: PathBuf,
    pub seed: u64,
}

#[derive(Debug, Clone)]
pub enum ProjectionType {
    /// Johnson-Lindenstrauss random projection (Gaussian)
    RandomGaussian { seed: u64 },
    /// Sparse random projection (faster, similar quality)
    RandomSparse { density: f32, seed: u64 },
    /// Learned linear projection (from WeightRegistry)
    Learned { checkpoint_key: String },
}

/// Projection matrix representation
#[derive(Debug, Clone)]
pub enum ProjectionMatrix {
    /// Dense matrix [output_dim, input_dim]
    Dense(Vec<Vec<f32>>),
    /// Sparse matrix (CSR format) for RandomSparse
    Sparse {
        values: Vec<f32>,
        col_indices: Vec<u32>,
        row_ptrs: Vec<u32>,
    },
}

/// Sparse projection manager
pub struct SparseProjection {
    config: SparseProjectionConfig,
    matrix: ProjectionMatrix,
    checksum: u64,
}

impl SparseProjection {
    /// Create or load projection matrix
    pub async fn new(config: SparseProjectionConfig) -> EmbeddingResult<Self>;

    /// Generate new random projection matrix
    pub fn generate(config: &SparseProjectionConfig) -> ProjectionMatrix;

    /// Save projection matrix to disk
    pub async fn save(&self) -> EmbeddingResult<PathBuf>;

    /// Load projection matrix from disk
    pub async fn load(config: &SparseProjectionConfig) -> EmbeddingResult<Self>;

    /// Check if persisted matrix exists
    pub fn exists(config: &SparseProjectionConfig) -> bool;

    /// Verify checksum of persisted matrix
    pub async fn verify(&self) -> EmbeddingResult<bool>;

    /// Project sparse vector to dense output
    pub fn project(&self, sparse_input: &SparseVector) -> Vec<f32>;

    /// Project batch of sparse vectors
    pub fn project_batch(&self, inputs: &[SparseVector]) -> Vec<Vec<f32>>;

    /// Get output dimension
    pub fn output_dim(&self) -> usize;
}

/// Sparse vector representation (SPLADE output)
#[derive(Debug, Clone)]
pub struct SparseVector {
    pub indices: Vec<u32>,  // Non-zero token indices
    pub values: Vec<f32>,   // Corresponding activation values
    pub dim: usize,         // Total vocabulary size
}

impl SparseVector {
    /// Density (fraction of non-zero elements)
    pub fn density(&self) -> f32;

    /// L2 norm
    pub fn norm(&self) -> f32;

    /// Normalize to unit length
    pub fn normalize(&mut self);
}

/// Integration with SPLADE model
impl SpladeModel {
    /// Embed with projection to target dimension
    pub async fn embed_projected(
        &self,
        input: &ModelInput,
        projection: &SparseProjection,
    ) -> EmbeddingResult<ModelEmbedding>;
}
```
  </signatures>

  <constraints>
    - Projection matrix MUST be deterministic given seed
    - Use same RNG algorithm across all platforms (ChaCha20)
    - Gaussian projection: scale by 1/sqrt(output_dim) for JL guarantee
    - Sparse projection: density ~1/sqrt(input_dim) optimal
    - Checksum computed over raw matrix bytes
    - Load time <50ms for 30k×1536 matrix
    - Projection operation <1ms for typical sparse vector (~100 non-zeros)
    - Storage: ~180MB for dense, ~18MB for sparse (10% density)
    - Integrate with WeightRegistry for learned projection fallback
  </constraints>

  <verification>
    <step>generate() produces identical matrix for same seed across runs</step>
    <step>save()/load() round-trip produces byte-identical matrix</step>
    <step>verify() detects corrupted matrices</step>
    <step>project() output has correct dimension (1536)</step>
    <step>project() preserves relative distances (JL property)</step>
    <step>project_batch() matches individual project() calls</step>
    <step>Learned projection loads from WeightRegistry correctly</step>
    <step>Same sparse input projects to same output across restarts</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/splade/projection.rs</file>
  <file>crates/context-graph-embeddings/src/models/splade/sparse_vector.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test splade::projection passes</criterion>
  <criterion>Determinism test: 100 seeds, verify identical matrices</criterion>
  <criterion>JL property: 1000 random pairs, distance distortion <20%</criterion>
  <criterion>Performance: project() <1ms, project_batch(100) <50ms</criterion>
  <criterion>Stability: embed same text before/after restart, embeddings match</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Johnson-Lindenstrauss Guarantee

For projecting from d=30,000 to k=1536 dimensions:
- Distance preservation: (1-ε) ≤ ||Ax - Ay|| / ||x - y|| ≤ (1+ε)
- With k = 1536, ε ≈ 0.1 (10% distortion) for n=1M points
- Scale factor: 1/sqrt(k) = 1/sqrt(1536) ≈ 0.0255

### Deterministic Random Matrix Generation

```rust
use rand_chacha::ChaCha20Rng;
use rand::SeedableRng;
use rand_distr::{Distribution, Normal};

impl SparseProjection {
    pub fn generate_gaussian(seed: u64, input_dim: usize, output_dim: usize) -> ProjectionMatrix {
        let mut rng = ChaCha20Rng::seed_from_u64(seed);
        let normal = Normal::new(0.0, 1.0 / (output_dim as f32).sqrt()).unwrap();

        let mut matrix = vec![vec![0.0f32; input_dim]; output_dim];
        for row in &mut matrix {
            for val in row.iter_mut() {
                *val = normal.sample(&mut rng);
            }
        }

        ProjectionMatrix::Dense(matrix)
    }

    pub fn generate_sparse(
        seed: u64,
        input_dim: usize,
        output_dim: usize,
        density: f32,
    ) -> ProjectionMatrix {
        let mut rng = ChaCha20Rng::seed_from_u64(seed);
        let scale = (1.0 / density).sqrt() / (output_dim as f32).sqrt();

        // Sparse random projection: {-1, 0, +1} with probability {density/2, 1-density, density/2}
        // ... CSR construction
        ProjectionMatrix::Sparse { values, col_indices, row_ptrs }
    }
}
```

### Efficient Sparse × Dense Projection

```rust
impl SparseProjection {
    pub fn project(&self, sparse_input: &SparseVector) -> Vec<f32> {
        let mut output = vec![0.0f32; self.config.output_dim];

        match &self.matrix {
            ProjectionMatrix::Dense(matrix) => {
                // For each non-zero in sparse input, add scaled column
                for (&idx, &val) in sparse_input.indices.iter().zip(&sparse_input.values) {
                    let col_idx = idx as usize;
                    for (out_idx, row) in matrix.iter().enumerate() {
                        output[out_idx] += val * row[col_idx];
                    }
                }
            }
            ProjectionMatrix::Sparse { values, col_indices, row_ptrs } => {
                // CSR-based sparse × sparse
                // ... optimized implementation
            }
        }

        output
    }
}
```

### Integration with WeightRegistry

```rust
impl SparseProjection {
    pub async fn from_weight_registry(
        registry: &WeightRegistry,
        checkpoint_key: &str,
        config: SparseProjectionConfig,
    ) -> EmbeddingResult<Self> {
        let weights = registry.load_weights(checkpoint_key).await?;
        let matrix = Self::deserialize_matrix(&weights)?;

        Ok(Self {
            config,
            matrix,
            checksum: Self::compute_checksum(&matrix),
        })
    }
}
```

### Persistence File Format

```
+------------------+
| HEADER (48 bytes)|
+------------------+
| Magic: "SPRJ" (4)|
| Version: u32     |
| Seed: u64        |
| InputDim: u32    |
| OutputDim: u32   |
| Type: u8 (0=D/1=S|
| Checksum: u64    |
| Reserved: 15B    |
+------------------+
| MATRIX DATA      |
| (format depends  |
|  on Type field)  |
+------------------+
```

---
*Task ID: M03-L33*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
