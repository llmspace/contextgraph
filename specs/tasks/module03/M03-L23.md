<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L23" version="1.0">
<metadata>
  <title>FuseMoE Main Fusion Module</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>23</sequence>
  <implements>PRD-EMB-004: FuseMoE Mixture of Experts Fusion</implements>
  <depends_on>M03-L22, M03-F05</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement the complete FuseMoE fusion layer that combines all 12 model embeddings
into a unified 1536D representation. This is the main entry point for the fusion
system, orchestrating the router, experts, and output normalization.

The FuseMoE module:
1. Takes ConcatenatedEmbedding (8320D from 12 models)
2. Routes through gating network to select top-2 experts
3. Computes weighted expert outputs
4. Applies output layer normalization
5. Returns FusedEmbedding with routing metadata

Latency target: <3ms per fusion operation.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct FuseMoE {
    router: Router,
    output_norm: LayerNorm,
    config: FusionConfig,
    device: Device,
}

impl FuseMoE {
    pub fn new(config: FusionConfig) -> EmbeddingResult<Self>;

    pub fn new_on_device(config: FusionConfig, device: Device) -> EmbeddingResult<Self>;

    /// Fuse concatenated embedding into final 1536D output
    pub fn fuse(&self, concatenated: &ConcatenatedEmbedding) -> EmbeddingResult<FusedEmbedding>;

    /// Batch fusion for multiple concatenated embeddings
    pub fn fuse_batch(
        &self,
        batch: &[ConcatenatedEmbedding]
    ) -> EmbeddingResult<Vec<FusedEmbedding>>;

    /// Get routing weights without full forward pass (for analysis)
    pub fn get_routing_weights(&self, input: &Tensor) -> EmbeddingResult<Vec<f32>>;

    /// Get configuration
    pub fn config(&self) -> &FusionConfig;

    /// Get device placement
    pub fn device(&self) -> &Device;

    /// Load pretrained weights
    pub fn load_weights(&mut self, path: impl AsRef<Path>) -> EmbeddingResult<()>;

    /// Save current weights
    pub fn save_weights(&self, path: impl AsRef<Path>) -> EmbeddingResult<()>;
}
```
  </signatures>

  <constraints>
    <constraint>Output: 1536D normalized embedding</constraint>
    <constraint>Latency target: <3ms per fusion</constraint>
    <constraint>Stores expert weights in FusedEmbedding.expert_weights</constraint>
    <constraint>Stores selected experts in FusedEmbedding.selected_experts</constraint>
    <constraint>Output normalization via LayerNorm</constraint>
    <constraint>Device-aware (CPU or CUDA)</constraint>
    <constraint>Weight persistence for model checkpointing</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/fusion/fusemoe.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes without warnings</criterion>
  <criterion>Unit test: fuse() returns FusedEmbedding with 1536D vector</criterion>
  <criterion>Unit test: FusedEmbedding.validate() passes</criterion>
  <criterion>Unit test: expert_weights sum to approximately 1.0</criterion>
  <criterion>Unit test: selected_experts contains 2 valid indices</criterion>
  <criterion>Unit test: fuse_batch processes multiple inputs correctly</criterion>
  <criterion>Unit test: save/load weights round-trip works</criterion>
  <criterion>Benchmark: fuse() latency <3ms</criterion>
  <criterion>Benchmark: fuse_batch() throughput with batch=32</criterion>
</validation_criteria>

<implementation_notes>
Main fusion flow:
```rust
fn fuse(&self, concatenated: &ConcatenatedEmbedding) -> EmbeddingResult<FusedEmbedding> {
    let start = Instant::now();

    // Convert to tensor [1, 8320]
    let input = Tensor::from_slice(&concatenated.concatenated, &[1, TOTAL_CONCATENATED], &self.device)?;

    // Route and forward through experts
    let output = self.router.forward(&input)?;  // [1, 1536]

    // Apply output normalization
    let normalized = self.output_norm.forward(&output)?;

    // Get routing info
    let routing = self.router.route(&input)?;
    let result = &routing[0];

    let latency_us = start.elapsed().as_micros() as u64;

    Ok(FusedEmbedding {
        vector: normalized.to_vec1()?,
        expert_weights: result.all_gate_probs.try_into().unwrap(),
        selected_experts: [result.selected_experts[0] as u8, result.selected_experts[1] as u8],
        pipeline_latency_us: latency_us,
        content_hash: concatenated.content_hash,
    })
}
```

Weight file format (safetensors):
- gating.layer_norm.weight
- gating.layer_norm.bias
- gating.projection.weight
- gating.projection.bias
- experts.{0-7}.input_proj.weight/bias
- experts.{0-7}.hidden.weight/bias
- experts.{0-7}.output_proj.weight/bias
- output_norm.weight/bias
</implementation_notes>
</task_spec>
