# M03-L13: Entity Model (E11 - MiniLM)

```xml
<task_spec id="M03-L13" version="1.0">
<metadata>
  <title>Implement Entity Model for Named Entity Embeddings</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>13</sequence>
  <implements>PRD E11 - Entity embedding with TransE-style relations</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Implement entity embedding using sentence-transformers/all-MiniLM-L6-v2.
Optimized for entity mentions and knowledge graph relations.
Follows TransE-style semantics where head + relation approximately equals tail.

Output: 384D dense vector.
Latency target: less than 2ms (extremely fast inference).

The model is ideal for:
- Named entity representation
- Knowledge graph link prediction
- Entity disambiguation
</context>

<definition_of_done>
  <signatures>
```rust
pub struct EntityModel {
    model: MiniLmModel,
    tokenizer: Tokenizer,
    device: Device,
    loaded: AtomicBool,
    config: SingleModelConfig,
}

impl EntityModel {
    pub fn new(models_dir: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Encode an entity with optional type context
    pub fn encode_entity(&self, name: &str, entity_type: Option<&str>) -> String;

    /// Encode a relation for TransE-style operations
    pub fn encode_relation(&self, relation: &str) -> String;

    /// TransE scoring: score = -||h + r - t||
    pub fn transe_score(
        head: &[f32],
        relation: &[f32],
        tail: &[f32]
    ) -> f32;

    /// Predict tail entity: t = h + r
    pub fn predict_tail(head: &[f32], relation: &[f32]) -> Vec<f32>;

    /// Predict relation: r = t - h
    pub fn predict_relation(head: &[f32], tail: &[f32]) -> Vec<f32>;
}

#[async_trait]
impl EmbeddingModel for EntityModel {
    fn model_id(&self) -> ModelId { ModelId::Entity }
    fn dimension(&self) -> usize { 384 }
    fn max_tokens(&self) -> usize { 128 }  // Entities are typically short
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for inference</constraint>
    <constraint>Model loaded from models/entity/ directory</constraint>
    <constraint>dimension() returns 384</constraint>
    <constraint>max_tokens() returns 128 (optimized for entity names)</constraint>
    <constraint>TransE-style semantics: h + r = t</constraint>
    <constraint>L2 normalization of embeddings</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
    <constraint>Latency target: less than 2ms per embedding</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/entity.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>EntityModel::new() creates valid instance</criterion>
  <criterion>Model loads weights from correct path</criterion>
  <criterion>embed() produces 384D vector</criterion>
  <criterion>encode_entity() produces formatted text</criterion>
  <criterion>transe_score() computes correct distance</criterion>
  <criterion>Embeddings are L2 normalized (unit norm)</criterion>
  <criterion>Latency under 2ms for typical entity names</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Architecture**: MiniLM (6 layers, distilled)
- **Hidden Size**: 384
- **Max Position**: 256 tokens
- **Parameters**: ~22M (compact, fast inference)

### Entity Encoding Format
```rust
impl EntityModel {
    pub fn encode_entity(&self, name: &str, entity_type: Option<&str>) -> String {
        match entity_type {
            Some(etype) => format!("[{}] {}", etype.to_uppercase(), name),
            None => name.to_string(),
        }
    }

    pub fn encode_relation(&self, relation: &str) -> String {
        // Convert snake_case to natural language
        relation.replace('_', " ")
    }
}
```

### TransE Operations
TransE models relations as translations in embedding space:
- **Principle**: For a valid triple (h, r, t), we have h + r approximately equals t
- **Scoring**: Lower distance means more likely valid triple

```rust
impl EntityModel {
    pub fn transe_score(head: &[f32], relation: &[f32], tail: &[f32]) -> f32 {
        // score = -||h + r - t||_2
        let mut sum_sq = 0.0;
        for i in 0..head.len() {
            let diff = head[i] + relation[i] - tail[i];
            sum_sq += diff * diff;
        }
        -sum_sq.sqrt()  // Negative L2 distance (higher is better)
    }

    pub fn predict_tail(head: &[f32], relation: &[f32]) -> Vec<f32> {
        // t = h + r
        head.iter().zip(relation.iter())
            .map(|(h, r)| h + r)
            .collect()
    }

    pub fn predict_relation(head: &[f32], tail: &[f32]) -> Vec<f32> {
        // r = t - h
        tail.iter().zip(head.iter())
            .map(|(t, h)| t - h)
            .collect()
    }
}
```

### Use Cases in Context Graph
1. **Entity Linking**: Match mentions to known entities
2. **Link Prediction**: Predict missing relations
3. **Entity Clustering**: Group similar entities
4. **Relation Classification**: Classify relation types

### Mean Pooling
Same as other MiniLM models, use mean pooling:
```rust
fn mean_pool(hidden_states: &Tensor, attention_mask: &Tensor) -> Tensor {
    let mask_expanded = attention_mask.unsqueeze(-1).expand_as(&hidden_states);
    let sum = (hidden_states * mask_expanded).sum(1);
    let count = mask_expanded.sum(1).clamp_min(1e-9);
    sum / count
}
```
