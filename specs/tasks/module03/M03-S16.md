# Task Specification: M03-S16

```xml
<task_spec id="M03-S16" version="1.0">
<metadata>
  <title>Pipeline Warm-up & JIT Trigger</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>46</sequence>
  <implements>
    - PRD: <200ms P95 latency from first interaction
    - RTX 5090 Technical Report: CUDA JIT compilation warm-up
    - Constitution: embeddings.pipeline.warmup
  </implements>
  <depends_on>M03-S01, M03-S05, M03-L23</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Implement a pipeline warm-up sequence that primes the GPU and triggers CUDA
JIT compilation during initialization. Without explicit warm-up, the first
50-100 interactions in "Infancy" will fail the <200ms P95 latency requirement
due to lazy kernel compilation and memory paging.

Key features:
1. Run synthetic tensors through all 12 embedding models
2. Trigger FuseMoE router and expert compilation
3. Prime GPU memory allocation paths
4. Populate cuDNN/cuBLAS plan caches
5. Verify all models produce valid output
6. Report warm-up metrics and any failures

Performance targets:
- Warm-up duration: <30 seconds total
- Post-warmup first request: <200ms P95
- Memory state: All model weights paged into VRAM
</context>

<definition_of_done>
  <signatures>
```rust
/// Warm-up configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WarmupConfig {
    /// Number of synthetic samples per model
    pub samples_per_model: usize,
    /// Batch sizes to warm up [1, 4, 16, 32]
    pub batch_sizes: Vec<usize>,
    /// Run parallel warm-up for all models
    pub parallel: bool,
    /// Timeout per model warm-up
    pub model_timeout_secs: u64,
    /// Skip models that fail warm-up
    pub skip_on_failure: bool,
    /// Verbose logging during warm-up
    pub verbose: bool,
}

impl Default for WarmupConfig {
    fn default() -> Self {
        Self {
            samples_per_model: 8,
            batch_sizes: vec![1, 4, 16, 32],
            parallel: true,
            model_timeout_secs: 60,
            skip_on_failure: false,
            verbose: true,
        }
    }
}

/// Warm-up result for a single model
#[derive(Debug, Clone)]
pub struct ModelWarmupResult {
    pub model_id: ModelId,
    pub success: bool,
    pub duration_ms: u64,
    pub samples_processed: usize,
    pub first_latency_ms: f64,
    pub final_latency_ms: f64,
    pub jit_compiled: bool,
    pub error: Option<String>,
}

/// Aggregate warm-up results
#[derive(Debug, Clone)]
pub struct WarmupReport {
    pub total_duration_ms: u64,
    pub models_warmed: usize,
    pub models_failed: usize,
    pub fusemoe_warmed: bool,
    pub peak_memory_gb: f64,
    pub model_results: Vec<ModelWarmupResult>,
    pub ready_for_inference: bool,
}

/// Pipeline warm-up manager
pub struct PipelineWarmup {
    config: WarmupConfig,
    synthetic_generator: SyntheticDataGenerator,
}

impl PipelineWarmup {
    /// Create warm-up manager with config
    pub fn new(config: WarmupConfig) -> Self;

    /// Run complete warm-up sequence
    pub async fn warmup(
        &self,
        pipeline: &EmbeddingPipeline,
    ) -> EmbeddingResult<WarmupReport>;

    /// Warm up single model
    pub async fn warmup_model(
        &self,
        model: &dyn EmbeddingModel,
    ) -> EmbeddingResult<ModelWarmupResult>;

    /// Warm up FuseMoE fusion layer
    pub async fn warmup_fusemoe(
        &self,
        fusemoe: &FuseMoE,
    ) -> EmbeddingResult<ModelWarmupResult>;

    /// Generate synthetic input for model
    fn generate_synthetic_input(&self, model_id: ModelId, batch_size: usize) -> Vec<ModelInput>;

    /// Verify model output is valid
    fn validate_output(&self, model_id: ModelId, output: &ModelEmbedding) -> bool;

    /// Prime GPU memory subsystem
    pub async fn prime_memory(&self, pool: &GpuMemoryPool) -> CudaResult<()>;

    /// Trigger cuDNN/cuBLAS autotuning
    pub async fn trigger_autotuning(&self, device: &Device) -> CudaResult<()>;
}

/// Synthetic data generator for warm-up
pub struct SyntheticDataGenerator {
    /// Sample texts of varying lengths
    text_samples: Vec<String>,
    /// Sample code snippets
    code_samples: Vec<String>,
    /// Random seed for reproducibility
    seed: u64,
}

impl SyntheticDataGenerator {
    pub fn new(seed: u64) -> Self;

    /// Generate text input of specified length
    pub fn generate_text(&self, min_tokens: usize, max_tokens: usize) -> String;

    /// Generate code input
    pub fn generate_code(&self, language: &str, lines: usize) -> String;

    /// Generate random embeddings for FuseMoE warm-up
    pub fn generate_concatenated_embedding(&self) -> ConcatenatedEmbedding;
}
```
  </signatures>

  <constraints>
    <constraint>Must run during EmbeddingPipeline::initialize()</constraint>
    <constraint>Total warm-up time <30 seconds</constraint>
    <constraint>Must warm all 12 models and FuseMoE</constraint>
    <constraint>Must test all configured batch sizes</constraint>
    <constraint>Must validate outputs are numerically sane</constraint>
    <constraint>Must report JIT compilation status</constraint>
    <constraint>Memory usage tracked during warm-up</constraint>
    <constraint>Timeout handling for stuck models</constraint>
    <constraint>Graceful degradation if model fails warm-up</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/pipeline/warmup.rs</file>
  <file>crates/context-graph-embeddings/src/pipeline/synthetic.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>warmup() completes in <30 seconds</criterion>
  <criterion>All 12 models produce valid embeddings</criterion>
  <criterion>FuseMoE produces valid 1536D output</criterion>
  <criterion>Post-warmup latency <200ms for single embed</criterion>
  <criterion>WarmupReport accurately reflects results</criterion>
  <criterion>Timeout handling works correctly</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Warm-up Sequence Flow
```
Pipeline Initialize
        |
        v
[Check GDS Availability] --> [Load Models via GDS]
        |
        v
[Prime Memory Pool] --> [Allocate/Free test buffers]
        |
        v
[Trigger Autotuning] --> [cuDNN workspace setup]
        |
        v
[Parallel Model Warm-up] --> [12 models concurrent]
        |                           |
        |                           +--> E1: Semantic
        |                           +--> E2: Temporal-Recent
        |                           +--> E3: Temporal-Periodic
        |                           +--> E4: Temporal-Position
        |                           +--> E5: Causal
        |                           +--> E6: Sparse
        |                           +--> E7: Code
        |                           +--> E8: Graph
        |                           +--> E9: HDC
        |                           +--> E10: Multimodal
        |                           +--> E11: Entity
        |                           +--> E12: Late-Interaction
        |
        v
[FuseMoE Warm-up] --> [Route + Expert compilation]
        |
        v
[Validation] --> [Check outputs, report metrics]
        |
        v
[Ready for Inference]
```

### Synthetic Data Generation
```rust
impl SyntheticDataGenerator {
    const SAMPLE_TEXTS: &'static [&'static str] = &[
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning models require careful optimization.",
        "Distributed systems face challenges with consistency.",
        "Natural language processing has advanced significantly.",
        // ... more samples
    ];

    const SAMPLE_CODE: &'static [&'static str] = &[
        "fn main() { println!(\"Hello, world!\"); }",
        "class Example { constructor() { this.value = 0; } }",
        "def process(data): return [x * 2 for x in data]",
        // ... more samples
    ];

    pub fn generate_text(&self, min_tokens: usize, max_tokens: usize) -> String {
        let mut rng = StdRng::seed_from_u64(self.seed);
        let target_tokens = rng.gen_range(min_tokens..=max_tokens);

        let mut result = String::new();
        while result.split_whitespace().count() < target_tokens {
            let sample = self.text_samples.choose(&mut rng).unwrap();
            result.push_str(sample);
            result.push(' ');
        }

        result.truncate(result.len().min(target_tokens * 6)); // ~6 chars per token
        result
    }

    pub fn generate_concatenated_embedding(&self) -> ConcatenatedEmbedding {
        let mut rng = StdRng::seed_from_u64(self.seed);
        let mut embeddings = Vec::new();

        for model_id in ModelId::iter() {
            let dim = ModelDimensions::get(model_id);
            let vector: Vec<f32> = (0..dim).map(|_| rng.gen_range(-1.0..1.0)).collect();

            embeddings.push(ModelEmbedding {
                model_id,
                vector,
                latency_us: 0,
                normalized: false,
            });
        }

        ConcatenatedEmbedding::from_model_embeddings(embeddings).unwrap()
    }
}
```

### Parallel Model Warm-up
```rust
impl PipelineWarmup {
    pub async fn warmup(
        &self,
        pipeline: &EmbeddingPipeline,
    ) -> EmbeddingResult<WarmupReport> {
        let start = Instant::now();
        let mut model_results = Vec::new();

        // Prime memory subsystem
        if let Err(e) = self.prime_memory(pipeline.gpu_pool()).await {
            warn!("Memory priming failed: {}", e);
        }

        // Trigger autotuning
        if let Err(e) = self.trigger_autotuning(pipeline.device()).await {
            warn!("Autotuning failed: {}", e);
        }

        // Warm up models
        if self.config.parallel {
            // Parallel warm-up
            let tasks: Vec<_> = pipeline.registry()
                .loaded_models()
                .iter()
                .map(|model| {
                    let config = self.config.clone();
                    let model = model.clone();
                    tokio::spawn(async move {
                        Self::warmup_single_model(&config, model.as_ref()).await
                    })
                })
                .collect();

            for task in tasks {
                match task.await {
                    Ok(Ok(result)) => model_results.push(result),
                    Ok(Err(e)) => {
                        if !self.config.skip_on_failure {
                            return Err(e);
                        }
                    }
                    Err(e) => warn!("Warm-up task panicked: {}", e),
                }
            }
        } else {
            // Sequential warm-up
            for model in pipeline.registry().loaded_models() {
                match self.warmup_model(model.as_ref()).await {
                    Ok(result) => model_results.push(result),
                    Err(e) if self.config.skip_on_failure => {
                        warn!("Model warm-up failed: {}", e);
                    }
                    Err(e) => return Err(e),
                }
            }
        }

        // Warm up FuseMoE
        let fusemoe_result = self.warmup_fusemoe(pipeline.fusemoe()).await?;
        model_results.push(fusemoe_result);

        let total_duration = start.elapsed();
        let models_warmed = model_results.iter().filter(|r| r.success).count();
        let models_failed = model_results.iter().filter(|r| !r.success).count();

        Ok(WarmupReport {
            total_duration_ms: total_duration.as_millis() as u64,
            models_warmed,
            models_failed,
            fusemoe_warmed: model_results.last().map(|r| r.success).unwrap_or(false),
            peak_memory_gb: self.get_peak_memory_gb(),
            model_results,
            ready_for_inference: models_failed == 0,
        })
    }

    async fn warmup_single_model(
        config: &WarmupConfig,
        model: &dyn EmbeddingModel,
    ) -> EmbeddingResult<ModelWarmupResult> {
        let start = Instant::now();
        let model_id = model.model_id();
        let mut first_latency = None;
        let mut final_latency = 0.0;
        let mut samples_processed = 0;

        for batch_size in &config.batch_sizes {
            for _ in 0..config.samples_per_model {
                let inputs = SyntheticDataGenerator::new(42)
                    .generate_inputs(model_id, *batch_size);

                let batch_start = Instant::now();
                let results = model.embed_batch(&inputs).await?;
                let latency_ms = batch_start.elapsed().as_secs_f64() * 1000.0;

                if first_latency.is_none() {
                    first_latency = Some(latency_ms);
                }
                final_latency = latency_ms / *batch_size as f64;

                // Validate outputs
                for output in &results {
                    if !Self::validate_embedding(model_id, output) {
                        return Err(EmbeddingError::WarmupValidationFailed(
                            format!("{:?} produced invalid output", model_id)
                        ));
                    }
                }

                samples_processed += results.len();
            }
        }

        Ok(ModelWarmupResult {
            model_id,
            success: true,
            duration_ms: start.elapsed().as_millis() as u64,
            samples_processed,
            first_latency_ms: first_latency.unwrap_or(0.0),
            final_latency_ms: final_latency,
            jit_compiled: true,
            error: None,
        })
    }
}
```

### Memory Priming
```rust
impl PipelineWarmup {
    pub async fn prime_memory(&self, pool: &GpuMemoryPool) -> CudaResult<()> {
        // Allocate and free buffers of each bucket size
        // This primes the allocation paths and triggers any lazy initialization

        for &bucket_size in &GpuMemoryPool::BUCKET_SIZES {
            for _ in 0..4 {
                let buffer = pool.allocate(bucket_size)?;
                // Touch memory to ensure it's paged in
                unsafe {
                    // memset via CUDA kernel or cuMemsetD8
                }
                drop(buffer); // Return to pool
            }
        }

        Ok(())
    }

    pub async fn trigger_autotuning(&self, device: &Device) -> CudaResult<()> {
        // Run small matmuls to trigger cuBLAS/cuDNN plan caching
        let sizes = [64, 128, 256, 512, 1024];

        for &size in &sizes {
            let a = Tensor::randn(&[size, size], device)?;
            let b = Tensor::randn(&[size, size], device)?;
            let _ = a.matmul(&b)?;
        }

        Ok(())
    }
}
```

### Integration with Pipeline
```rust
impl EmbeddingPipeline {
    pub async fn initialize(&self) -> EmbeddingResult<()> {
        // Load all models
        self.registry.load_all().await?;

        // Run warm-up sequence
        let warmup = PipelineWarmup::new(WarmupConfig::default());
        let report = warmup.warmup(self).await?;

        if !report.ready_for_inference {
            return Err(EmbeddingError::InitializationFailed(format!(
                "{} models failed warm-up", report.models_failed
            )));
        }

        info!("Pipeline warm-up complete in {}ms, {} models ready",
            report.total_duration_ms, report.models_warmed);

        self.initialized.store(true, Ordering::Release);
        Ok(())
    }
}
```

---
*Task ID: M03-S16*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
