# Task Specification: M03-S04

```xml
<task_spec id="M03-S04" version="1.0">
<metadata>
  <title>CUDA Device Trait</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>43</sequence>
  <implements>
    - PRD: GPU acceleration via CUDA 13.1
    - PRD: RTX 5090 target hardware support
    - Constitution: cuda.device.abstraction
  </implements>
  <depends_on>
    - M03-F16 (Module Structure and Exports)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Define the CUDA device abstraction trait in the context-graph-cuda crate.
This trait provides a hardware-agnostic interface for GPU device management,
memory queries, and synchronization operations.

The trait is designed to:
1. Abstract over different CUDA implementations (native, cudarc, etc.)
2. Support future GPU backends (ROCm, Metal, etc.)
3. Provide safe Rust wrappers around unsafe GPU operations
4. Enable testing with mock GPU implementations

Target hardware: NVIDIA RTX 5090 with CUDA 13.1
- 32GB VRAM (24GB allocated for models)
- Compute capability 10.0+
- Green contexts support
</context>

<definition_of_done>
  <signatures>
    <signature>
```rust
/// CUDA device abstraction trait
pub trait CudaDevice: Send + Sync {
    /// Get the device ID (0-indexed)
    fn device_id(&self) -> u32;

    /// Get device name
    fn device_name(&self) -> &str;

    /// Get total GPU memory in bytes
    fn total_memory(&self) -> usize;

    /// Get available (free) GPU memory in bytes
    fn available_memory(&self) -> usize;

    /// Get used GPU memory in bytes
    fn used_memory(&self) -> usize;

    /// Synchronize device (wait for all operations to complete)
    fn synchronize(&self) -> CudaResult<()>;

    /// Get compute capability as (major, minor)
    fn compute_capability(&self) -> (u32, u32);

    /// Check if device supports feature
    fn supports_feature(&self, feature: CudaFeature) -> bool;

    /// Set as current device for this thread
    fn set_current(&self) -> CudaResult<()>;

    /// Check if this is the current device
    fn is_current(&self) -> bool;

    /// Allocate memory on device
    fn allocate(&self, size: usize) -> CudaResult<GpuBuffer>;

    /// Free allocated memory
    fn deallocate(&self, buffer: GpuBuffer) -> CudaResult<()>;

    /// Copy from host to device
    fn copy_to_device(&self, host: &[u8], device: &mut GpuBuffer) -> CudaResult<()>;

    /// Copy from device to host
    fn copy_to_host(&self, device: &GpuBuffer, host: &mut [u8]) -> CudaResult<()>;
}
```
    </signature>
    <signature>
```rust
/// GPU buffer handle
#[derive(Debug)]
pub struct GpuBuffer {
    /// Raw pointer to GPU memory
    ptr: *mut std::ffi::c_void,

    /// Size of buffer in bytes
    size: usize,

    /// Device ID this buffer belongs to
    device_id: u32,
}
```
    </signature>
    <signature>
```rust
impl GpuBuffer {
    /// Create new buffer (internal use only)
    pub(crate) fn new(ptr: *mut std::ffi::c_void, size: usize, device_id: u32) -> Self;

    /// Get buffer size in bytes
    pub fn size(&self) -> usize;

    /// Get device ID
    pub fn device_id(&self) -> u32;

    /// Get raw pointer (unsafe)
    pub unsafe fn as_ptr(&self) -> *mut std::ffi::c_void;

    /// Check if buffer is valid (non-null)
    pub fn is_valid(&self) -> bool;
}
```
    </signature>
    <signature>
```rust
/// CUDA feature flags
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum CudaFeature {
    /// CUDA graphs support
    CudaGraphs,

    /// Tensor cores
    TensorCores,

    /// FP16 compute
    Fp16Compute,

    /// BF16 compute
    Bf16Compute,

    /// Dynamic parallelism
    DynamicParallelism,

    /// Unified memory
    UnifiedMemory,

    /// Green contexts (CUDA 13.1+)
    GreenContexts,

    /// Multi-instance GPU
    Mig,

    /// Async copy
    AsyncCopy,
}
```
    </signature>
    <signature>
```rust
/// CUDA error type
#[derive(Debug, Error)]
pub enum CudaError {
    #[error("Device not found: {device_id}")]
    DeviceNotFound { device_id: u32 },

    #[error("Out of memory: requested {requested} bytes, available {available}")]
    OutOfMemory { requested: usize, available: usize },

    #[error("Invalid buffer")]
    InvalidBuffer,

    #[error("Synchronization failed: {message}")]
    SyncFailed { message: String },

    #[error("CUDA driver error: {code}")]
    DriverError { code: i32 },

    #[error("Device not initialized")]
    NotInitialized,

    #[error("Feature not supported: {feature:?}")]
    FeatureNotSupported { feature: CudaFeature },

    #[error("Copy failed: {message}")]
    CopyFailed { message: String },
}

pub type CudaResult<T> = Result<T, CudaError>;
```
    </signature>
    <signature>
```rust
/// Get number of CUDA devices available
pub fn device_count() -> CudaResult<u32>;

/// Get device by ID
pub fn get_device(device_id: u32) -> CudaResult<Box<dyn CudaDevice>>;

/// Get all available devices
pub fn get_all_devices() -> CudaResult<Vec<Box<dyn CudaDevice>>>;

/// Check if CUDA is available
pub fn is_available() -> bool;

/// Get CUDA driver version
pub fn driver_version() -> CudaResult<(u32, u32)>;

/// Get CUDA runtime version
pub fn runtime_version() -> CudaResult<(u32, u32)>;
```
    </signature>
  </signatures>

  <constraints>
    - Thread-safe: CudaDevice must be Send + Sync
    - Memory safety: GpuBuffer must prevent use-after-free
    - Drop safety: GpuBuffer should implement Drop to free memory
    - Error handling: All fallible operations return CudaResult
    - Feature detection: Accurate capability detection for RTX 5090
    - Multi-device: Support multiple GPUs
    - Compute capability: RTX 5090 is compute 10.0+
    - Memory tracking: Track allocations for debugging
    - Safe by default: Hide unsafe operations behind safe APIs
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-cuda/src/device.rs</file>
  <file>crates/context-graph-cuda/src/error.rs</file>
  <file>crates/context-graph-cuda/src/buffer.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes with no errors</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>CudaDevice trait is Send + Sync</criterion>
  <criterion>GpuBuffer implements Drop for cleanup</criterion>
  <criterion>All CudaFeature variants are defined</criterion>
  <criterion>CudaError is thiserror-derived</criterion>
  <criterion>device_count() function compiles</criterion>
  <criterion>is_available() function compiles</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### GpuBuffer Drop Implementation
```rust
impl Drop for GpuBuffer {
    fn drop(&mut self) {
        if !self.ptr.is_null() {
            // In real implementation, call cudaFree
            // For now, just mark as freed
            self.ptr = std::ptr::null_mut();
        }
    }
}

// Prevent copying (would cause double-free)
impl !Clone for GpuBuffer {}
```

### Mock Device for Testing
```rust
pub struct MockCudaDevice {
    device_id: u32,
    total_memory: usize,
    allocated: AtomicUsize,
}

impl CudaDevice for MockCudaDevice {
    fn device_id(&self) -> u32 { self.device_id }
    fn total_memory(&self) -> usize { self.total_memory }
    fn available_memory(&self) -> usize {
        self.total_memory - self.allocated.load(Ordering::Relaxed)
    }
    // ... etc
}
```

### RTX 5090 Feature Support
```rust
impl CudaDevice for Rtx5090Device {
    fn compute_capability(&self) -> (u32, u32) {
        (10, 0) // SM 10.0 for Blackwell architecture
    }

    fn supports_feature(&self, feature: CudaFeature) -> bool {
        match feature {
            CudaFeature::TensorCores => true,
            CudaFeature::Fp16Compute => true,
            CudaFeature::Bf16Compute => true,
            CudaFeature::CudaGraphs => true,
            CudaFeature::GreenContexts => true, // CUDA 13.1
            CudaFeature::AsyncCopy => true,
            CudaFeature::UnifiedMemory => true,
            _ => true,
        }
    }
}
```

### Conditional Compilation
```rust
#[cfg(feature = "cuda")]
pub use native_cuda::*;

#[cfg(not(feature = "cuda"))]
pub use mock_cuda::*;
```

---
*Task ID: M03-S04*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
