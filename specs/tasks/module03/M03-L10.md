# M03-L10: Graph Model (E8 - MiniLM)

```xml
<task_spec id="M03-L10" version="1.0">
<metadata>
  <title>Implement Graph Model for Structural Relations</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>10</sequence>
  <implements>PRD E8 - Graph-GNN embedding with message passing</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Implement graph embedding using sentence-transformers/paraphrase-MiniLM-L6-v2.
This model is used for encoding structural and relational information in the
knowledge graph. Optimized for short relation descriptions and entity contexts.

Output: 384D dense vector.
Latency target: less than 5ms.

The model is efficient due to knowledge distillation from larger models,
making it ideal for high-throughput relation encoding.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct GraphModel {
    model: MiniLmModel,
    tokenizer: Tokenizer,
    device: Device,
    loaded: AtomicBool,
    config: SingleModelConfig,
}

impl GraphModel {
    pub fn new(models_dir: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Encode a relation triple as text for embedding
    pub fn encode_relation(&self, subject: &str, predicate: &str, object: &str) -> String;

    /// Encode graph context around a node
    pub fn encode_context(&self, node: &str, neighbors: &[(String, String)]) -> String;
}

#[async_trait]
impl EmbeddingModel for GraphModel {
    fn model_id(&self) -> ModelId { ModelId::Graph }
    fn dimension(&self) -> usize { 384 }
    fn max_tokens(&self) -> usize { 256 }
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for inference</constraint>
    <constraint>Model loaded from models/graph/ directory</constraint>
    <constraint>dimension() returns 384</constraint>
    <constraint>max_tokens() returns 256 (optimized for short texts)</constraint>
    <constraint>Optimized for short relation descriptions</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
    <constraint>Latency target: less than 5ms per embedding</constraint>
    <constraint>Mean pooling for sentence representation</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/graph.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>GraphModel::new() creates valid instance</criterion>
  <criterion>Model loads weights from correct path</criterion>
  <criterion>embed() produces 384D vector</criterion>
  <criterion>encode_relation() produces valid text format</criterion>
  <criterion>encode_context() handles variable neighbor counts</criterion>
  <criterion>Embedding values are normalized (unit L2 norm)</criterion>
  <criterion>Latency under 5ms for typical inputs</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `sentence-transformers/paraphrase-MiniLM-L6-v2`
- **Architecture**: MiniLM (6 layers, distilled)
- **Hidden Size**: 384
- **Max Position**: 256 tokens (model supports 512 but we optimize for short)
- **Parameters**: ~22M (compact)

### Relation Encoding Format
```rust
impl GraphModel {
    pub fn encode_relation(&self, subject: &str, predicate: &str, object: &str) -> String {
        // Format: "subject predicate object"
        // Example: "Alice works_at Anthropic"
        format!("{} {} {}", subject, predicate.replace('_', " "), object)
    }

    pub fn encode_context(&self, node: &str, neighbors: &[(String, String)]) -> String {
        // Format: "node: rel1 neighbor1, rel2 neighbor2, ..."
        let context: Vec<String> = neighbors.iter()
            .take(5)  // Limit to 5 neighbors for token budget
            .map(|(rel, neighbor)| format!("{} {}", rel.replace('_', " "), neighbor))
            .collect();
        format!("{}: {}", node, context.join(", "))
    }
}
```

### Use Cases
1. **Relation Similarity**: Compare semantic similarity of relations
2. **Entity Contextualization**: Encode node with local graph context
3. **Link Prediction**: Compare relation embeddings for scoring

### Mean Pooling
Same mean pooling strategy as other sentence-transformer models:
```rust
fn mean_pool(hidden_states: &Tensor, attention_mask: &Tensor) -> Tensor {
    let mask_expanded = attention_mask.unsqueeze(-1).expand_as(&hidden_states);
    let sum = (hidden_states * mask_expanded).sum(1);
    let count = mask_expanded.sum(1).clamp_min(1e-9);
    sum / count
}
```
