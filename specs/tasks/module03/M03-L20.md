<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L20" version="1.0">
<metadata>
  <title>GatingNetwork for FuseMoE Expert Routing</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>20</sequence>
  <implements>PRD-EMB-004: FuseMoE Mixture of Experts Fusion</implements>
  <depends_on>M03-F02, M03-F14</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Implement the gating network that routes inputs to experts in the FuseMoE fusion
layer. The gating network takes the concatenated embedding (8320D from all 12 models)
and produces routing probabilities over 8 experts. It uses a learned linear projection
followed by softmax to determine which experts should process each input.

The gating network includes:
- Layer normalization for input stability
- Linear projection to num_experts dimensions
- Temperature-scaled softmax for routing
- Optional noise injection during training
- Laplace smoothing for numerical stability
</context>

<definition_of_done>
  <signatures>
```rust
pub struct GatingNetwork {
    layer_norm: LayerNorm,
    projection: Linear,
    num_experts: usize,
    temperature: f32,
}

impl GatingNetwork {
    pub fn new(input_dim: usize, num_experts: usize, temperature: f32) -> Self;

    /// Forward pass producing expert probabilities
    /// Input: [batch_size, input_dim]
    /// Output: [batch_size, num_experts] with softmax probabilities
    pub fn forward(&self, input: &Tensor) -> Tensor;

    /// Forward with noise injection for training load balancing
    /// noise_std: standard deviation of Gaussian noise added before softmax
    pub fn forward_with_noise(&self, input: &Tensor, noise_std: f32) -> Tensor;

    /// Apply Laplace smoothing: (p + alpha) / (1 + alpha * K)
    pub fn apply_laplace_smoothing(&self, probs: &Tensor, alpha: f32) -> Tensor;

    /// Get number of experts
    pub fn num_experts(&self) -> usize;
}

pub struct LayerNorm {
    weight: Tensor,
    bias: Tensor,
    eps: f32,
}

pub struct Linear {
    weight: Tensor,
    bias: Option<Tensor>,
}
```
  </signatures>

  <constraints>
    <constraint>Input dimension: 8320 (TOTAL_CONCATENATED from dimensions module)</constraint>
    <constraint>Output dimension: 8 (num_experts)</constraint>
    <constraint>Softmax probabilities sum to 1.0</constraint>
    <constraint>Laplace smoothing formula: (p + alpha) / (1 + alpha * K) where K=num_experts</constraint>
    <constraint>Temperature scaling: softmax(x / temperature)</constraint>
    <constraint>LayerNorm with eps=1e-5 for numerical stability</constraint>
    <constraint>Use candle for tensor operations</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/fusion/gating.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes without warnings</criterion>
  <criterion>Unit test: output shape is [batch, num_experts]</criterion>
  <criterion>Unit test: softmax probabilities sum to 1.0</criterion>
  <criterion>Unit test: Laplace smoothing produces valid probabilities</criterion>
  <criterion>Unit test: temperature=1.0 equals standard softmax</criterion>
  <criterion>Unit test: higher temperature produces more uniform distribution</criterion>
  <criterion>Unit test: noise injection affects output variability</criterion>
</validation_criteria>

<implementation_notes>
Using candle for tensor operations:
```rust
use candle_core::{Tensor, Device};
use candle_nn::{linear, layer_norm, Linear, LayerNorm};
```

Softmax with temperature:
```rust
fn softmax_with_temp(logits: &Tensor, temp: f32) -> Tensor {
    let scaled = (logits / temp)?;
    candle_nn::ops::softmax(&scaled, 1)? // dim=1 for [batch, experts]
}
```

Laplace smoothing prevents zero probabilities:
- Adds alpha (default 0.01) to each probability
- Renormalizes to sum to 1.0
- Prevents numerical issues in downstream log computations
</implementation_notes>
</task_spec>
