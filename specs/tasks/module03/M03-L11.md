# M03-L11: HDC Model (E9 - Custom)

```xml
<task_spec id="M03-L11" version="1.0">
<metadata>
  <title>Implement HDC Model (Hyperdimensional Computing)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>11</sequence>
  <implements>PRD E9 - HDC embedding with XOR binding and Hamming distance</implements>
  <depends_on>M03-F09</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement hyperdimensional computing (HDC) embedding model.
HDC uses high-dimensional binary vectors with XOR binding for composition
and Hamming distance for similarity. This provides a computationally
efficient symbolic reasoning representation.

Output: 1024D float vector (projected from 10K-bit binary).
This is a custom model with no pretrained weights.

HDC embeddings are particularly useful for:
- Symbolic manipulation and composition
- Noise-robust pattern matching
- Low-latency inference (no neural network forward pass)
</context>

<definition_of_done>
  <signatures>
```rust
use bitvec::prelude::*;

pub struct HdcModel {
    /// Base hypervectors for each character (random, frozen)
    base_vectors: HashMap<char, BitVec<u64, Lsb0>>,
    /// Position encoding vectors for sequence binding
    position_vectors: Vec<BitVec<u64, Lsb0>>,
    /// HDC dimension in bits
    d_hdc: usize,  // 10240 bits
    /// Output dimension after projection
    d_output: usize,  // 1024 floats
    /// Projection matrix from binary to float
    projection: Vec<f32>,
    /// Whether model is initialized
    initialized: AtomicBool,
}

impl HdcModel {
    pub fn new(config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Initialize random base vectors (one-time operation)
    fn initialize_base_vectors(&mut self, seed: u64);

    /// Encode a single character to hypervector
    pub fn encode_char(&self, c: char) -> BitVec<u64, Lsb0>;

    /// Encode a string using holographic reduced representation
    pub fn encode_string(&self, text: &str) -> BitVec<u64, Lsb0>;

    /// XOR binding of two hypervectors
    pub fn bind(a: &BitVec<u64, Lsb0>, b: &BitVec<u64, Lsb0>) -> BitVec<u64, Lsb0>;

    /// Majority bundling of multiple hypervectors
    pub fn bundle(vectors: &[BitVec<u64, Lsb0>]) -> BitVec<u64, Lsb0>;

    /// Permutation (rotation) for position encoding
    pub fn permute(v: &BitVec<u64, Lsb0>, positions: usize) -> BitVec<u64, Lsb0>;

    /// Project binary vector to float vector
    pub fn project_to_float(&self, binary: &BitVec<u64, Lsb0>) -> Vec<f32>;

    /// Hamming distance between two hypervectors
    pub fn hamming_distance(a: &BitVec<u64, Lsb0>, b: &BitVec<u64, Lsb0>) -> usize;

    /// Normalized Hamming similarity (0 to 1)
    pub fn similarity(a: &BitVec<u64, Lsb0>, b: &BitVec<u64, Lsb0>) -> f32;
}

#[async_trait]
impl EmbeddingModel for HdcModel {
    fn model_id(&self) -> ModelId { ModelId::Hdc }
    fn dimension(&self) -> usize { 1024 }
    fn max_tokens(&self) -> usize { 4096 }  // Characters, not tokens
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text, InputType::Code] }

    fn is_loaded(&self) -> bool { self.initialized.load(Ordering::SeqCst) }
    async fn load(&self) -> EmbeddingResult<()>;  // Initialize base vectors
    async fn unload(&self) -> EmbeddingResult<()>;  // Clear vectors

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool { true }  // No warmup needed
}
```
  </signatures>

  <constraints>
    <constraint>Uses bitvec crate for efficient bit operations</constraint>
    <constraint>is_custom() returns true (no pretrained weights)</constraint>
    <constraint>load() initializes random base vectors (deterministic with seed)</constraint>
    <constraint>d_hdc = 10240 bits for high-dimensional representation</constraint>
    <constraint>d_output = 1024 floats after projection</constraint>
    <constraint>XOR for binding, majority vote for bundling</constraint>
    <constraint>Circular permutation for position encoding</constraint>
    <constraint>Projects 10K-bit to 1024D float via learned projection</constraint>
    <constraint>Thread-safe with atomic initialization flag</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/custom/hdc.rs</file>
  <file>crates/context-graph-embeddings/src/models/custom/mod.rs (update exports)</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>HdcModel::new() creates valid instance</criterion>
  <criterion>Base vectors are deterministic with same seed</criterion>
  <criterion>bind() is commutative: A XOR B = B XOR A</criterion>
  <criterion>bind() is self-inverse: A XOR A = 0</criterion>
  <criterion>bundle() produces representative vector</criterion>
  <criterion>embed() produces 1024D float vector</criterion>
  <criterion>Hamming similarity in range [0, 1]</criterion>
  <criterion>Similar strings have similarity greater than 0.5</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### HDC Theory Background
Hyperdimensional Computing uses high-dimensional (typically 10,000+ dimensions)
binary or bipolar vectors for symbolic computation. Key properties:

1. **Quasi-orthogonality**: Random high-D vectors are nearly orthogonal
2. **Holographic**: Information distributed across all dimensions
3. **Noise-robust**: Errors in individual bits don't destroy meaning

### Holographic Reduced Representation (HRR)

For encoding sequences, we use:
```rust
fn encode_string(&self, text: &str) -> BitVec<u64, Lsb0> {
    let chars: Vec<char> = text.chars().collect();

    // Encode each character with position binding
    let encoded: Vec<BitVec<u64, Lsb0>> = chars.iter().enumerate()
        .map(|(pos, &c)| {
            let char_vec = self.encode_char(c);
            let pos_vec = &self.position_vectors[pos % self.position_vectors.len()];
            Self::bind(&char_vec, pos_vec)  // Position-bound character
        })
        .collect();

    // Bundle all position-bound characters
    Self::bundle(&encoded)
}
```

### Majority Bundling
```rust
fn bundle(vectors: &[BitVec<u64, Lsb0>]) -> BitVec<u64, Lsb0> {
    let n = vectors.len();
    let d = vectors[0].len();
    let threshold = n / 2;

    let mut result = bitvec![u64, Lsb0; 0; d];

    for i in 0..d {
        let count = vectors.iter().filter(|v| v[i]).count();
        result.set(i, count > threshold);
    }

    result
}
```

### Binary to Float Projection
```rust
fn project_to_float(&self, binary: &BitVec<u64, Lsb0>) -> Vec<f32> {
    // Simple projection: group bits and compute means
    let groups = self.d_hdc / self.d_output;  // 10240 / 1024 = 10

    (0..self.d_output)
        .map(|i| {
            let start = i * groups;
            let end = start + groups;
            let ones = binary[start..end].count_ones();
            (ones as f32 / groups as f32) * 2.0 - 1.0  // Scale to [-1, 1]
        })
        .collect()
}
```
