<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L02" version="1.0">
<metadata>
  <title>MemoryTracker for GPU/CPU Resource Management</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>2</sequence>
  <implements>PRD: GPU memory management, resource tracking</implements>
  <depends_on>M03-F15</depends_on>
  <estimated_hours>2</estimated_hours>
</metadata>

<context>
Implement MemoryTracker for tracking GPU and CPU memory usage by loaded models.
This component prevents out-of-memory conditions by enforcing memory budgets
and providing allocation/deallocation tracking per device.

Key responsibilities:
- Track GPU memory per device (multi-GPU support)
- Track CPU memory for CPU-placed models
- Enforce memory limits before allocation
- Provide available memory queries
- Support RTX 5090 with 32GB VRAM (default 24GB usable)
</context>

<definition_of_done>
  <signatures>
```rust
pub struct MemoryTracker {
    gpu_memory: HashMap<u32, GpuMemoryInfo>,
    cpu_memory: usize,
    max_gpu_memory: HashMap<u32, usize>,
    max_cpu_memory: usize,
}

#[derive(Debug, Clone)]
pub struct GpuMemoryInfo {
    pub allocated: usize,
    pub peak: usize,
    pub allocation_count: u32,
}

impl MemoryTracker {
    /// Create new tracker from GPU configuration
    pub fn new(gpu_config: &GpuConfig) -> Self;

    /// Check if allocation is possible without exceeding limits
    pub fn can_allocate(&self, device: DevicePlacement, bytes: usize) -> bool;

    /// Record an allocation
    pub fn allocate(&mut self, device: DevicePlacement, bytes: usize) -> EmbeddingResult<()>;

    /// Record a deallocation
    pub fn deallocate(&mut self, device: DevicePlacement, bytes: usize);

    /// Get available memory for device
    pub fn available(&self, device: DevicePlacement) -> usize;

    /// Get total allocated memory for device
    pub fn allocated(&self, device: DevicePlacement) -> usize;

    /// Get peak memory usage for device
    pub fn peak(&self, device: DevicePlacement) -> usize;

    /// Reset peak tracking
    pub fn reset_peak(&mut self);

    /// Get summary of all memory usage
    pub fn summary(&self) -> MemorySummary;
}

#[derive(Debug, Clone)]
pub struct MemorySummary {
    pub gpu_allocated: HashMap<u32, usize>,
    pub gpu_available: HashMap<u32, usize>,
    pub cpu_allocated: usize,
    pub cpu_available: usize,
    pub total_peak: usize,
}
```
  </signatures>

  <constraints>
    <constraint>GPU memory limit derived from GpuConfig.memory_fraction (default 0.9)</constraint>
    <constraint>CPU memory limit configurable, defaults to 16GB</constraint>
    <constraint>DevicePlacement::Auto maps to first available GPU or CPU fallback</constraint>
    <constraint>Allocation fails with error if would exceed limit</constraint>
    <constraint>Thread-safe when used with RwLock wrapper in ModelRegistry</constraint>
    <constraint>Peak tracking records maximum allocation reached</constraint>
  </constraints>

  <verification>
    <step>can_allocate returns false when limit would be exceeded</step>
    <step>allocate fails with EmbeddingError when over budget</step>
    <step>deallocate correctly reduces tracked memory</step>
    <step>available() returns max - allocated</step>
    <step>Multi-GPU tracking works with different device IDs</step>
    <step>Peak memory tracked accurately</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/memory.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test passes</criterion>
  <criterion>Memory accounting accurate to byte level</criterion>
  <criterion>No overflow in arithmetic operations</criterion>
</validation_criteria>
</task_spec>
