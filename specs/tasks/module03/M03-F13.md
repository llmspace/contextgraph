# M03-F13: BatchConfig Struct

```xml
<task_spec id="M03-F13" version="1.0">
<metadata>
  <title>Define BatchConfig for Batch Processing</title>
  <status>ready</status>
  <layer>foundation</layer>
  <sequence>13</sequence>
  <implements>constitution.yaml:embeddings.batch, PRD Section 4.3</implements>
  <depends_on>none</depends_on>
  <estimated_hours>1</estimated_hours>
</metadata>

<context>
Configuration for dynamic batch processing of embedding requests.
The batch processor accumulates requests and triggers batch inference when:
- Batch reaches max_batch_size, OR
- max_wait_ms timeout expires

This enables high throughput (>100 items/sec) by amortizing model invocation overhead.
</context>

<definition_of_done>
  <signatures>
```rust
use serde::{Deserialize, Serialize};

/// Configuration for batch processing.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchConfig {
    /// Maximum batch size before triggering inference.
    /// Default: 32
    pub max_batch_size: usize,

    /// Minimum batch size (wait for more if below).
    /// Default: 1
    pub min_batch_size: usize,

    /// Maximum wait time in milliseconds before triggering partial batch.
    /// Default: 50
    pub max_wait_ms: u64,

    /// Enable dynamic batching based on load.
    /// Default: true
    pub dynamic_batching: bool,

    /// Padding strategy for variable-length inputs.
    pub padding_strategy: PaddingStrategy,

    /// Sort inputs by length before batching (reduces padding waste).
    /// Default: true
    pub sort_by_length: bool,
}

/// Padding strategy for variable-length sequences in a batch.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, Default)]
#[serde(rename_all = "snake_case")]
pub enum PaddingStrategy {
    /// Pad all sequences to the model's max_tokens.
    MaxLength,

    /// Pad to the longest sequence in the batch.
    #[default]
    DynamicMax,

    /// Pad to next power of two (cache-friendly).
    PowerOfTwo,

    /// Use predefined length buckets (64, 128, 256, 512).
    Bucket,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 32,
            min_batch_size: 1,
            max_wait_ms: 50,
            dynamic_batching: true,
            padding_strategy: PaddingStrategy::DynamicMax,
            sort_by_length: true,
        }
    }
}

impl BatchConfig {
    /// Validate batch configuration values.
    pub fn validate(&self) -> Result<(), String> {
        if self.max_batch_size == 0 {
            return Err("max_batch_size must be > 0".to_string());
        }
        if self.min_batch_size > self.max_batch_size {
            return Err("min_batch_size cannot exceed max_batch_size".to_string());
        }
        if self.max_wait_ms == 0 && self.min_batch_size > 1 {
            return Err("max_wait_ms must be > 0 when min_batch_size > 1".to_string());
        }
        Ok(())
    }
}
```
  </signatures>

  <constraints>
    - max_batch_size must be > 0
    - min_batch_size <= max_batch_size
    - max_wait_ms should be 10-100ms for latency-sensitive applications
    - PaddingStrategy must be Copy for efficiency
    - Default uses DynamicMax padding (most memory efficient)
    - sort_by_length reduces padding waste by grouping similar lengths
  </constraints>

  <verification>
    - Default::default() passes validate()
    - Serde roundtrip works correctly
    - validate() rejects max_batch_size = 0
    - validate() rejects min > max
    - All PaddingStrategy variants serialize to snake_case
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/config.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check --package context-graph-embeddings passes</criterion>
  <criterion>cargo test --package context-graph-embeddings passes</criterion>
  <criterion>cargo clippy --package context-graph-embeddings -- -D warnings shows 0 warnings</criterion>
  <criterion>Default config passes validation</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Performance Considerations
- `max_batch_size = 32` balances throughput vs memory
- `max_wait_ms = 50` keeps P95 latency under 200ms
- `sort_by_length = true` can reduce padding by 20-40%

### Example TOML
```toml
[batch]
max_batch_size = 32
min_batch_size = 1
max_wait_ms = 50
dynamic_batching = true
padding_strategy = "dynamic_max"
sort_by_length = true
```

### Test Cases
1. `test_default_values` - All defaults correct
2. `test_validate_valid` - Default passes
3. `test_validate_zero_batch_size` - Returns error
4. `test_validate_min_exceeds_max` - Returns error
5. `test_padding_strategy_variants` - All 4 variants
6. `test_serde_roundtrip` - Serialize/deserialize equality

---

*Task ID: M03-F13*
*Module: 03 - 12-Model Embedding Pipeline*
*Layer: Foundation*
*Created: 2026-01-01*
