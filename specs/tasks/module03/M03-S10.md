<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-S10" version="1.0">
<metadata>
  <title>Integration Tests</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>46</sequence>
  <implements>
    - PRD: End-to-end pipeline verification
    - Quality gate: Pipeline Operational
    - Performance targets: &lt;200ms latency, &gt;100/sec throughput
  </implements>
  <depends_on>
    - M03-S01 (EmbeddingPipeline Core)
    - M03-S03 (EmbeddingProvider Bridge)
  </depends_on>
  <estimated_hours>6</estimated_hours>
</metadata>

<context>
Create end-to-end integration tests for the embedding pipeline. These tests
exercise the full flow from text input through all 12 models to the final
1536D FuseMoE output. Tests use real downloaded models (not mocks) and run
on actual GPU hardware to verify production-like behavior.

Integration tests validate:
- Complete embedding pipeline functionality
- Performance meets latency and throughput targets
- Cache behavior (hits and misses)
- Model hot-swap without request drops
- GPU memory stays within budget
- Error handling and recovery
</context>

<definition_of_done>
  <signatures>
    <signature name="test_module_structure">
// crates/context-graph-embeddings/tests/integration/mod.rs
mod pipeline_tests;
mod batch_tests;
mod cache_tests;
mod hotswap_tests;
mod memory_tests;
mod latency_tests;
    </signature>
    <signature name="pipeline_tests">
// crates/context-graph-embeddings/tests/integration/pipeline_tests.rs
#[tokio::test]
async fn test_e2e_text_to_1536d_embedding();

#[tokio::test]
async fn test_e2e_code_embedding();

#[tokio::test]
async fn test_e2e_multimodal_text();

#[tokio::test]
async fn test_pipeline_initialization();

#[tokio::test]
async fn test_pipeline_health_check();

#[tokio::test]
async fn test_pipeline_shutdown_cleanup();

#[tokio::test]
async fn test_embedding_provider_compatibility();
    </signature>
    <signature name="batch_tests">
// crates/context-graph-embeddings/tests/integration/batch_tests.rs
#[tokio::test]
async fn test_batch_throughput_100_per_sec();

#[tokio::test]
async fn test_batch_32_items();

#[tokio::test]
async fn test_batch_varying_lengths();

#[tokio::test]
async fn test_batch_concurrent_submissions();
    </signature>
    <signature name="cache_tests">
// crates/context-graph-embeddings/tests/integration/cache_tests.rs
#[tokio::test]
async fn test_cache_hit_returns_identical_embedding();

#[tokio::test]
async fn test_cache_miss_computes_embedding();

#[tokio::test]
async fn test_cache_hit_latency_under_1ms();

#[tokio::test]
async fn test_cache_eviction_lru();

#[tokio::test]
async fn test_cache_persistence_roundtrip();
    </signature>
    <signature name="hotswap_tests">
// crates/context-graph-embeddings/tests/integration/hotswap_tests.rs
#[tokio::test]
async fn test_hotswap_no_dropped_requests();

#[tokio::test]
async fn test_hotswap_validation_rollback();

#[tokio::test]
async fn test_hotswap_concurrent_with_requests();
    </signature>
    <signature name="memory_tests">
// crates/context-graph-embeddings/tests/integration/memory_tests.rs
#[tokio::test]
async fn test_gpu_memory_within_budget();

#[tokio::test]
async fn test_memory_tracker_accuracy();

#[tokio::test]
async fn test_model_unload_frees_memory();
    </signature>
    <signature name="latency_tests">
// crates/context-graph-embeddings/tests/integration/latency_tests.rs
#[tokio::test]
async fn test_single_embed_under_200ms_p95();

#[tokio::test]
async fn test_fusemoe_under_3ms();

#[tokio::test]
async fn test_semantic_model_under_5ms();

#[tokio::test]
async fn test_code_model_under_10ms();
    </signature>
  </signatures>

  <constraints>
    <constraint>Use real downloaded models (no stubs or mocks)</constraint>
    <constraint>No mock data - generate real embeddings</constraint>
    <constraint>Run on actual GPU hardware (skip if unavailable with #[ignore])</constraint>
    <constraint>Tests must be reproducible (fixed random seeds)</constraint>
    <constraint>Clean up resources after each test</constraint>
    <constraint>Use tokio::test for async tests</constraint>
    <constraint>Mark long-running tests with appropriate timeouts</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/tests/integration/mod.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/pipeline_tests.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/batch_tests.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/cache_tests.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/hotswap_tests.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/memory_tests.rs</file>
  <file>crates/context-graph-embeddings/tests/integration/latency_tests.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo test --package context-graph-embeddings --test integration passes</criterion>
  <criterion>E2E test produces 1536D output with valid values</criterion>
  <criterion>Batch throughput exceeds 100 items/sec at batch size 32</criterion>
  <criterion>Cache hit returns in under 1ms</criterion>
  <criterion>Hot-swap completes without dropped requests</criterion>
  <criterion>GPU memory usage stays under 24GB</criterion>
  <criterion>All latency targets met at P95</criterion>
</validation_criteria>

<test_specifications>
  <test name="test_e2e_text_to_1536d_embedding">
    <description>
      Full pipeline test: Input text, process through all 12 models,
      fuse via FuseMoE, output 1536D embedding.
    </description>
    <input>"The quick brown fox jumps over the lazy dog."</input>
    <expected>
      - FusedEmbedding with vector.len() == 1536
      - All values in [-1.0, 1.0] after normalization
      - expert_weights sum to approximately 1.0
      - pipeline_latency_us &lt; 200_000
    </expected>
  </test>

  <test name="test_batch_throughput_100_per_sec">
    <description>
      Verify batch processing achieves target throughput.
    </description>
    <input>320 sample texts (10 batches of 32)</input>
    <expected>
      - All 320 embeddings completed
      - Total time &lt; 3.2 seconds (100/sec)
      - No errors in batch
    </expected>
  </test>

  <test name="test_cache_hit_returns_identical_embedding">
    <description>
      Verify cache returns identical embedding for same input.
    </description>
    <input>Same text embedded twice</input>
    <expected>
      - First call: cache miss, computes embedding
      - Second call: cache hit, returns identical vector
      - Vectors are byte-for-byte identical
    </expected>
  </test>

  <test name="test_hotswap_no_dropped_requests">
    <description>
      Verify hot-swap queues requests without drops.
    </description>
    <input>
      - Start continuous embedding requests
      - Trigger model hot-swap mid-stream
      - Continue requests until swap complete
    </input>
    <expected>
      - No requests return error during swap
      - All requests get valid embeddings
      - Requests during swap may have higher latency (queued)
    </expected>
  </test>

  <test name="test_gpu_memory_within_budget">
    <description>
      Verify all 12 models loaded stay within 24GB GPU memory.
    </description>
    <input>Load all 12 models</input>
    <expected>
      - Total GPU memory &lt; 24GB
      - MemoryTracker reports accurate usage
      - No OOM errors
    </expected>
  </test>
</test_specifications>

<test_data>
  <sample_texts>
    - "The quick brown fox jumps over the lazy dog."
    - "Machine learning models process natural language."
    - "fn main() { println!(\"Hello, world!\"); }"
    - "SELECT * FROM users WHERE active = true;"
    - "The mitochondria is the powerhouse of the cell."
  </sample_texts>
</test_data>

<notes>
Use #[ignore] attribute for tests requiring specific hardware.
Consider using test fixtures loaded from test-data/ directory.
Add retry logic for flaky GPU initialization.
Log detailed metrics for debugging failed performance tests.
</notes>
</task_spec>
