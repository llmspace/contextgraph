# Task Specification: M03-L30

```xml
<task_spec id="M03-L30" version="1.0">
<metadata>
  <title>Grouped GEMM for MoE Expert Execution</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>30</sequence>
  <implements>
    - RTX 5090 Technical Report: Grouped GEMM 4x speedup for MoE
    - PRD: <3ms FuseMoE fusion target
    - Architecture: Parallel expert execution optimization
  </implements>
  <depends_on>M03-L21, M03-L22, M03-S04</depends_on>
  <estimated_hours>5</estimated_hours>
</metadata>

<context>
The RTX 5090 technical report explicitly mentions **Grouped GEMM** for a 4x speedup
in MoE (Mixture of Experts) models. Standard GEMM runs experts one-by-one, but
Grouped GEMM (available in CUDA 13.1+ and cuBLAS) allows the GPU to process all
selected experts in a single kernel launch.

Without Grouped GEMM, the FuseMoE layer (M03-L21) executes 8 expert networks
sequentially or with separate kernel launches, creating significant overhead.
With Grouped GEMM, all k active experts (typically k=2) execute in parallel
within a single kernel, dramatically reducing fusion latency.

Key features:
1. Batched expert execution via cuBLAS grouped GEMM
2. CUTLASS integration for custom MoE kernels
3. Dynamic expert routing with batched execution
4. Memory-efficient expert weight organization
5. Fallback to standard GEMM for non-Blackwell GPUs

Performance targets:
- Expert execution: <1ms for k=2 active experts
- Kernel launch overhead: <50μs
- Memory bandwidth: Near-peak utilization
- Total FuseMoE: <3ms (including gating)
</context>

<definition_of_done>
  <signatures>
```rust
use std::sync::Arc;

/// Configuration for grouped GEMM execution
#[derive(Debug, Clone)]
pub struct GroupedGemmConfig {
    /// Number of expert groups to batch
    pub group_count: usize,
    /// GEMM algorithm selection
    pub algorithm: GemmAlgorithm,
    /// Use tensor cores if available
    pub use_tensor_cores: bool,
    /// Workspace size for cuBLAS
    pub workspace_mb: usize,
    /// Precision for computation
    pub compute_precision: ComputePrecision,
}

/// GEMM algorithm variants
#[derive(Debug, Clone, Copy)]
pub enum GemmAlgorithm {
    /// cuBLAS default heuristic
    CublasDefault,
    /// cuBLAS grouped GEMM (Blackwell optimized)
    CublasGrouped,
    /// CUTLASS custom kernel
    Cutlass,
    /// Fallback sequential execution
    Sequential,
}

/// Compute precision for GEMM
#[derive(Debug, Clone, Copy)]
pub enum ComputePrecision {
    FP32,
    FP16,
    TF32,
    FP8,
}

/// Expert weight layout optimized for grouped GEMM
#[derive(Debug)]
pub struct GroupedExpertWeights {
    /// All expert weights packed contiguously
    /// Shape: [num_experts, hidden_dim, expert_dim]
    pub weights: Tensor,
    /// Expert biases (optional)
    /// Shape: [num_experts, expert_dim]
    pub biases: Option<Tensor>,
    /// Number of experts
    pub num_experts: usize,
    /// Hidden dimension
    pub hidden_dim: usize,
    /// Expert output dimension
    pub expert_dim: usize,
    /// Device placement
    pub device: Device,
}

/// Input batch prepared for grouped GEMM
#[derive(Debug)]
pub struct GroupedGemmInput {
    /// Batched input tokens assigned to experts
    /// Shape: [total_tokens, hidden_dim]
    pub inputs: Tensor,
    /// Expert indices for each token
    /// Shape: [total_tokens]
    pub expert_indices: Vec<usize>,
    /// Token counts per expert
    /// Shape: [num_experts]
    pub tokens_per_expert: Vec<usize>,
    /// Cumulative offsets for expert batches
    pub expert_offsets: Vec<usize>,
}

/// Grouped GEMM executor for MoE
pub struct GroupedGemmExecutor {
    config: GroupedGemmConfig,
    cublas_handle: CublasHandle,
    workspace: GpuBuffer,
    device_caps: DeviceCapabilities,
}

impl GroupedGemmExecutor {
    /// Create executor with device capability detection
    pub fn new(device: &Device, config: GroupedGemmConfig) -> CudaResult<Self>;

    /// Check if grouped GEMM is available
    pub fn supports_grouped_gemm(&self) -> bool;

    /// Execute grouped GEMM for MoE forward pass
    ///
    /// This is the core optimization - runs all active experts
    /// in a single kernel launch rather than sequential launches.
    pub fn execute_grouped(
        &self,
        inputs: &GroupedGemmInput,
        weights: &GroupedExpertWeights,
    ) -> CudaResult<Tensor>;

    /// Execute sequential GEMM (fallback)
    pub fn execute_sequential(
        &self,
        inputs: &GroupedGemmInput,
        weights: &GroupedExpertWeights,
    ) -> CudaResult<Tensor>;

    /// Prepare input batch for grouped execution
    pub fn prepare_input(
        &self,
        tokens: &Tensor,
        routing: &RoutingDecision,
    ) -> CudaResult<GroupedGemmInput>;

    /// Scatter output back to original token order
    pub fn scatter_output(
        &self,
        expert_outputs: &Tensor,
        input: &GroupedGemmInput,
        original_shape: &[usize],
    ) -> CudaResult<Tensor>;

    /// Get optimal algorithm for current batch
    pub fn select_algorithm(
        &self,
        batch_size: usize,
        active_experts: usize,
    ) -> GemmAlgorithm;
}

/// Expert weight manager for grouped GEMM
pub struct ExpertWeightManager {
    /// Packed weights for all experts
    weights: GroupedExpertWeights,
    /// Optional quantized weights
    quantized: Option<QuantizedExpertWeights>,
}

impl ExpertWeightManager {
    /// Load and pack expert weights for grouped execution
    pub fn load(
        expert_weights: &[Tensor],
        expert_biases: Option<&[Tensor]>,
        device: &Device,
    ) -> CudaResult<Self>;

    /// Get weights reference
    pub fn weights(&self) -> &GroupedExpertWeights;

    /// Get quantized weights if available
    pub fn quantized_weights(&self) -> Option<&QuantizedExpertWeights>;

    /// Quantize weights for FP8/FP4 execution
    pub fn quantize(&mut self, quantizer: &BlackwellQuantizer) -> CudaResult<()>;
}

/// Quantized expert weights for FP8/FP4 grouped GEMM
#[derive(Debug)]
pub struct QuantizedExpertWeights {
    pub weights: QuantizedTensor,
    pub biases: Option<Tensor>,
    pub format: QuantFormat,
}
```
  </signatures>

  <constraints>
    <constraint>Grouped GEMM requires cuBLAS 13.1+ or CUTLASS</constraint>
    <constraint>Fallback to sequential for Ampere and older GPUs</constraint>
    <constraint>Expert weights must be contiguously packed</constraint>
    <constraint>Token-to-expert mapping must be materialized before GEMM</constraint>
    <constraint>Workspace allocation reused across calls</constraint>
    <constraint>Thread-safe for concurrent MoE inference</constraint>
    <constraint>Must handle variable tokens per expert</constraint>
    <constraint>Scatter operation must preserve token order</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-cuda/src/gemm/mod.rs</file>
  <file>crates/context-graph-cuda/src/gemm/grouped.rs</file>
  <file>crates/context-graph-cuda/src/gemm/expert_weights.rs</file>
  <file>crates/context-graph-cuda/src/gemm/kernels.cu</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>Grouped GEMM detected on Blackwell (SM 10.0+)</criterion>
  <criterion>Fallback works on older GPUs</criterion>
  <criterion>2-expert execution completes in <1ms</criterion>
  <criterion>Output dimensions correct after scatter</criterion>
  <criterion>Numerical accuracy within 1e-4 of sequential</criterion>
  <criterion>Workspace memory bounded and reused</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Grouped GEMM Concept
```
Traditional MoE (Sequential):
┌─────────────┐   ┌─────────────┐   ┌─────────────┐
│ Token Batch │──►│  Expert 0   │──►│  Output 0   │
└─────────────┘   └─────────────┘   └─────────────┘
                         ↓
┌─────────────┐   ┌─────────────┐   ┌─────────────┐
│ Token Batch │──►│  Expert 1   │──►│  Output 1   │
└─────────────┘   └─────────────┘   └─────────────┘
      ...              ...              ...

Total: N kernel launches for N experts

Grouped GEMM MoE (Single Launch):
┌─────────────────────────────────────────────────┐
│                 Grouped GEMM                     │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐         │
│  │Expert 0 │  │Expert 1 │  │Expert 2 │  ...    │
│  └────┬────┘  └────┬────┘  └────┬────┘         │
│       │            │            │               │
│       ▼            ▼            ▼               │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐         │
│  │Output 0 │  │Output 1 │  │Output 2 │  ...    │
│  └─────────┘  └─────────┘  └─────────┘         │
└─────────────────────────────────────────────────┘

Total: 1 kernel launch for all experts
```

### cuBLAS Grouped GEMM API
```rust
// Pseudo-code for cuBLAS grouped GEMM
impl GroupedGemmExecutor {
    pub fn execute_grouped(
        &self,
        inputs: &GroupedGemmInput,
        weights: &GroupedExpertWeights,
    ) -> CudaResult<Tensor> {
        let num_experts = weights.num_experts;

        // Prepare GEMM descriptors for each expert group
        let mut alpha = vec![1.0f32; num_experts];
        let mut beta = vec![0.0f32; num_experts];

        // A matrices: input tokens for each expert
        let mut a_ptrs: Vec<*const f32> = Vec::with_capacity(num_experts);
        let mut m_array: Vec<i32> = Vec::with_capacity(num_experts);

        // B matrices: expert weights
        let mut b_ptrs: Vec<*const f32> = Vec::with_capacity(num_experts);

        // C matrices: output buffers
        let mut c_ptrs: Vec<*mut f32> = Vec::with_capacity(num_experts);

        for expert_idx in 0..num_experts {
            let token_count = inputs.tokens_per_expert[expert_idx];
            if token_count == 0 {
                continue;
            }

            let offset = inputs.expert_offsets[expert_idx];

            // Input slice for this expert
            a_ptrs.push(inputs.inputs.ptr_at(offset));
            m_array.push(token_count as i32);

            // Weight matrix for this expert
            b_ptrs.push(weights.weights.ptr_at(expert_idx * weights.hidden_dim * weights.expert_dim));

            // Output buffer
            c_ptrs.push(output_buffer.ptr_at_mut(offset));
        }

        // Execute grouped GEMM
        unsafe {
            cublas_sys::cublasGemmGroupedBatchedEx(
                self.cublas_handle.as_raw(),
                cublas_sys::CUBLAS_OP_N,
                cublas_sys::CUBLAS_OP_N,
                n_array.as_ptr(),      // Expert dims
                m_array.as_ptr(),      // Token counts
                k_array.as_ptr(),      // Hidden dims
                alpha.as_ptr(),
                b_ptrs.as_ptr(),       // Weights
                ldb_array.as_ptr(),
                a_ptrs.as_ptr(),       // Inputs
                lda_array.as_ptr(),
                beta.as_ptr(),
                c_ptrs.as_ptr(),       // Outputs
                ldc_array.as_ptr(),
                active_experts,
                cublas_sys::CUDA_R_32F,
                cublas_sys::CUBLAS_COMPUTE_32F,
            )?;
        }

        Ok(output_tensor)
    }
}
```

### Token-to-Expert Preparation
```rust
impl GroupedGemmExecutor {
    pub fn prepare_input(
        &self,
        tokens: &Tensor,
        routing: &RoutingDecision,
    ) -> CudaResult<GroupedGemmInput> {
        let batch_size = tokens.shape()[0];
        let hidden_dim = tokens.shape()[1];
        let num_experts = routing.num_experts;

        // Count tokens per expert
        let mut tokens_per_expert = vec![0usize; num_experts];
        for &expert_idx in &routing.selected_experts {
            tokens_per_expert[expert_idx] += 1;
        }

        // Compute offsets
        let mut expert_offsets = vec![0usize; num_experts + 1];
        for i in 0..num_experts {
            expert_offsets[i + 1] = expert_offsets[i] + tokens_per_expert[i];
        }

        // Gather tokens to expert groups
        let total_tokens: usize = tokens_per_expert.iter().sum();
        let mut gathered = Tensor::zeros(&[total_tokens, hidden_dim], tokens.device())?;

        let mut expert_counters = vec![0usize; num_experts];
        let mut expert_indices = Vec::with_capacity(total_tokens);

        for (token_idx, &expert_idx) in routing.selected_experts.iter().enumerate() {
            let dest_idx = expert_offsets[expert_idx] + expert_counters[expert_idx];
            gathered.copy_row_from(dest_idx, tokens, token_idx)?;
            expert_indices.push(expert_idx);
            expert_counters[expert_idx] += 1;
        }

        Ok(GroupedGemmInput {
            inputs: gathered,
            expert_indices,
            tokens_per_expert,
            expert_offsets: expert_offsets[..num_experts].to_vec(),
        })
    }
}
```

### Output Scatter
```rust
impl GroupedGemmExecutor {
    pub fn scatter_output(
        &self,
        expert_outputs: &Tensor,
        input: &GroupedGemmInput,
        original_shape: &[usize],
    ) -> CudaResult<Tensor> {
        let batch_size = original_shape[0];
        let expert_dim = expert_outputs.shape()[1];

        let mut output = Tensor::zeros(&[batch_size, expert_dim], expert_outputs.device())?;

        // Scatter back to original positions
        let mut expert_counters = vec![0usize; input.tokens_per_expert.len()];

        for original_idx in 0..batch_size {
            let expert_idx = input.expert_indices[original_idx];
            let gathered_idx = input.expert_offsets[expert_idx] + expert_counters[expert_idx];

            output.copy_row_from(original_idx, expert_outputs, gathered_idx)?;
            expert_counters[expert_idx] += 1;
        }

        Ok(output)
    }
}
```

### Integration with FuseMoE
```rust
impl ExpertPool {
    pub async fn forward(
        &self,
        tokens: &Tensor,
        routing: &RoutingDecision,
    ) -> CudaResult<Tensor> {
        // Use grouped GEMM if available
        if self.gemm_executor.supports_grouped_gemm() {
            let input = self.gemm_executor.prepare_input(tokens, routing)?;
            let expert_outputs = self.gemm_executor.execute_grouped(
                &input,
                &self.weight_manager.weights(),
            )?;
            self.gemm_executor.scatter_output(
                &expert_outputs,
                &input,
                tokens.shape(),
            )
        } else {
            // Fallback to sequential
            self.forward_sequential(tokens, routing).await
        }
    }
}
```

### CUTLASS Alternative (Custom Kernel)
```cpp
// kernels.cu - CUTLASS grouped GEMM for maximum performance

template <typename Element, typename Layout>
void grouped_gemm_moe(
    const Element* inputs,      // [total_tokens, hidden_dim]
    const Element* weights,     // [num_experts, hidden_dim, expert_dim]
    Element* outputs,           // [total_tokens, expert_dim]
    const int* expert_offsets,  // [num_experts + 1]
    const int* tokens_per_expert, // [num_experts]
    int num_experts,
    int hidden_dim,
    int expert_dim,
    cudaStream_t stream
) {
    using GemmKernel = cutlass::gemm::device::GemmGrouped<
        Element, Layout,
        Element, Layout,
        Element, Layout,
        float
    >;

    typename GemmKernel::Arguments args{
        cutlass::gemm::GemmCoord{expert_dim, max_tokens, hidden_dim},
        num_experts,
        // ... problem sizes per expert
    };

    GemmKernel gemm_op;
    gemm_op(args, nullptr, stream);
}
```

---
*Task ID: M03-L30*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
