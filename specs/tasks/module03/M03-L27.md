# Task Specification: M03-L27

```xml
<task_spec id="M03-L27" version="1.0">
<metadata>
  <title>EmbeddingPriors and Context Priming</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>27</sequence>
  <implements>
    - PRD Section 8: Predictive Coding mechanism
    - Technical Engine: L5 → L1 feedback loop
    - Constitution: bioinspired.predictive_coding
  </implements>
  <depends_on>
    - M03-F01 (ModelId)
    - M03-F14 (FusionConfig)
    - M03-L20 (GatingNetwork)
    - M03-L23 (FuseMoE)
  </depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement the Predictive Coding / Context Priming system that allows the Coherence layer
(L5) to "prime" the Embedding Pipeline (L1) with contextual expectations. This creates
a top-down feedback loop where knowledge about the current domain/context biases the
embedding process toward more relevant representations.

Example: If the Coherence layer knows we are in a "Medical" domain context, it can prime
the pipeline to weight the Semantic (E1) and Entity (E11) models higher, while reducing
emphasis on Code (E7).

This implements the predictive coding paradigm where higher layers send predictions
down to lower layers, and lower layers send prediction errors up.
</context>

<definition_of_done>
  <signatures>
```rust
/// Prior weights for each embedding model
#[derive(Debug, Clone)]
pub struct EmbeddingPriors {
    /// Weights per model (indexed by ModelId ordinal)
    pub model_weights: [f32; 12],
    /// Expected domain for context matching
    pub expected_domain: Option<DomainContext>,
    /// Confidence in the prior (0.0 - 1.0)
    pub confidence: f32,
    /// Time-to-live for this prior
    pub ttl: Duration,
    /// When this prior was created
    pub created_at: Instant,
}

/// Domain context for priming
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum DomainContext {
    Medical,
    Legal,
    Technical,
    Scientific,
    Financial,
    Creative,
    Conversational,
    CodeDevelopment,
    DataAnalysis,
    Custom(String),
}

/// Manages priors and applies them to the pipeline
pub struct PriorManager {
    active_priors: RwLock<Vec<EmbeddingPriors>>,
    domain_templates: HashMap<DomainContext, [f32; 12]>,
    default_weights: [f32; 12],
}

impl PriorManager {
    /// Create manager with domain templates
    pub fn new() -> Self;

    /// Add a prior (combines with existing priors)
    pub fn add_prior(&self, prior: EmbeddingPriors);

    /// Set a prior (replaces existing)
    pub fn set_prior(&self, prior: EmbeddingPriors);

    /// Clear all active priors
    pub fn clear_priors(&self);

    /// Get aggregated weights from all active priors
    pub fn get_aggregated_weights(&self) -> [f32; 12];

    /// Apply priors to FuseMoE gating scores
    pub fn apply_to_gating(&self, raw_scores: &[f32; 8]) -> [f32; 8];

    /// Prime for a specific domain context
    pub fn prime_for_domain(&self, domain: DomainContext, confidence: f32);

    /// Expire stale priors
    pub fn expire_stale(&self);

    /// Get prior for a domain
    pub fn domain_prior(&self, domain: &DomainContext) -> EmbeddingPriors;
}

/// Builder for constructing priors
pub struct PriorBuilder {
    weights: [f32; 12],
    domain: Option<DomainContext>,
    confidence: f32,
    ttl: Duration,
}

impl PriorBuilder {
    pub fn new() -> Self;
    pub fn boost_model(self, model: ModelId, factor: f32) -> Self;
    pub fn suppress_model(self, model: ModelId, factor: f32) -> Self;
    pub fn domain(self, domain: DomainContext) -> Self;
    pub fn confidence(self, confidence: f32) -> Self;
    pub fn ttl(self, ttl: Duration) -> Self;
    pub fn build(self) -> EmbeddingPriors;
}

/// Prediction error feedback (L1 → L5)
#[derive(Debug, Clone)]
pub struct PredictionError {
    /// Mismatch between expected and actual embeddings
    pub embedding_mismatch: f32,
    /// Which models contributed most to the error
    pub model_contributions: [f32; 12],
    /// Suggested prior adjustment
    pub suggested_adjustment: EmbeddingPriors,
}
```
  </signatures>

  <constraints>
    - Thread-safe: PriorManager accessed from multiple contexts
    - Prior aggregation: multiple priors combine via weighted average
    - Confidence-weighted: higher confidence priors have more influence
    - TTL expiration: stale priors automatically removed
    - Normalization: aggregated weights always sum to 1.0
    - Smooth blending: priors blend with defaults, not override
    - Domain templates predefined for common domains
    - Prior application <1μs overhead
  </constraints>

  <verification>
    <step>prime_for_domain("Medical") boosts E1, E11</step>
    <step>prime_for_domain("CodeDevelopment") boosts E7, E8</step>
    <step>Multiple priors aggregate correctly by confidence</step>
    <step>Expired priors are removed on expire_stale()</step>
    <step>Weights always normalized to sum to 1.0</step>
    <step>apply_to_gating correctly modifies expert scores</step>
    <step>PriorBuilder fluent API works correctly</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/priming/priors.rs</file>
  <file>crates/context-graph-embeddings/src/priming/manager.rs</file>
  <file>crates/context-graph-embeddings/src/priming/domains.rs</file>
  <file>crates/context-graph-embeddings/src/priming/mod.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test priming passes</criterion>
  <criterion>Domain templates cover all DomainContext variants</criterion>
  <criterion>Prior aggregation benchmark <1μs</criterion>
  <criterion>Thread-safety stress test passes</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Domain Templates

```rust
impl PriorManager {
    fn init_domain_templates() -> HashMap<DomainContext, [f32; 12]> {
        let mut templates = HashMap::new();

        // Medical: boost semantic, entity recognition
        templates.insert(DomainContext::Medical, [
            1.3,  // E1 Semantic (boost)
            1.0,  // E2 Recent
            1.0,  // E3 Periodic
            1.0,  // E4 Positional
            1.1,  // E5 Causal (slight boost for medical reasoning)
            0.8,  // E6 Sparse
            0.5,  // E7 Code (suppress)
            1.0,  // E8 Graph
            1.0,  // E9 HDC
            0.9,  // E10 Multimodal
            1.4,  // E11 Entity (boost for medical terms)
            0.9,  // E12 ColBERT
        ]);

        // Code Development: boost code, sparse, graph
        templates.insert(DomainContext::CodeDevelopment, [
            0.8,  // E1 Semantic (reduce)
            1.0,  // E2 Recent
            0.8,  // E3 Periodic
            1.0,  // E4 Positional
            1.1,  // E5 Causal
            1.3,  // E6 Sparse (boost for keyword matching)
            1.5,  // E7 Code (major boost)
            1.2,  // E8 Graph (boost for code structure)
            1.0,  // E9 HDC
            0.6,  // E10 Multimodal (suppress)
            0.9,  // E11 Entity
            1.0,  // E12 ColBERT
        ]);

        // ... other domains
        templates
    }
}
```

### Integration with FuseMoE

```rust
impl FuseMoE {
    pub fn fuse_with_priors(
        &self,
        concatenated: &ConcatenatedEmbedding,
        prior_manager: &PriorManager,
    ) -> EmbeddingResult<FusedEmbedding> {
        // Get raw gating scores
        let raw_scores = self.gating.forward(&concatenated.data)?;

        // Apply prior weights to expert scores
        let primed_scores = prior_manager.apply_to_gating(&raw_scores);

        // Continue with routing using primed scores
        let routing = self.router.route(&primed_scores)?;

        // ... rest of fusion
    }
}
```

### Prediction Error Feedback

```rust
impl PriorManager {
    /// Compute prediction error when actual context differs from expected
    pub fn compute_prediction_error(
        &self,
        expected: &EmbeddingPriors,
        actual_embedding: &FusedEmbedding,
        reference_embedding: &FusedEmbedding,
    ) -> PredictionError {
        let mismatch = cosine_distance(
            &actual_embedding.data,
            &reference_embedding.data,
        );

        // Analyze which models contributed to mismatch
        let contributions = self.analyze_contributions(
            actual_embedding,
            reference_embedding,
        );

        PredictionError {
            embedding_mismatch: mismatch,
            model_contributions: contributions,
            suggested_adjustment: self.suggest_adjustment(&contributions),
        }
    }
}
```

---
*Task ID: M03-L27*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
