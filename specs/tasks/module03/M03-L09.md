# M03-L09: Code Model (E7 - Salesforce/codet5p-110m-embedding)

## Metadata
| Field | Value |
|-------|-------|
| **Task ID** | M03-L09 |
| **Title** | Code Model (E7 - CodeT5+) |
| **Status** | complete |
| **Layer** | logic |
| **Sequence** | 9 |
| **Implements** | PRD-EMB-007: E7 Code embedding with AST-aware encoding |
| **Dependencies** | M03-F09 (EmbeddingModel trait - COMPLETE), M03-L01 (ModelRegistry - COMPLETE) |
| **Estimated Hours** | 4 |
| **Crate** | context-graph-embeddings |
| **Primary File** | `crates/context-graph-embeddings/src/models/pretrained/code.rs` |

---

## CRITICAL: Read This First

**ASSUME THIS TASK IS WRONG UNTIL VERIFIED**. Before implementing:
1. Verify ALL file paths and structures against the actual codebase
2. Confirm trait signatures match `EmbeddingModel` in `src/traits/embedding_model.rs`
3. Follow the exact pattern used in `semantic.rs`, `causal.rs`, `sparse.rs`
4. NO MOCK DATA - use stub implementations with deterministic hash-based outputs
5. **NO BACKWARDS COMPATIBILITY** - if something breaks, fail fast with clear errors

---

## Context

### What This Task Implements
Implement code embedding using `Salesforce/codet5p-110m-embedding` via candle-transformers. CodeT5+ is Salesforce's encoder-decoder model optimized specifically for code embedding with a dedicated 256D embedding head that projects to 768D.

### Why CodeT5+ Matters
- **Code-Focused Architecture**: T5 architecture fine-tuned specifically on code
- **Multi-Language Support**: Python, Java, JavaScript, PHP, Ruby, Go, C, C++, Rust
- **Dedicated Embedding Head**: 256D output optimized for code similarity
- **Smaller Footprint**: 110M parameters with fast inference
- **Code Search Optimized**: Designed for semantic code retrieval tasks

### Model Specifications (from model_id.rs - VERIFIED 2026-01-01)

The `model_id.rs` file defines these EXACT specifications that MUST be matched:

```rust
// From crates/context-graph-embeddings/src/types/model_id.rs lines 94, 115, 147
Self::Code => 256,     // CodeT5p embed_dim (internal d_model=768)
Self::Code => 768,     // 256 embed -> 768 via projection (projected_dimension)
Self::Code => Some("Salesforce/codet5p-110m-embedding"),
```

| Specification | Value | Source |
|--------------|-------|--------|
| HuggingFace Repo | `Salesforce/codet5p-110m-embedding` | model_id.rs:147 |
| Architecture | T5 Encoder + Embedding Head | HuggingFace |
| Native Dimension | 256 (embed_dim) | model_id.rs:94 |
| Internal Hidden Size | 768 (d_model) | HuggingFace |
| Projected Dimension | 768 | model_id.rs:115 |
| Max Position | 512 tokens | T5 default |
| Vocab Size | 32100 (T5 tokenizer) | HuggingFace |
| Latency Target | < 10ms P95 | constitution.yaml |

**DIMENSION FLOW** (CRITICAL - MUST match model_id.rs):
```
Input â†’ T5 Encoder (d_model=768) â†’ Embedding Head â†’ 256D native
                                                   â†“
                                    Projection Layer â†’ 768D projected
```

---

## Source of Truth

### Canonical References
1. **PRD Requirements**: `/home/cabdru/contextgraph/docs2/contextprd.md`
2. **Tech Constitution**: `/home/cabdru/contextgraph/docs2/constitution.yaml`
3. **Traceability**: `/home/cabdru/contextgraph/specs/tasks/module03/_traceability.md`
4. **Task Index**: `/home/cabdru/contextgraph/specs/tasks/module03/_index.md`
5. **CUDA Reference**: `/home/cabdru/contextgraph/docs2/CUDA-13-1-RTX-5090-Report.md`
6. **System Specs**: `/home/cabdru/contextgraph/docs2/systemspecs.md`

### Existing Codebase Files (VERIFIED 2026-01-01)
| File | Purpose | Status |
|------|---------|--------|
| `crates/context-graph-embeddings/src/types/model_id.rs` | ModelId::Code = 6, dimension() = 256, projected_dimension() = 768 | EXISTS |
| `crates/context-graph-embeddings/src/traits/embedding_model.rs` | EmbeddingModel trait | EXISTS |
| `crates/context-graph-embeddings/src/traits/model_factory.rs` | SingleModelConfig | EXISTS |
| `crates/context-graph-embeddings/src/models/pretrained/mod.rs` | Module exports | EXISTS - NEEDS UPDATE |
| `crates/context-graph-embeddings/src/models/pretrained/semantic.rs` | Reference pattern | EXISTS |
| `crates/context-graph-embeddings/src/models/pretrained/causal.rs` | Reference pattern | EXISTS |
| `crates/context-graph-embeddings/src/models/pretrained/sparse.rs` | Reference pattern | EXISTS |
| `crates/context-graph-embeddings/src/error.rs` | EmbeddingError types | EXISTS |

### ModelId Enum (VERIFIED from model_id.rs)
```rust
pub enum ModelId {
    Semantic = 0,          // E1 - e5-large-v2 (1024D)
    TemporalRecent = 1,    // E2 - Custom (512D)
    TemporalPeriodic = 2,  // E3 - Custom (512D)
    TemporalPositional = 3,// E4 - Custom (512D)
    Causal = 4,            // E5 - Longformer (768D)
    Sparse = 5,            // E6 - SPLADE (30522 -> 1536D)
    Code = 6,              // E7 - CodeT5+ (256D -> 768D projected) <-- THIS TASK
    Graph = 7,             // E8 - MiniLM (384D)
    Hdc = 8,               // E9 - Custom (10K-bit -> 1024D)
    Multimodal = 9,        // E10 - CLIP (768D)
    Entity = 10,           // E11 - MiniLM (384D)
    LateInteraction = 11,  // E12 - ColBERT (128D/token)
}

// dimension() returns 256 for Code
// projected_dimension() returns 768 for Code
// model_repo() returns "Salesforce/codet5p-110m-embedding" for Code
```

---

## Implementation Requirements

### Constants (MUST define in code.rs)
```rust
/// Native embedding dimension for CodeT5+ (from model_id.rs).
pub const CODE_NATIVE_DIMENSION: usize = 256;

/// Projected dimension for FuseMoE compatibility (from model_id.rs).
pub const CODE_PROJECTED_DIMENSION: usize = 768;

/// Maximum input tokens for CodeT5+.
pub const CODE_MAX_TOKENS: usize = 512;

/// Latency budget in milliseconds (P95 target from constitution.yaml).
pub const CODE_LATENCY_BUDGET_MS: u64 = 10;

/// Model identifier string (from model_id.rs).
pub const CODE_MODEL_NAME: &str = "Salesforce/codet5p-110m-embedding";

/// Vocabulary size for CodeT5+ (T5 tokenizer).
pub const CODE_VOCAB_SIZE: usize = 32100;

/// Supported programming languages for language-aware tokenization.
pub const SUPPORTED_LANGUAGES: &[&str] = &[
    "python", "java", "javascript", "php", "ruby", "go",
    "rust", "c", "cpp", "csharp", "typescript", "kotlin", "swift"
];
```

### ModelState Enum (follow existing pattern from semantic.rs)
```rust
/// Internal state that varies based on feature flags.
#[allow(dead_code)]
enum ModelState {
    /// Unloaded - no weights in memory.
    Unloaded,

    /// Loaded with candle model and tokenizer (candle feature).
    #[cfg(feature = "candle")]
    Loaded {
        // TODO: candle CodeT5+ model when feature implemented
        // model: candle_transformers::models::t5::T5EncoderModel,
        // tokenizer: tokenizers::Tokenizer,
        // device: candle::Device,
    },

    /// Stub for testing without real weights.
    #[cfg(not(feature = "candle"))]
    Stub,
}
```

### CodeModel Struct (MUST implement)
```rust
/// Code embedding model using Salesforce/codet5p-110m-embedding.
///
/// This is the code understanding model (E7) producing 256D native vectors
/// projected to 768D for FuseMoE compatibility.
///
/// # Thread Safety
/// - `AtomicBool` for `loaded` state (lock-free reads)
/// - `AtomicUsize` for memory tracking
/// - Inner model/tokenizer require explicit synchronization if mutable
///
/// # Memory Layout
/// - Total estimated: ~440MB for FP32 weights (110M params Ã— 4 bytes)
/// - With FP16 quantization: ~220MB
pub struct CodeModel {
    /// Model weights and inference engine.
    model_state: std::sync::RwLock<ModelState>,

    /// Path to model weights directory.
    model_path: PathBuf,

    /// Configuration for this model instance.
    config: SingleModelConfig,

    /// Whether model weights are loaded and ready.
    loaded: AtomicBool,

    /// Memory used by model weights (bytes).
    memory_size: AtomicUsize,
}
```

### Required Methods (MUST implement all)

#### Constructor
```rust
impl CodeModel {
    /// Create a new CodeModel instance.
    ///
    /// Model is NOT loaded after construction. Call initialization before `embed()`.
    ///
    /// # Arguments
    /// * `model_path` - Path to directory containing model weights
    /// * `config` - Device placement and quantization settings
    ///
    /// # Errors
    /// - `EmbeddingError::ConfigError` if config validation fails
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;
}
```

#### Code-Specific Methods
```rust
impl CodeModel {
    /// Check if a language is supported for specialized tokenization.
    ///
    /// Case-insensitive comparison against SUPPORTED_LANGUAGES.
    #[inline]
    pub fn is_language_supported(language: &str) -> bool {
        SUPPORTED_LANGUAGES.iter().any(|&l| l.eq_ignore_ascii_case(language))
    }

    /// Extract language hint from ModelInput if present.
    fn extract_language(input: &ModelInput) -> Option<String>;

    /// Project native 256D to 768D for FuseMoE.
    ///
    /// Uses learned projection layer (stub: deterministic linear projection).
    fn project_to_fused(&self, native: &[f32]) -> Vec<f32>;

    /// Generate stub embedding from code content hash.
    ///
    /// Deterministic: same input always produces same output.
    fn generate_stub_embedding(&self, code: &str, language: Option<&str>) -> Vec<f32>;
}
```

#### EmbeddingModel Trait Implementation (MUST match trait signature exactly)
```rust
#[async_trait]
impl EmbeddingModel for CodeModel {
    fn model_id(&self) -> ModelId {
        ModelId::Code
    }

    fn supported_input_types(&self) -> &[InputType] {
        // Code model supports both Code and Text inputs
        &[InputType::Code, InputType::Text]
    }

    fn is_initialized(&self) -> bool {
        self.loaded.load(Ordering::Acquire)
    }

    /// Generate embedding for code or text input.
    ///
    /// Returns 768D projected embedding (256D native â†’ 768D projected).
    ///
    /// # Arguments
    /// * `input` - Code or Text input
    ///
    /// # Returns
    /// * `ModelEmbedding` with 768D vector (projected_dimension)
    ///
    /// # Errors
    /// - `EmbeddingError::NotInitialized` if model not loaded
    /// - `EmbeddingError::UnsupportedModality` for Image/Audio inputs
    /// - `EmbeddingError::EmptyInput` for empty content
    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
}
```

---

## Files to Create/Modify

### CREATE: `crates/context-graph-embeddings/src/models/pretrained/code.rs`

Full implementation following the pattern from `semantic.rs` and `sparse.rs`. Include:
- All constants defined above
- `CodeModel` struct with thread-safe state
- `EmbeddingModel` trait implementation
- Stub implementation for testing without candle feature
- Comprehensive unit tests (35+ tests minimum)

### UPDATE: `crates/context-graph-embeddings/src/models/pretrained/mod.rs`

Add module and exports for CodeModel:
```rust
mod code;

pub use code::{
    CodeModel,
    CODE_LATENCY_BUDGET_MS,
    CODE_MAX_TOKENS,
    CODE_MODEL_NAME,
    CODE_NATIVE_DIMENSION,
    CODE_PROJECTED_DIMENSION,
    CODE_VOCAB_SIZE,
    SUPPORTED_LANGUAGES,
};
```

---

## Stub Implementation Pattern

For testing without real CodeT5+ weights, use deterministic hash-based stub:

```rust
/// Generate deterministic code embedding from content hash.
///
/// Uses content hash to produce reproducible embeddings.
/// Same input always produces same output (deterministic).
fn generate_stub_embedding(&self, code: &str, language: Option<&str>) -> Vec<f32> {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let mut hasher = DefaultHasher::new();
    code.hash(&mut hasher);
    if let Some(lang) = language {
        lang.to_lowercase().hash(&mut hasher);
    }
    let seed = hasher.finish();

    // Generate deterministic 256D native embedding
    let mut native = Vec::with_capacity(CODE_NATIVE_DIMENSION);
    let mut rng_state = seed;

    for _ in 0..CODE_NATIVE_DIMENSION {
        rng_state = rng_state.wrapping_mul(6364136223846793005).wrapping_add(1);
        let val = ((rng_state >> 32) as f32 / u32::MAX as f32) * 2.0 - 1.0;
        native.push(val);
    }

    // L2 normalize native
    let norm: f32 = native.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for x in &mut native {
            *x /= norm;
        }
    }

    // Project to 768D
    self.project_to_fused(&native)
}

/// Project 256D native embedding to 768D for FuseMoE.
///
/// Stub: deterministic linear projection using hash-based weights.
fn project_to_fused(&self, native: &[f32]) -> Vec<f32> {
    assert_eq!(native.len(), CODE_NATIVE_DIMENSION);

    let mut projected = vec![0.0f32; CODE_PROJECTED_DIMENSION];

    // Deterministic projection (real impl uses learned weights)
    for (i, &val) in native.iter().enumerate() {
        for j in 0..CODE_PROJECTED_DIMENSION {
            // Deterministic weight based on indices
            let weight = ((i * 31 + j * 17) % 1000) as f32 / 1000.0 - 0.5;
            projected[j] += val * weight;
        }
    }

    // L2 normalize projected
    let norm: f32 = projected.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        for x in &mut projected {
            *x /= norm;
        }
    }

    projected
}
```

---

## Test Requirements (NO MOCK DATA - 35+ minimum)

### Construction Tests
```rust
#[test]
fn test_code_model_new_valid_config() {
    let config = SingleModelConfig::default();
    let model = CodeModel::new(Path::new("models/code"), config);
    assert!(model.is_ok());
}

#[test]
fn test_code_model_new_zero_batch_size_fails() {
    let mut config = SingleModelConfig::default();
    config.max_batch_size = 0;
    let result = CodeModel::new(Path::new("models/code"), config);
    assert!(matches!(result, Err(EmbeddingError::ConfigError { .. })));
}

#[test]
fn test_code_model_not_initialized_after_new() {
    let model = create_test_code_model();
    assert!(!model.is_initialized());
}
```

### Trait Implementation Tests (CRITICAL - verify model_id.rs values)
```rust
#[test]
fn test_code_model_id() {
    let model = create_test_code_model();
    assert_eq!(model.model_id(), ModelId::Code);
}

#[test]
fn test_code_native_dimension() {
    // MUST match model_id.rs: dimension() returns 256 for Code
    assert_eq!(ModelId::Code.dimension(), 256);
}

#[test]
fn test_code_projected_dimension() {
    // MUST match model_id.rs: projected_dimension() returns 768 for Code
    let model = create_test_code_model();
    assert_eq!(model.projected_dimension(), 768);
}

#[test]
fn test_code_max_tokens() {
    let model = create_test_code_model();
    assert_eq!(model.max_tokens(), 512);
}

#[test]
fn test_code_supported_input_types() {
    let model = create_test_code_model();
    let types = model.supported_input_types();
    assert!(types.contains(&InputType::Code));
    assert!(types.contains(&InputType::Text));
    assert!(!types.contains(&InputType::Image));
    assert!(!types.contains(&InputType::Audio));
}

#[test]
fn test_code_is_pretrained() {
    let model = create_test_code_model();
    assert!(model.is_pretrained());
}

#[test]
fn test_code_model_repo() {
    // MUST match model_id.rs
    assert_eq!(ModelId::Code.model_repo(), Some("Salesforce/codet5p-110m-embedding"));
}
```

### Language Support Tests
```rust
#[test]
fn test_language_supported_python() {
    assert!(CodeModel::is_language_supported("python"));
}

#[test]
fn test_language_supported_rust() {
    assert!(CodeModel::is_language_supported("rust"));
}

#[test]
fn test_language_supported_javascript() {
    assert!(CodeModel::is_language_supported("javascript"));
}

#[test]
fn test_language_unsupported_brainfuck() {
    assert!(!CodeModel::is_language_supported("brainfuck"));
}

#[test]
fn test_language_case_insensitive() {
    assert!(CodeModel::is_language_supported("Python"));
    assert!(CodeModel::is_language_supported("RUST"));
    assert!(CodeModel::is_language_supported("JavaScript"));
}
```

### Embedding Tests (CRITICAL - verify 768D projected output)
```rust
#[tokio::test]
async fn test_embed_code_returns_projected_dimension() {
    let model = create_loaded_code_model().await;
    let input = ModelInput::code("fn main() { println!(\"hello\"); }", Some("rust")).unwrap();
    let result = model.embed(&input).await.unwrap();
    // MUST return projected dimension (768), not native (256)
    assert_eq!(result.vector.len(), 768, "Must return projected 768D, not native 256D");
}

#[tokio::test]
async fn test_embed_text_works() {
    let model = create_loaded_code_model().await;
    let input = ModelInput::text("def foo(): pass").unwrap();
    let result = model.embed(&input).await.unwrap();
    assert_eq!(result.vector.len(), 768);
    assert_eq!(result.model_id, ModelId::Code);
}

#[tokio::test]
async fn test_embed_deterministic() {
    let model = create_loaded_code_model().await;
    let input = ModelInput::code("fn test() {}", Some("rust")).unwrap();
    let result1 = model.embed(&input).await.unwrap();
    let result2 = model.embed(&input).await.unwrap();
    assert_eq!(result1.vector, result2.vector, "Same input must produce same output");
}

#[tokio::test]
async fn test_embed_different_inputs_different_embeddings() {
    let model = create_loaded_code_model().await;
    let input1 = ModelInput::code("fn foo() {}", Some("rust")).unwrap();
    let input2 = ModelInput::code("fn bar() {}", Some("rust")).unwrap();
    let result1 = model.embed(&input1).await.unwrap();
    let result2 = model.embed(&input2).await.unwrap();
    assert_ne!(result1.vector, result2.vector);
}

#[tokio::test]
async fn test_embed_language_affects_embedding() {
    let model = create_loaded_code_model().await;
    let code = "def foo(): pass";
    let input_py = ModelInput::code(code, Some("python")).unwrap();
    let input_no_lang = ModelInput::code(code, None).unwrap();
    let result_py = model.embed(&input_py).await.unwrap();
    let result_no = model.embed(&input_no_lang).await.unwrap();
    // Different language hints should produce different embeddings
    assert_ne!(result_py.vector, result_no.vector);
}

#[tokio::test]
async fn test_embed_without_initialization_fails() {
    let model = create_test_code_model();
    assert!(!model.is_initialized());
    let input = ModelInput::text("test").unwrap();
    let result = model.embed(&input).await;
    assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
}

#[tokio::test]
async fn test_embed_unsupported_modality_fails() {
    let model = create_loaded_code_model().await;
    // If Image input is available
    if let Ok(input) = ModelInput::image(vec![0u8; 100], "image/png") {
        let result = model.embed(&input).await;
        assert!(matches!(result, Err(EmbeddingError::UnsupportedModality { .. })));
    }
}

#[tokio::test]
async fn test_embed_normalized_output() {
    let model = create_loaded_code_model().await;
    let input = ModelInput::code("print('hello')", Some("python")).unwrap();
    let result = model.embed(&input).await.unwrap();

    // Check L2 norm is approximately 1.0
    let norm: f32 = result.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
    assert!((norm - 1.0).abs() < 0.01, "L2 norm should be ~1.0, got {}", norm);
}
```

### Projection Tests
```rust
#[test]
fn test_projection_correct_dimensions() {
    let model = create_test_code_model();
    let native = vec![0.1f32; CODE_NATIVE_DIMENSION];
    let projected = model.project_to_fused(&native);
    assert_eq!(projected.len(), CODE_PROJECTED_DIMENSION);
}

#[test]
fn test_projection_deterministic() {
    let model = create_test_code_model();
    let native = vec![0.1f32; CODE_NATIVE_DIMENSION];
    let projected1 = model.project_to_fused(&native);
    let projected2 = model.project_to_fused(&native);
    assert_eq!(projected1, projected2);
}
```

---

## Edge Case Tests (MANDATORY - with State Verification)

### Edge Case 1: Empty Code Input
```rust
#[tokio::test]
async fn test_edge_case_empty_code() {
    println!("=== EDGE CASE: Empty Code Input ===");
    let model = create_loaded_code_model().await;

    // BEFORE STATE
    println!("BEFORE: Creating empty code input");
    let input_result = ModelInput::code("", Some("python"));
    println!("BEFORE: input_result = {:?}", input_result);

    match input_result {
        Ok(input) => {
            // EXECUTE
            let result = model.embed(&input).await;

            // AFTER STATE
            println!("AFTER: result = {:?}", result);

            match result {
                Ok(embedding) => {
                    println!("AFTER: Got embedding with {} dimensions", embedding.vector.len());
                    assert_eq!(embedding.vector.len(), 768);
                }
                Err(e) => {
                    println!("AFTER: Got expected error: {:?}", e);
                    assert!(matches!(e, EmbeddingError::EmptyInput { .. } | EmbeddingError::InvalidInput { .. }));
                }
            }
        }
        Err(e) => {
            println!("AFTER: ModelInput creation failed (expected): {:?}", e);
        }
    }
    println!("=== EDGE CASE COMPLETE ===\n");
}
```

### Edge Case 2: Maximum Token Length
```rust
#[tokio::test]
async fn test_edge_case_max_tokens() {
    println!("=== EDGE CASE: Maximum Token Length ===");
    let model = create_loaded_code_model().await;

    // BEFORE STATE - Create code that exceeds 512 tokens
    let long_code = "let x = 1;\n".repeat(1000);
    println!("BEFORE: Created code with {} chars", long_code.len());
    let input = ModelInput::code(&long_code, Some("javascript")).unwrap();

    // EXECUTE
    let result = model.embed(&input).await;

    // AFTER STATE
    println!("AFTER: result success = {}", result.is_ok());

    match result {
        Ok(embedding) => {
            println!("AFTER: Embedding produced (truncation occurred)");
            assert_eq!(embedding.vector.len(), 768);
        }
        Err(e) => {
            println!("AFTER: Error on overlong input: {:?}", e);
            // Either truncation or error is acceptable - document which
        }
    }
    println!("=== EDGE CASE COMPLETE ===\n");
}
```

### Edge Case 3: Unicode and Special Characters in Code
```rust
#[tokio::test]
async fn test_edge_case_unicode_code() {
    println!("=== EDGE CASE: Unicode in Code ===");
    let model = create_loaded_code_model().await;

    // BEFORE STATE - Code with unicode identifiers and strings
    let unicode_code = r#"
        def å‡½æ•°å():
            # Comment with emojis ðŸš€
            return "ä½ å¥½ä¸–ç•Œ \u{1F600}"
    "#;
    println!("BEFORE: unicode_code = {}", unicode_code);
    let input = ModelInput::code(unicode_code, Some("python")).unwrap();

    // EXECUTE
    let result = model.embed(&input).await;

    // AFTER STATE
    println!("AFTER: result = {:?}", result.is_ok());

    let embedding = result.expect("Unicode code should produce embedding");
    println!("AFTER: embedding.vector.len() = {}", embedding.vector.len());
    assert_eq!(embedding.vector.len(), 768);

    // Verify not all zeros
    let sum: f32 = embedding.vector.iter().map(|x| x.abs()).sum();
    println!("AFTER: embedding abs sum = {}", sum);
    assert!(sum > 0.0, "Embedding should not be all zeros");
    println!("=== EDGE CASE COMPLETE ===\n");
}
```

---

## Full State Verification (MANDATORY)

### 1. Define Source of Truth
The final result is stored in:
- **File System**: `crates/context-graph-embeddings/src/models/pretrained/code.rs`
- **Module Exports**: `crates/context-graph-embeddings/src/models/pretrained/mod.rs`
- **Test Results**: `cargo test` output
- **Dimension Verification**: embedding.vector.len() == 768

### 2. Execute & Inspect Commands
```bash
# Verify file exists and has content
ls -la crates/context-graph-embeddings/src/models/pretrained/code.rs
wc -l crates/context-graph-embeddings/src/models/pretrained/code.rs

# Verify exports are correct
grep -n "mod code" crates/context-graph-embeddings/src/models/pretrained/mod.rs
grep -n "pub use code" crates/context-graph-embeddings/src/models/pretrained/mod.rs

# Verify struct exists
grep -n "pub struct CodeModel" crates/context-graph-embeddings/src/models/pretrained/code.rs

# Verify trait implementation
grep -n "impl EmbeddingModel for CodeModel" crates/context-graph-embeddings/src/models/pretrained/code.rs

# Verify constants match model_id.rs
grep -n "CODE_NATIVE_DIMENSION.*256" crates/context-graph-embeddings/src/models/pretrained/code.rs
grep -n "CODE_PROJECTED_DIMENSION.*768" crates/context-graph-embeddings/src/models/pretrained/code.rs

# Run tests and capture output
cargo test -p context-graph-embeddings code -- --nocapture 2>&1 | tee /tmp/code_test_output.log

# Count tests
echo "Unit tests: $(grep -c '#\[test\]' crates/context-graph-embeddings/src/models/pretrained/code.rs)"
echo "Async tests: $(grep -c '#\[tokio::test\]' crates/context-graph-embeddings/src/models/pretrained/code.rs)"
```

### 3. Boundary & Edge Case Audit
Run the 3 edge cases and verify:
1. **Empty Input**: Verify error or valid embedding
2. **Maximum Tokens**: Verify truncation behavior
3. **Unicode Input**: Verify non-zero embedding

### 4. Evidence of Success
Provide log showing:
```
[INFO] CodeModel initialized: model_path=/path/to/model
[INFO] embed() returning 768D projected embedding (256D native â†’ 768D)
[INFO] test code_model_id ... ok
[INFO] test code_native_dimension ... ok
[INFO] test code_projected_dimension ... ok
[INFO] test embed_code_returns_projected_dimension ... ok
... (35+ tests passing)
```

---

## Definition of Done

### Constraints Checklist
- [ ] Native dimension is 256 (matches model_id.rs)
- [ ] Projected dimension is 768 (matches model_id.rs)
- [ ] `embed()` returns 768D projected vector
- [ ] `dimension()` returns 256 (via ModelId::Code.dimension())
- [ ] `projected_dimension()` returns 768 (via ModelId::Code.projected_dimension())
- [ ] `max_tokens()` returns 512
- [ ] `model_id()` returns `ModelId::Code`
- [ ] `supported_input_types()` includes Code and Text
- [ ] `model_repo()` returns "Salesforce/codet5p-110m-embedding"
- [ ] Thread-safe with Send + Sync bounds
- [ ] Latency target: less than 10ms P95

### Validation Criteria
- [ ] `cargo check -p context-graph-embeddings` passes
- [ ] `cargo test -p context-graph-embeddings` passes (35+ tests)
- [ ] `cargo clippy -p context-graph-embeddings -- -D warnings` passes
- [ ] All edge case tests include state printing
- [ ] No mock data - only deterministic stubs
- [ ] Integration with pretrained/mod.rs complete

---

## Sherlock-Holmes Verification Checklist

After implementation, sherlock-holmes agent MUST verify:

### File Existence & Structure
- [ ] `crates/context-graph-embeddings/src/models/pretrained/code.rs` exists
- [ ] File has 500+ lines of code (not a stub file)
- [ ] `mod code;` line present in mod.rs
- [ ] Exports present in mod.rs

### Struct Verification
- [ ] `CodeModel` struct exists with required fields
- [ ] `model_state`, `model_path`, `config`, `loaded`, `memory_size` fields present

### Constants Verification (MUST match model_id.rs)
- [ ] `CODE_NATIVE_DIMENSION` = 256
- [ ] `CODE_PROJECTED_DIMENSION` = 768
- [ ] `CODE_MAX_TOKENS` = 512
- [ ] `CODE_LATENCY_BUDGET_MS` = 10
- [ ] `CODE_MODEL_NAME` = "Salesforce/codet5p-110m-embedding"
- [ ] `SUPPORTED_LANGUAGES` array exists

### Trait Implementation
- [ ] `#[async_trait]` attribute on impl block
- [ ] `impl EmbeddingModel for CodeModel` present
- [ ] `model_id()` returns `ModelId::Code`
- [ ] `supported_input_types()` returns slice containing Code and Text
- [ ] `is_initialized()` checks atomic loaded state
- [ ] `embed()` returns 768D projected vector

### Dimension Verification (CRITICAL)
- [ ] `ModelId::Code.dimension()` returns 256
- [ ] `ModelId::Code.projected_dimension()` returns 768
- [ ] `embed()` output has exactly 768 elements
- [ ] Projection from 256D â†’ 768D is implemented

### Test Verification
- [ ] At least 35 tests exist
- [ ] All tests pass
- [ ] Edge case tests include println! state verification
- [ ] Dimension tests verify model_id.rs values

### Build Verification
- [ ] `cargo check -p context-graph-embeddings` passes
- [ ] `cargo test -p context-graph-embeddings code` passes
- [ ] `cargo clippy -p context-graph-embeddings -- -D warnings` passes

---

## Manual Verification Commands

```bash
# 1. Verify file exists
ls -la crates/context-graph-embeddings/src/models/pretrained/code.rs

# 2. Verify exports in mod.rs
grep "code" crates/context-graph-embeddings/src/models/pretrained/mod.rs

# 3. Verify constants match model_id.rs
grep "256" crates/context-graph-embeddings/src/models/pretrained/code.rs | head -5
grep "768" crates/context-graph-embeddings/src/models/pretrained/code.rs | head -5

# 4. Verify model_id.rs dimensions (source of truth)
grep -A 2 "Code =>" crates/context-graph-embeddings/src/types/model_id.rs

# 5. Run tests
cargo test -p context-graph-embeddings code --no-fail-fast -- --nocapture

# 6. Check build
cargo check -p context-graph-embeddings

# 7. Run clippy
cargo clippy -p context-graph-embeddings -- -D warnings

# 8. Count tests
echo "Total tests: $(($(grep -c '#\[test\]' crates/context-graph-embeddings/src/models/pretrained/code.rs) + $(grep -c '#\[tokio::test\]' crates/context-graph-embeddings/src/models/pretrained/code.rs)))"
```

---

## Reference Pattern

Follow the exact patterns from these files:
1. `semantic.rs` - Basic pretrained model structure
2. `sparse.rs` - Model with projection layer (30522 â†’ 1536)
3. `causal.rs` - Long-context handling

Key patterns:
- `RwLock<ModelState>` for thread-safe state
- `AtomicBool` for loaded state
- `AtomicUsize` for memory tracking
- `#[cfg(feature = "candle")]` for feature-gating
- Stub implementation for testing

---

## CRITICAL: Final Verification Step

After implementation is complete, you MUST use sherlock-holmes subagent:

```
Task("Verify CodeModel implementation complete", "sherlock-holmes", {
    task_file: "specs/tasks/module03/M03-L09.md",
    files: [
        "crates/context-graph-embeddings/src/models/pretrained/code.rs",
        "crates/context-graph-embeddings/src/models/pretrained/mod.rs",
        "crates/context-graph-embeddings/src/types/model_id.rs"
    ],
    critical_checks: [
        "embed() returns 768D vector",
        "CODE_NATIVE_DIMENSION = 256",
        "CODE_PROJECTED_DIMENSION = 768",
        "projection from 256D to 768D implemented"
    ],
    build_commands: [
        "cargo check -p context-graph-embeddings",
        "cargo test -p context-graph-embeddings code",
        "cargo clippy -p context-graph-embeddings -- -D warnings"
    ],
    test_count_minimum: 35,
    fail_on_any_issue: true
})
```

If sherlock-holmes identifies ANY issues, fix them before marking complete.

---

---

## IMPLEMENTATION COMPLETE

### Sherlock-Holmes Verification (2026-01-01)

**VERDICT: IMPLEMENTATION VERIFIED**

| Check | Status |
|-------|--------|
| CODE_NATIVE_DIMENSION = 256 | âœ… PASS |
| CODE_PROJECTED_DIMENSION = 768 | âœ… PASS |
| CODE_MAX_TOKENS = 512 | âœ… PASS |
| CODE_LATENCY_BUDGET_MS = 10 | âœ… PASS |
| CodeModel struct with RwLock<ModelState> | âœ… PASS |
| AtomicBool loaded state | âœ… PASS |
| model_id() returns ModelId::Code | âœ… PASS |
| supported_input_types() = [Code, Text] | âœ… PASS |
| embed() returns 768D output | âœ… PASS |
| L2 normalization applied | âœ… PASS |
| Deterministic (xxhash64) | âœ… PASS |
| load/unload transitions | âœ… PASS |
| mod.rs exports | âœ… PASS |
| Tests (40) >= 35 minimum | âœ… PASS |
| Edge cases tested | âœ… PASS |
| cargo check | âœ… PASS |
| cargo test (826 tests passed) | âœ… PASS |
| cargo clippy -D warnings | âœ… PASS |

### Files Modified
- **CREATED**: `crates/context-graph-embeddings/src/models/pretrained/code.rs` (959 lines)
- **UPDATED**: `crates/context-graph-embeddings/src/models/pretrained/mod.rs` (exports added)

### Test Results
- 40 CodeModel-specific tests
- 826 total tests in context-graph-embeddings crate
- 100% pass rate

---

*Task Document Updated: 2026-01-01*
*Module: 03 - 12-Model Embedding Pipeline*
*Task: M03-L09 Code Model (E7 - CodeT5+)*
*Version: 3.0.0*
*Aligned with model_id.rs: Code = 6, dimension=256, projected=768*
*IMPLEMENTATION COMPLETE - Sherlock Verified*
