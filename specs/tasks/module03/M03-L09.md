# M03-L09: Code Model (E7 - CodeBERT)

```xml
<task_spec id="M03-L09" version="1.0">
<metadata>
  <title>Implement Code Model using CodeBERT</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>9</sequence>
  <implements>PRD E7 - Code embedding with AST-aware encoding</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement code embedding using microsoft/codebert-base via candle-transformers.
CodeBERT understands programming language syntax and semantics through
bimodal pre-training on natural language and code.

Output: 768D dense vector.
Latency target: less than 10ms.

The model supports Code input type with language-aware tokenization.
Model weights are loaded from models/code/ directory matching models_config.toml.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct CodeModel {
    model: CodeBertModel,
    tokenizer: Tokenizer,
    device: Device,
    loaded: AtomicBool,
    config: SingleModelConfig,
}

impl CodeModel {
    pub fn new(models_dir: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;
    pub fn tokenize(&self, code: &str, language: &str) -> EmbeddingResult<TokenizedInput>;
}

#[async_trait]
impl EmbeddingModel for CodeModel {
    fn model_id(&self) -> ModelId { ModelId::Code }
    fn dimension(&self) -> usize { 768 }
    fn max_tokens(&self) -> usize { 512 }
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Code, InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for inference</constraint>
    <constraint>Model loaded from models/code/ directory</constraint>
    <constraint>Supports Code input type with language field</constraint>
    <constraint>Language-aware tokenization for syntax highlighting</constraint>
    <constraint>dimension() returns 768</constraint>
    <constraint>max_tokens() returns 512</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
    <constraint>Latency target: less than 10ms per embedding</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/code.rs</file>
  <file>crates/context-graph-embeddings/src/models/pretrained/mod.rs (update exports)</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>CodeModel::new() creates valid instance</criterion>
  <criterion>Model loads weights from correct path</criterion>
  <criterion>embed() produces 768D vector</criterion>
  <criterion>embed() handles both Code and Text input types</criterion>
  <criterion>Embedding values are valid (no NaN/Inf)</criterion>
  <criterion>Memory usage tracked correctly</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `microsoft/codebert-base`
- **Architecture**: RoBERTa-based transformer
- **Hidden Size**: 768
- **Max Position**: 512 tokens
- **Vocab Size**: 50265

### Language Support
CodeBERT supports multiple programming languages:
- Python, Java, JavaScript, PHP, Ruby, Go
- Language hint improves tokenization quality

### Tokenization Strategy
```rust
// For Code input type
fn tokenize_code(&self, code: &str, language: &str) -> TokenizedInput {
    // Prepend language token if available
    let prefixed = format!("[{}] {}", language.to_uppercase(), code);
    self.tokenizer.encode(&prefixed, true)
}
```

### Mean Pooling
Use mean pooling over token embeddings (excluding special tokens) for final representation:
```rust
fn mean_pool(hidden_states: &Tensor, attention_mask: &Tensor) -> Tensor {
    let mask_expanded = attention_mask.unsqueeze(-1).expand_as(&hidden_states);
    let sum = (hidden_states * mask_expanded).sum(1);
    let count = mask_expanded.sum(1).clamp_min(1e-9);
    sum / count
}
```
