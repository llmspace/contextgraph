# M03-F14: FusionConfig Struct

```xml
<task_spec id="M03-F14" version="1.0">
<metadata>
  <title>Define FusionConfig for FuseMoE</title>
  <status>ready</status>
  <layer>foundation</layer>
  <sequence>14</sequence>
  <implements>constitution.yaml:embeddings.fusion, PRD Section 4.3</implements>
  <depends_on>none</depends_on>
  <estimated_hours>1</estimated_hours>
</metadata>

<context>
Configuration for the FuseMoE (Mixture of Experts) fusion layer that combines
outputs from all 12 embedding models into the final 1536D unified embedding.

Key parameters:
- 8 experts (specialized fusion networks)
- Top-2 expert selection per input
- Load balancing to prevent expert collapse
- Laplace smoothing for stable routing
</context>

<definition_of_done>
  <signatures>
```rust
use serde::{Deserialize, Serialize};

/// Configuration for FuseMoE fusion layer.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FusionConfig {
    /// Number of expert networks.
    /// Default: 8
    pub num_experts: usize,

    /// Number of experts to route each input to (Top-K).
    /// Default: 2
    pub top_k: usize,

    /// Output embedding dimension.
    /// Default: 1536
    pub output_dim: usize,

    /// Hidden dimension in expert networks.
    /// Default: 4096
    pub expert_hidden_dim: usize,

    /// Load balance loss coefficient.
    /// Penalizes uneven expert utilization.
    /// Default: 0.01
    pub load_balance_coef: f32,

    /// Capacity factor for expert buffers.
    /// 1.25 = 25% overhead above average load.
    /// Default: 1.25
    pub capacity_factor: f32,

    /// Temperature for softmax gating.
    /// Lower = sharper expert selection.
    /// Default: 1.0
    pub temperature: f32,

    /// Noise standard deviation for exploration (training only).
    /// Set to 0.0 for inference.
    /// Default: 0.0
    pub noise_std: f32,

    /// Laplace smoothing alpha for stable routing.
    /// Formula: (p + alpha) / (1 + alpha * K)
    /// Default: 0.01
    pub laplace_alpha: f32,
}

impl Default for FusionConfig {
    fn default() -> Self {
        Self {
            num_experts: 8,
            top_k: 2,
            output_dim: 1536,
            expert_hidden_dim: 4096,
            load_balance_coef: 0.01,
            capacity_factor: 1.25,
            temperature: 1.0,
            noise_std: 0.0,
            laplace_alpha: 0.01,
        }
    }
}

impl FusionConfig {
    /// Validate fusion configuration values.
    pub fn validate(&self) -> Result<(), String> {
        if self.num_experts == 0 {
            return Err("num_experts must be > 0".to_string());
        }
        if self.top_k == 0 || self.top_k > self.num_experts {
            return Err(format!(
                "top_k must be in [1, {}]",
                self.num_experts
            ));
        }
        if self.output_dim == 0 {
            return Err("output_dim must be > 0".to_string());
        }
        if self.temperature <= 0.0 {
            return Err("temperature must be > 0".to_string());
        }
        if self.capacity_factor < 1.0 {
            return Err("capacity_factor must be >= 1.0".to_string());
        }
        if self.laplace_alpha < 0.0 {
            return Err("laplace_alpha must be >= 0".to_string());
        }
        Ok(())
    }

    /// Create inference configuration (noise_std = 0).
    pub fn for_inference() -> Self {
        Self {
            noise_std: 0.0,
            ..Default::default()
        }
    }

    /// Create training configuration (noise_std > 0).
    pub fn for_training() -> Self {
        Self {
            noise_std: 0.1,
            ..Default::default()
        }
    }
}
```
  </signatures>

  <constraints>
    - num_experts typically 4, 8, or 16
    - top_k must be <= num_experts
    - output_dim = 1536 per PRD specification
    - capacity_factor >= 1.0 (no underprovisioning)
    - temperature > 0 (avoid division by zero)
    - noise_std = 0.0 for inference (deterministic)
    - laplace_alpha >= 0 (smoothing cannot be negative)
  </constraints>

  <verification>
    - Default::default() passes validate()
    - validate() rejects num_experts = 0
    - validate() rejects top_k > num_experts
    - validate() rejects temperature <= 0
    - for_inference() has noise_std = 0.0
    - for_training() has noise_std > 0
    - output_dim defaults to 1536
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/config.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check --package context-graph-embeddings passes</criterion>
  <criterion>cargo test --package context-graph-embeddings passes</criterion>
  <criterion>cargo clippy --package context-graph-embeddings -- -D warnings shows 0 warnings</criterion>
  <criterion>Default config has output_dim = 1536</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### FuseMoE Architecture Reference
```
Input: Concatenated embeddings (8320D)
        |
        v
  [Gating Network] --> Expert weights (8 values)
        |
        v
  [Top-2 Selection] --> Select 2 experts
        |
        v
  [Expert Networks] --> Each: 8320 -> 4096 -> 4096 -> 1536
        |
        v
  [Weighted Sum] --> Final 1536D embedding
```

### Example TOML
```toml
[fusion]
num_experts = 8
top_k = 2
output_dim = 1536
expert_hidden_dim = 4096
load_balance_coef = 0.01
capacity_factor = 1.25
temperature = 1.0
noise_std = 0.0
laplace_alpha = 0.01
```

### Test Cases
1. `test_default_num_experts` - Equals 8
2. `test_default_top_k` - Equals 2
3. `test_default_output_dim` - Equals 1536
4. `test_validate_valid` - Default passes
5. `test_validate_top_k_exceeds_experts` - Returns error
6. `test_for_inference_no_noise` - noise_std = 0.0
7. `test_for_training_has_noise` - noise_std > 0

---

*Task ID: M03-F14*
*Module: 03 - 12-Model Embedding Pipeline*
*Layer: Foundation*
*Created: 2026-01-01*
