# Task Specification: M03-S19

```xml
<task_spec id="M03-S19" version="1.0">
<metadata>
  <title>Semantic Alignment Verification</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>66</sequence>
  <implements>
    - PRD Section 7.4: FuseMoE unified latent space
    - Constitution: quality.semantic_coherence
    - Technical Engine: Cross-modal embedding alignment
  </implements>
  <depends_on>
    - M03-S01 (EmbeddingPipeline)
    - M03-L23 (FuseMoE)
    - M03-S10 (Integration Tests)
  </depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
The 12-model pipeline fuses embeddings from different latent spaces:
- E1 (E5): Semantic text latent space
- E7 (CodeBERT): Code latent space
- E10 (CLIP): Vision-language latent space
- E9 (HDC): Hyperdimensional symbolic space

The FuseMoE layer creates a "Shared Latent Space" (1536D), but there is no guarantee
that semantically equivalent content from different modalities aligns properly.

WITHOUT alignment verification:
- "Python code for a loop" (text) → Vector A
- `for i in range(10):` (code) → Vector B
- A and B might have low similarity despite semantic equivalence

This task implements Semantic Alignment Verification tests that ensure:
1. Cross-modal semantic pairs have high similarity (>0.7)
2. Unrelated content has low similarity (<0.3)
3. The unified space preserves relative distances

These are "sense check" integration tests that validate the FuseMoE is producing
a coherent unified space, not a fragmented one.
</context>

<definition_of_done>
  <signatures>
```rust
use std::collections::HashMap;

/// Semantic alignment test configuration
#[derive(Debug, Clone)]
pub struct AlignmentTestConfig {
    pub min_positive_similarity: f32,   // e.g., 0.7
    pub max_negative_similarity: f32,   // e.g., 0.3
    pub margin: f32,                    // positive - negative > margin
    pub test_pairs: Vec<AlignmentPair>,
}

/// A pair of inputs that should align (or not)
#[derive(Debug, Clone)]
pub struct AlignmentPair {
    pub id: String,
    pub input_a: AlignmentInput,
    pub input_b: AlignmentInput,
    pub expected_relationship: Relationship,
}

#[derive(Debug, Clone)]
pub enum AlignmentInput {
    Text(String),
    Code { language: String, content: String },
    // Future: Image, etc.
}

#[derive(Debug, Clone, Copy)]
pub enum Relationship {
    /// Should have high similarity (>min_positive_similarity)
    Equivalent,
    /// Should have medium similarity (between thresholds)
    Related,
    /// Should have low similarity (<max_negative_similarity)
    Unrelated,
}

/// Alignment test results
#[derive(Debug, Clone)]
pub struct AlignmentTestResult {
    pub pair_id: String,
    pub similarity: f32,
    pub expected_relationship: Relationship,
    pub passed: bool,
    pub embedding_a_norm: f32,
    pub embedding_b_norm: f32,
}

#[derive(Debug, Clone)]
pub struct AlignmentTestSummary {
    pub total_pairs: usize,
    pub passed: usize,
    pub failed: usize,
    pub pass_rate: f32,
    pub avg_positive_similarity: f32,
    pub avg_negative_similarity: f32,
    pub margin_achieved: f32,
    pub results: Vec<AlignmentTestResult>,
}

/// Semantic alignment verifier
pub struct AlignmentVerifier {
    pipeline: Arc<EmbeddingPipeline>,
    config: AlignmentTestConfig,
}

impl AlignmentVerifier {
    /// Create verifier with pipeline and config
    pub fn new(pipeline: Arc<EmbeddingPipeline>, config: AlignmentTestConfig) -> Self;

    /// Load standard test corpus
    pub fn with_standard_corpus(pipeline: Arc<EmbeddingPipeline>) -> Self;

    /// Run all alignment tests
    pub async fn verify_all(&self) -> EmbeddingResult<AlignmentTestSummary>;

    /// Run specific test pair
    pub async fn verify_pair(&self, pair: &AlignmentPair) -> EmbeddingResult<AlignmentTestResult>;

    /// Compute cosine similarity between embeddings
    pub fn cosine_similarity(a: &FusedEmbedding, b: &FusedEmbedding) -> f32;

    /// Generate alignment report
    pub fn generate_report(&self, summary: &AlignmentTestSummary) -> String;
}

/// Standard test corpus categories
pub mod test_corpus {
    /// Text-to-code equivalence pairs
    pub fn text_code_pairs() -> Vec<AlignmentPair>;

    /// Paraphrase pairs (same meaning, different words)
    pub fn paraphrase_pairs() -> Vec<AlignmentPair>;

    /// Negative pairs (unrelated content)
    pub fn negative_pairs() -> Vec<AlignmentPair>;

    /// Cross-domain pairs (e.g., description to implementation)
    pub fn cross_domain_pairs() -> Vec<AlignmentPair>;
}
```
  </signatures>

  <constraints>
    - Test corpus covers text↔code, text↔text, code↔code modalities
    - Minimum 20 positive pairs, 20 negative pairs
    - Tests run in <10 seconds (parallelized embedding)
    - Failure threshold: <90% pass rate = test failure
    - Report includes per-pair breakdown for debugging
    - Standard corpus embedded in library (no external files)
    - Cosine similarity computed on normalized 1536D vectors
    - Tests deterministic (no random sampling)
  </constraints>

  <verification>
    <step>text_code_pairs() returns ≥10 equivalent pairs</step>
    <step>Text "Python code for a loop" and `for i in range(10):` have sim >0.7</step>
    <step>Unrelated text and code have similarity <0.3</step>
    <step>Paraphrase pairs have similarity >0.8</step>
    <step>verify_all() runs in <10 seconds</step>
    <step>90%+ pass rate on standard corpus</step>
    <step>generate_report() produces readable output</step>
    <step>Integration test fails if FuseMoE weights are randomized</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/testing/alignment.rs</file>
  <file>crates/context-graph-embeddings/src/testing/corpus.rs</file>
  <file>crates/context-graph-embeddings/src/testing/mod.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test testing::alignment passes</criterion>
  <criterion>Standard corpus achieves 90%+ pass rate</criterion>
  <criterion>Text-code equivalence pairs achieve >0.7 similarity</criterion>
  <criterion>Negative pairs achieve <0.3 similarity</criterion>
  <criterion>Integration test runs in <10 seconds</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Standard Test Corpus

```rust
pub mod test_corpus {
    use super::*;

    pub fn text_code_pairs() -> Vec<AlignmentPair> {
        vec![
            AlignmentPair {
                id: "loop_basic".into(),
                input_a: AlignmentInput::Text("Python code for iterating over a range".into()),
                input_b: AlignmentInput::Code {
                    language: "python".into(),
                    content: "for i in range(10):\n    print(i)".into(),
                },
                expected_relationship: Relationship::Equivalent,
            },
            AlignmentPair {
                id: "function_def".into(),
                input_a: AlignmentInput::Text("A function that calculates factorial".into()),
                input_b: AlignmentInput::Code {
                    language: "python".into(),
                    content: "def factorial(n):\n    return 1 if n <= 1 else n * factorial(n-1)".into(),
                },
                expected_relationship: Relationship::Equivalent,
            },
            AlignmentPair {
                id: "http_request".into(),
                input_a: AlignmentInput::Text("Make an HTTP GET request in JavaScript".into()),
                input_b: AlignmentInput::Code {
                    language: "javascript".into(),
                    content: "fetch('https://api.example.com').then(r => r.json())".into(),
                },
                expected_relationship: Relationship::Equivalent,
            },
            AlignmentPair {
                id: "list_comprehension".into(),
                input_a: AlignmentInput::Text("Create a list of squares of numbers 1 to 10".into()),
                input_b: AlignmentInput::Code {
                    language: "python".into(),
                    content: "squares = [x**2 for x in range(1, 11)]".into(),
                },
                expected_relationship: Relationship::Equivalent,
            },
            // ... more pairs
        ]
    }

    pub fn negative_pairs() -> Vec<AlignmentPair> {
        vec![
            AlignmentPair {
                id: "unrelated_1".into(),
                input_a: AlignmentInput::Text("The weather is sunny today".into()),
                input_b: AlignmentInput::Code {
                    language: "rust".into(),
                    content: "fn quicksort<T: Ord>(arr: &mut [T]) { ... }".into(),
                },
                expected_relationship: Relationship::Unrelated,
            },
            AlignmentPair {
                id: "unrelated_2".into(),
                input_a: AlignmentInput::Text("Recipe for chocolate cake".into()),
                input_b: AlignmentInput::Code {
                    language: "python".into(),
                    content: "import socket\ns = socket.socket()".into(),
                },
                expected_relationship: Relationship::Unrelated,
            },
            // ... more pairs
        ]
    }

    pub fn paraphrase_pairs() -> Vec<AlignmentPair> {
        vec![
            AlignmentPair {
                id: "paraphrase_1".into(),
                input_a: AlignmentInput::Text("How to sort a list in Python".into()),
                input_b: AlignmentInput::Text("Python list sorting methods".into()),
                expected_relationship: Relationship::Equivalent,
            },
            AlignmentPair {
                id: "paraphrase_2".into(),
                input_a: AlignmentInput::Text("Remove duplicates from array".into()),
                input_b: AlignmentInput::Text("Deduplicate array elements".into()),
                expected_relationship: Relationship::Equivalent,
            },
            // ... more pairs
        ]
    }
}
```

### Verification Logic

```rust
impl AlignmentVerifier {
    pub async fn verify_pair(&self, pair: &AlignmentPair) -> EmbeddingResult<AlignmentTestResult> {
        // Embed both inputs
        let emb_a = self.embed_input(&pair.input_a).await?;
        let emb_b = self.embed_input(&pair.input_b).await?;

        let similarity = Self::cosine_similarity(&emb_a, &emb_b);

        let passed = match pair.expected_relationship {
            Relationship::Equivalent => similarity >= self.config.min_positive_similarity,
            Relationship::Related => {
                similarity >= self.config.max_negative_similarity
                    && similarity <= self.config.min_positive_similarity
            }
            Relationship::Unrelated => similarity <= self.config.max_negative_similarity,
        };

        Ok(AlignmentTestResult {
            pair_id: pair.id.clone(),
            similarity,
            expected_relationship: pair.expected_relationship,
            passed,
            embedding_a_norm: emb_a.magnitude(),
            embedding_b_norm: emb_b.magnitude(),
        })
    }

    pub fn cosine_similarity(a: &FusedEmbedding, b: &FusedEmbedding) -> f32 {
        let dot: f32 = a.vector.iter().zip(&b.vector).map(|(x, y)| x * y).sum();
        let norm_a: f32 = a.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = b.vector.iter().map(|x| x * x).sum::<f32>().sqrt();

        if norm_a == 0.0 || norm_b == 0.0 {
            0.0
        } else {
            dot / (norm_a * norm_b)
        }
    }
}
```

### Integration Test

```rust
#[tokio::test]
async fn test_semantic_alignment() {
    let pipeline = EmbeddingPipeline::new(test_config()).await.unwrap();
    let verifier = AlignmentVerifier::with_standard_corpus(Arc::new(pipeline));

    let summary = verifier.verify_all().await.unwrap();

    println!("{}", verifier.generate_report(&summary));

    assert!(
        summary.pass_rate >= 0.90,
        "Alignment pass rate {:.1}% below 90% threshold. Failed pairs: {:?}",
        summary.pass_rate * 100.0,
        summary.results.iter().filter(|r| !r.passed).collect::<Vec<_>>()
    );

    assert!(
        summary.avg_positive_similarity >= 0.7,
        "Average positive similarity {:.3} below 0.7",
        summary.avg_positive_similarity
    );

    assert!(
        summary.avg_negative_similarity <= 0.3,
        "Average negative similarity {:.3} above 0.3",
        summary.avg_negative_similarity
    );
}
```

### Report Output Example

```
===== Semantic Alignment Verification Report =====

Configuration:
  Min Positive Similarity: 0.70
  Max Negative Similarity: 0.30
  Required Margin: 0.40

Results Summary:
  Total Pairs: 50
  Passed: 47 (94.0%)
  Failed: 3 (6.0%)

Metrics:
  Avg Positive Similarity: 0.78
  Avg Negative Similarity: 0.18
  Margin Achieved: 0.60 ✓

Failed Pairs:
  ❌ loop_nested: Expected Equivalent, got 0.62 (below 0.70)
  ❌ async_callback: Expected Equivalent, got 0.58 (below 0.70)
  ❌ regex_pattern: Expected Unrelated, got 0.35 (above 0.30)

✓ OVERALL: PASS (94.0% >= 90.0% threshold)
=================================================
```

---
*Task ID: M03-S19*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
