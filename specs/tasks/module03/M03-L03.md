<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L03" version="1.0">
<metadata>
  <title>Semantic Model (E1 - intfloat/e5-large-v2)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>3</sequence>
  <implements>PRD: E1 Semantic embedding, 1024D output</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement semantic embedding model using intfloat/e5-large-v2 via candle-transformers.
This is the primary semantic understanding model producing 1024D dense vectors.

Model specifications:
- HuggingFace repo: intfloat/e5-large-v2
- Output dimension: 1024D
- Max tokens: 512
- Latency target: less than 5ms
- Instruction prefixes: "query: " for queries, "passage: " for documents

The e5 model family uses instruction prefixes to distinguish between queries
and passages, improving retrieval accuracy. This model provides the foundation
semantic representation for the 12-model ensemble.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct SemanticModel {
    model: E5Model,
    tokenizer: Tokenizer,
    device: Device,
    config: SingleModelConfig,
    loaded: AtomicBool,
    memory_size: usize,
}

impl SemanticModel {
    /// Create new semantic model instance
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Get instruction prefix for e5 model
    pub fn instruction_prefix(&self, is_query: bool) -> &'static str;

    /// Prepare input with instruction prefix
    fn prepare_input(&self, input: &ModelInput) -> String;
}

#[async_trait]
impl EmbeddingModel for SemanticModel {
    fn model_id(&self) -> ModelId { ModelId::Semantic }
    fn dimension(&self) -> usize { 1024 }
    fn max_tokens(&self) -> usize { 512 }
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for BERT-style inference</constraint>
    <constraint>Model loaded from models/semantic/ directory structure</constraint>
    <constraint>Supports instruction prefix for e5: "query: " or "passage: "</constraint>
    <constraint>Default prefix is "passage: " for documents</constraint>
    <constraint>dimension() returns 1024</constraint>
    <constraint>Mean pooling over token embeddings for sentence representation</constraint>
    <constraint>Output normalized to unit vector</constraint>
    <constraint>Latency target: less than 5ms single embed</constraint>
  </constraints>

  <verification>
    <step>Model loads from local path without network access</step>
    <step>embed() returns 1024D vector</step>
    <step>Output is L2 normalized (norm approximately 1.0)</step>
    <step>Instruction prefix applied correctly</step>
    <step>Batch embedding works with padding</step>
    <step>Memory usage reported accurately</step>
    <step>Latency under 5ms for single short text</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/semantic.rs</file>
  <file>crates/context-graph-embeddings/src/models/pretrained/mod.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test passes</criterion>
  <criterion>Integration test with real model weights</criterion>
  <criterion>Latency benchmark under 5ms P95</criterion>
</validation_criteria>
</task_spec>
