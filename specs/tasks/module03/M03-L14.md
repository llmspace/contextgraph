# M03-L14: Late-Interaction Model (E12 - ColBERT)

```xml
<task_spec id="M03-L14" version="1.0">
<metadata>
  <title>Implement Late-Interaction Model using ColBERT</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>14</sequence>
  <implements>PRD E12 - Late-Interaction embedding with ColBERT MaxSim</implements>
  <depends_on>M03-F09, M03-L01</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
Implement late-interaction embedding using colbert-ir/colbertv2.0.
ColBERT produces per-token embeddings (128D each) enabling fine-grained
matching via MaxSim scoring. Unlike single-vector models, ColBERT preserves
token-level information for more expressive retrieval.

Output for fusion: 128D pooled vector.
Storage: Full per-token embeddings for MaxSim retrieval.
Latency target: less than 8ms.
</context>

<definition_of_done>
  <signatures>
```rust
/// Per-token embeddings from ColBERT
pub struct TokenEmbeddings {
    /// Token vectors [num_tokens, 128]
    pub vectors: Vec<Vec<f32>>,
    /// Token strings for debugging
    pub tokens: Vec<String>,
    /// Mask for valid tokens (excludes padding)
    pub mask: Vec<bool>,
}

pub struct LateInteractionModel {
    model: ColBertModel,
    tokenizer: Tokenizer,
    device: Device,
    loaded: AtomicBool,
    config: SingleModelConfig,
    /// Linear projection to 128D
    projection: Linear,
}

impl LateInteractionModel {
    pub fn new(models_dir: &Path, config: SingleModelConfig) -> EmbeddingResult<Self>;

    /// Get full per-token embeddings for MaxSim scoring
    pub async fn embed_tokens(&self, text: &str) -> EmbeddingResult<TokenEmbeddings>;

    /// Pool token embeddings to single vector for fusion
    pub fn pool_tokens(&self, token_embs: &TokenEmbeddings) -> Vec<f32>;

    /// ColBERT MaxSim scoring: sum of max similarities
    pub fn maxsim_score(
        query_tokens: &TokenEmbeddings,
        doc_tokens: &TokenEmbeddings,
    ) -> f32;

    /// Batch MaxSim for efficient retrieval
    pub fn batch_maxsim(
        query_tokens: &TokenEmbeddings,
        doc_batch: &[TokenEmbeddings],
    ) -> Vec<f32>;
}

#[async_trait]
impl EmbeddingModel for LateInteractionModel {
    fn model_id(&self) -> ModelId { ModelId::LateInteraction }
    fn dimension(&self) -> usize { 128 }  // Pooled output
    fn max_tokens(&self) -> usize { 512 }
    fn supported_inputs(&self) -> &[InputType] { &[InputType::Text] }

    fn is_loaded(&self) -> bool;
    async fn load(&self) -> EmbeddingResult<()>;
    async fn unload(&self) -> EmbeddingResult<()>;

    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>>;

    fn memory_usage_bytes(&self) -> usize;
    fn warmup_complete(&self) -> bool;
}
```
  </signatures>

  <constraints>
    <constraint>Uses candle-transformers for inference</constraint>
    <constraint>Model loaded from models/late_interaction/ directory</constraint>
    <constraint>Produces per-token embeddings (128D each)</constraint>
    <constraint>dimension() returns 128 (pooled for fusion)</constraint>
    <constraint>max_tokens() returns 512</constraint>
    <constraint>MaxSim scoring for late interaction retrieval</constraint>
    <constraint>Stores full token embeddings for retrieval use</constraint>
    <constraint>Pools to single 128D vector for FuseMoE input</constraint>
    <constraint>Thread-safe with Send + Sync bounds</constraint>
    <constraint>Latency target: less than 8ms per embedding</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/pretrained/late_interaction.rs</file>
  <file>crates/context-graph-embeddings/src/models/pretrained/token_embeddings.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test --lib passes</criterion>
  <criterion>LateInteractionModel::new() creates valid instance</criterion>
  <criterion>Model loads weights from correct path</criterion>
  <criterion>embed_tokens() produces variable-length token embeddings</criterion>
  <criterion>Each token embedding is 128D</criterion>
  <criterion>pool_tokens() produces single 128D vector</criterion>
  <criterion>maxsim_score() computes correct late-interaction score</criterion>
  <criterion>Token embeddings are L2 normalized</criterion>
  <criterion>Latency under 8ms for typical inputs</criterion>
</validation_criteria>
</task_spec>
```

---

## Implementation Notes

### Model Details
- **HuggingFace Repo**: `colbert-ir/colbertv2.0`
- **Base Model**: BERT-based
- **Token Embedding Dimension**: 128
- **Max Sequence Length**: 512 tokens
- **Architecture**: BERT + Linear projection to 128D

### ColBERT MaxSim Scoring
The key innovation of ColBERT is the MaxSim scoring function:

```rust
impl LateInteractionModel {
    pub fn maxsim_score(
        query_tokens: &TokenEmbeddings,
        doc_tokens: &TokenEmbeddings,
    ) -> f32 {
        let mut total_score = 0.0;

        for (i, q_vec) in query_tokens.vectors.iter().enumerate() {
            if !query_tokens.mask[i] {
                continue;  // Skip padding
            }

            // Find maximum similarity to any document token
            let max_sim = doc_tokens.vectors.iter()
                .enumerate()
                .filter(|(j, _)| doc_tokens.mask[*j])
                .map(|(_, d_vec)| {
                    cosine_similarity(q_vec, d_vec)
                })
                .fold(f32::NEG_INFINITY, f32::max);

            total_score += max_sim;
        }

        total_score
    }
}
```

### Token Pooling for Fusion
For the FuseMoE fusion layer, we need a single 128D vector:

```rust
impl LateInteractionModel {
    pub fn pool_tokens(&self, token_embs: &TokenEmbeddings) -> Vec<f32> {
        // Mean pooling over valid tokens
        let valid_vectors: Vec<&Vec<f32>> = token_embs.vectors.iter()
            .zip(token_embs.mask.iter())
            .filter(|(_, &valid)| valid)
            .map(|(v, _)| v)
            .collect();

        let n = valid_vectors.len() as f32;
        let dim = valid_vectors[0].len();

        let mut pooled = vec![0.0f32; dim];
        for v in valid_vectors {
            for (i, val) in v.iter().enumerate() {
                pooled[i] += val / n;
            }
        }

        // L2 normalize
        let norm: f32 = pooled.iter().map(|x| x * x).sum::<f32>().sqrt();
        pooled.iter_mut().for_each(|x| *x /= norm);

        pooled
    }
}
```

### Efficient Batch MaxSim
For retrieval, we want to score a query against many documents efficiently:

```rust
impl LateInteractionModel {
    pub fn batch_maxsim(
        query_tokens: &TokenEmbeddings,
        doc_batch: &[TokenEmbeddings],
    ) -> Vec<f32> {
        // Can be SIMD-accelerated for production
        doc_batch.iter()
            .map(|doc| Self::maxsim_score(query_tokens, doc))
            .collect()
    }
}
```

### Storage Considerations
Per-token embeddings require more storage than single-vector models:
- **Single text (100 tokens)**: 100 * 128 * 4 bytes = 51.2 KB
- **Compression**: Can use int8 quantization to reduce to 12.8 KB
- **Trade-off**: Better retrieval quality vs. storage cost
