<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L17" version="1.0">
<metadata>
  <title>BatchProcessor Implementation with Dynamic Batching</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>17</sequence>
  <implements>PRD-EMB-002: Batch Processing Pipeline</implements>
  <depends_on>M03-L01, M03-L16, M03-F13</depends_on>
  <estimated_hours>5</estimated_hours>
</metadata>

<context>
Implement batch processor with dynamic batching per model. The BatchProcessor manages
per-model queues and processes embedding requests efficiently by grouping them into
optimal batch sizes. Batching triggers either when max_batch_size is reached OR when
the timeout expires (max_wait_ms). Requests are sorted by sequence length to minimize
padding waste and maximize GPU utilization.

This component is critical for achieving the throughput target of >100 items/sec at
batch size 32. Each model has its own queue to enable parallel processing across
different embedding models.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct BatchProcessor {
    registry: Arc<ModelRegistry>,
    queues: RwLock<HashMap<ModelId, BatchQueue>>,
    config: BatchConfig,
    workers: Vec<JoinHandle<()>>,
}

impl BatchProcessor {
    pub fn new(registry: Arc<ModelRegistry>, config: BatchConfig) -> Self;

    pub async fn submit(&self, model_id: ModelId, input: ModelInput)
        -> EmbeddingResult<ModelEmbedding>;

    pub async fn submit_batch(&self, model_id: ModelId, inputs: Vec<ModelInput>)
        -> EmbeddingResult<Vec<ModelEmbedding>>;

    pub fn shutdown(&self);
}
```
  </signatures>

  <constraints>
    <constraint>Batch triggers at max_batch_size OR timeout (max_wait_ms)</constraint>
    <constraint>Sort by sequence length reduces padding waste</constraint>
    <constraint>Per-model queues for parallelism (12 queues total)</constraint>
    <constraint>Throughput target: >100 items/sec at batch 32</constraint>
    <constraint>Thread-safe via RwLock on queues HashMap</constraint>
    <constraint>Worker threads for each active model</constraint>
    <constraint>Graceful shutdown waits for in-flight batches</constraint>
    <constraint>Uses oneshot channels for individual request responses</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/batch/processor.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes without warnings</criterion>
  <criterion>Unit tests for batch accumulation logic</criterion>
  <criterion>Unit tests for timeout triggering</criterion>
  <criterion>Unit tests for graceful shutdown</criterion>
  <criterion>Integration test: submit 100 requests, verify all complete</criterion>
  <criterion>Benchmark: verify >100 items/sec throughput</criterion>
</validation_criteria>

<implementation_notes>
The BatchProcessor spawns worker tasks for each model that:
1. Wait for requests in the queue
2. Accumulate until batch_size or timeout
3. Sort batch by input length
4. Call model.embed_batch()
5. Dispatch results via oneshot channels

Key algorithms:
- Dynamic batching with configurable min/max sizes
- Length-based sorting for padding efficiency
- Backpressure via bounded channels if needed
</implementation_notes>
</task_spec>
