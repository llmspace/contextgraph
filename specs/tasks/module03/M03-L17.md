# M03-L17: BatchProcessor Implementation with Dynamic Batching

```xml
<task_spec id="M03-L17" version="2.1">
<metadata>
  <title>BatchProcessor: Async Multi-Model Batch Orchestration</title>
  <status>complete</status>
  <layer>logic</layer>
  <sequence>17</sequence>
  <implements>PRD-EMB-002: Batch Processing Pipeline, Constitution perf.throughput.embed_batch >1000/sec</implements>
  <depends_on>M03-L16 (BatchQueue/types.rs - COMPLETE), M03-L15 (DefaultModelFactory - COMPLETE), M03-F13 (BatchConfig - COMPLETE)</depends_on>
  <estimated_hours>5</estimated_hours>
  <last_updated>2026-01-01</last_updated>
  <completed_date>2026-01-01</completed_date>
  <verified_by>sherlock-holmes</verified_by>
</metadata>

<critical_context>
<!-- READ THIS FIRST - ESSENTIAL FOR CORRECT IMPLEMENTATION -->

CODEBASE STATE (verified 2026-01-01):
- M03-L16 COMPLETE: batch/types.rs (1270 lines) - BatchQueue, BatchRequest, Batch, BatchQueueStats
- M03-L15 COMPLETE: models/factory.rs (1021 lines) - DefaultModelFactory for all 12 models
- M03-F13 COMPLETE: models/registry.rs - ModelRegistry with lazy loading
- Batch module: crates/context-graph-embeddings/src/batch/{mod.rs, types.rs}
- THIS TASK CREATES: crates/context-graph-embeddings/src/batch/processor.rs

EXISTING TYPES YOU MUST USE (from batch/types.rs):
```rust
// BatchRequest - individual request with response channel
pub struct BatchRequest {
    pub id: Uuid,
    pub input: ModelInput,
    pub model_id: ModelId,
    pub response_tx: oneshot::Sender<EmbeddingResult<ModelEmbedding>>,
    pub submitted_at: Instant,
    pub priority: u8,
}

// BatchQueue - per-model queue with flush logic
pub struct BatchQueue {
    requests: VecDeque<BatchRequest>,
    config: BatchConfig,
    model_id: ModelId,
    stats: BatchQueueStats,
}
impl BatchQueue {
    pub fn new(model_id: ModelId, config: BatchConfig) -> Self;
    pub fn push(&mut self, request: BatchRequest);
    pub fn should_flush(&self) -> bool; // true if max_batch_size OR timeout reached
    pub fn drain_batch(&mut self) -> Option<Batch>; // extracts batch, sorts by length if enabled
    pub fn cancel_all(&mut self, message: impl Into<String>);
}

// Batch - assembled batch ready for GPU
pub struct Batch {
    pub id: Uuid,
    pub model_id: ModelId,
    pub inputs: Vec<ModelInput>,
    pub response_txs: Vec<oneshot::Sender<EmbeddingResult<ModelEmbedding>>>,
    pub request_ids: Vec<Uuid>,
    pub assembled_at: Instant,
    pub total_tokens: usize,
}
impl Batch {
    pub fn complete(self, results: Vec<EmbeddingResult<ModelEmbedding>>);
    pub fn fail(self, message: impl Into<String>);
}
```

EXISTING CONFIG (from config.rs):
```rust
pub struct BatchConfig {
    pub max_batch_size: usize,    // Default: 32
    pub max_wait_ms: u64,         // Default: 50ms
    pub sort_by_length: bool,     // Default: true
}
```

MODEL REGISTRY INTERFACE (from models/registry.rs):
```rust
impl ModelRegistry {
    pub async fn get_model(&self, model_id: ModelId) -> EmbeddingResult<Arc<dyn EmbeddingModel>>;
    // Lazy loads model on first access
}
```

EMBEDDING MODEL INTERFACE (from traits/embedding_model.rs):
```rust
#[async_trait]
pub trait EmbeddingModel: Send + Sync {
    fn model_id(&self) -> ModelId;
    fn is_initialized(&self) -> bool;
    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding>;
    async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>> {
        // Default implementation: sequential embed - OVERRIDE FOR GPU BATCHING
    }
}
```

CONSTITUTION TARGETS:
- perf.throughput.embed_batch: ">1000/sec"
- perf.latency.batch_embed_64: "<50ms"
- Constitution perf: single_embed <10ms

DESIGN PRINCIPLES (from constitution.yaml):
- NO FALLBACKS: All errors propagate via EmbeddingError
- FAIL FAST: Invalid state = immediate error with context
- NO MOCK DATA: Tests use real model registry and actual embeddings
- NO BACKWARDS COMPATIBILITY HACKS: System works or fails clearly
</critical_context>

<context>
Implement BatchProcessor that orchestrates multi-model batch processing with:
1. Per-model queues (12 queues, one per ModelId)
2. Worker tasks that monitor queues and trigger processing
3. Dynamic batching: flush on max_batch_size OR timeout
4. Async request/response via oneshot channels
5. Graceful shutdown with in-flight batch completion

The BatchProcessor is the central coordinator between clients submitting requests
and the ModelRegistry that provides loaded models for inference.
</context>

<definition_of_done>
  <signatures>
```rust
// File: crates/context-graph-embeddings/src/batch/processor.rs

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;

use tokio::sync::{mpsc, RwLock, Notify};
use tokio::task::JoinHandle;

use crate::config::BatchConfig;
use crate::error::{EmbeddingError, EmbeddingResult};
use crate::models::ModelRegistry;
use crate::types::{ModelEmbedding, ModelId, ModelInput};

use super::{Batch, BatchQueue, BatchRequest};

/// Configuration for the BatchProcessor.
#[derive(Debug, Clone)]
pub struct BatchProcessorConfig {
    /// Per-model batch configuration.
    pub batch_config: BatchConfig,

    /// How often to check queues for timeout (default: 10ms).
    pub poll_interval_ms: u64,

    /// Maximum concurrent batches across all models (default: 4).
    /// Limits GPU memory pressure.
    pub max_concurrent_batches: usize,

    /// Channel buffer size for incoming requests (default: 1000).
    pub request_buffer_size: usize,
}

impl Default for BatchProcessorConfig {
    fn default() -> Self {
        Self {
            batch_config: BatchConfig::default(),
            poll_interval_ms: 10,
            max_concurrent_batches: 4,
            request_buffer_size: 1000,
        }
    }
}

/// Statistics for the BatchProcessor.
#[derive(Debug, Clone, Default)]
pub struct BatchProcessorStats {
    /// Total requests submitted.
    pub requests_submitted: u64,
    /// Total batches processed.
    pub batches_processed: u64,
    /// Total requests completed successfully.
    pub requests_completed: u64,
    /// Total requests failed.
    pub requests_failed: u64,
    /// Current queue depth across all models.
    pub current_queue_depth: usize,
    /// Currently processing batch count.
    pub active_batches: usize,
}

/// Multi-model batch processor with dynamic batching.
///
/// Manages per-model queues and worker tasks that process embedding requests
/// in optimal batch sizes for GPU efficiency.
///
/// # Thread Safety
/// All operations are thread-safe. Internal state uses Arc<RwLock<>>.
///
/// # Lifecycle
/// 1. Create with `new()` - starts worker task
/// 2. Submit requests with `submit()` or `submit_batch()`
/// 3. Shutdown with `shutdown()` - waits for in-flight batches
pub struct BatchProcessor {
    /// Model registry for accessing loaded models.
    registry: Arc<ModelRegistry>,

    /// Per-model queues protected by RwLock.
    queues: Arc<RwLock<HashMap<ModelId, BatchQueue>>>,

    /// Configuration.
    config: BatchProcessorConfig,

    /// Channel for submitting requests to the worker.
    request_tx: mpsc::Sender<BatchRequest>,

    /// Worker task handle.
    worker_handle: Option<JoinHandle<()>>,

    /// Shutdown signal.
    shutdown_notify: Arc<Notify>,

    /// Statistics.
    stats: Arc<RwLock<BatchProcessorStats>>,
}

impl BatchProcessor {
    /// Create a new BatchProcessor and start the worker task.
    ///
    /// # Arguments
    /// * `registry` - Model registry for accessing models
    /// * `config` - Processor configuration
    ///
    /// # Errors
    /// * `EmbeddingError::ConfigError` if config is invalid
    pub async fn new(
        registry: Arc<ModelRegistry>,
        config: BatchProcessorConfig,
    ) -> EmbeddingResult<Self>;

    /// Submit a single embedding request.
    ///
    /// The request is queued and processed when the batch is ready
    /// (either max_batch_size reached or timeout expired).
    ///
    /// # Arguments
    /// * `model_id` - Target model
    /// * `input` - Input to embed
    ///
    /// # Returns
    /// The embedding result when processing completes.
    ///
    /// # Errors
    /// * `EmbeddingError::NotInitialized` if model not available
    /// * `EmbeddingError::BatchError` if processing fails
    /// * `EmbeddingError::Timeout` if request times out in queue
    pub async fn submit(
        &self,
        model_id: ModelId,
        input: ModelInput,
    ) -> EmbeddingResult<ModelEmbedding>;

    /// Submit multiple inputs for batch processing.
    ///
    /// Inputs are queued together and processed efficiently.
    /// Results are returned in the same order as inputs.
    ///
    /// # Arguments
    /// * `model_id` - Target model (same for all inputs)
    /// * `inputs` - Inputs to embed
    ///
    /// # Returns
    /// Embeddings in same order as inputs.
    ///
    /// # Errors
    /// * Returns first error encountered
    /// * All inputs fail if any critical error occurs
    pub async fn submit_batch(
        &self,
        model_id: ModelId,
        inputs: Vec<ModelInput>,
    ) -> EmbeddingResult<Vec<ModelEmbedding>>;

    /// Submit request with priority (higher = more urgent).
    pub async fn submit_with_priority(
        &self,
        model_id: ModelId,
        input: ModelInput,
        priority: u8,
    ) -> EmbeddingResult<ModelEmbedding>;

    /// Get current queue depth for a model.
    pub async fn queue_depth(&self, model_id: ModelId) -> usize;

    /// Get total queue depth across all models.
    pub async fn total_queue_depth(&self) -> usize;

    /// Get current statistics snapshot.
    pub async fn stats(&self) -> BatchProcessorStats;

    /// Check if processor is running.
    pub fn is_running(&self) -> bool;

    /// Graceful shutdown - waits for in-flight batches.
    pub async fn shutdown(&mut self);
}

// SAFETY: All internal state is protected by Arc<RwLock<>> or mpsc channels
unsafe impl Send for BatchProcessor {}
unsafe impl Sync for BatchProcessor {}
```
  </signatures>

  <constraints>
    <constraint>Per-model queues: One BatchQueue per ModelId (12 total)</constraint>
    <constraint>Worker task monitors ALL queues in single loop with poll_interval_ms</constraint>
    <constraint>Batch triggers: should_flush() = max_batch_size OR timeout</constraint>
    <constraint>Sort by length: Enabled via BatchConfig.sort_by_length for padding efficiency</constraint>
    <constraint>Concurrency limit: max_concurrent_batches limits simultaneous GPU usage</constraint>
    <constraint>Throughput target: >100 items/sec at batch 32 (Constitution: >1000/sec)</constraint>
    <constraint>Latency target: batch_embed_64 < 50ms</constraint>
    <constraint>Graceful shutdown: Complete all in-flight batches before returning</constraint>
    <constraint>Request channels: oneshot for individual responses, mpsc for request submission</constraint>
    <constraint>Thread-safe: Arc<RwLock<>> for shared state, atomics for statistics</constraint>
    <constraint>NO FALLBACKS: All errors propagate immediately</constraint>
    <constraint>NO MOCK DATA: Tests use real ModelRegistry</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file action="create">crates/context-graph-embeddings/src/batch/processor.rs</file>
</files_to_create>

<files_to_modify>
  <file action="modify">crates/context-graph-embeddings/src/batch/mod.rs</file>
</files_to_modify>

<mod_rs_update>
Add to crates/context-graph-embeddings/src/batch/mod.rs:

```rust
mod processor;

pub use processor::{
    BatchProcessor,
    BatchProcessorConfig,
    BatchProcessorStats,
};
```
</mod_rs_update>

<implementation_pattern>
```rust
// File: crates/context-graph-embeddings/src/batch/processor.rs

use std::collections::HashMap;
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::time::Duration;

use tokio::sync::{mpsc, RwLock, Notify, Semaphore};
use tokio::task::JoinHandle;
use tokio::time::interval;

use crate::config::BatchConfig;
use crate::error::{EmbeddingError, EmbeddingResult};
use crate::models::ModelRegistry;
use crate::types::{ModelEmbedding, ModelId, ModelInput};

use super::{Batch, BatchQueue, BatchRequest};

// ============================================================================
// CONFIGURATION
// ============================================================================

#[derive(Debug, Clone)]
pub struct BatchProcessorConfig {
    pub batch_config: BatchConfig,
    pub poll_interval_ms: u64,
    pub max_concurrent_batches: usize,
    pub request_buffer_size: usize,
}

impl Default for BatchProcessorConfig {
    fn default() -> Self {
        Self {
            batch_config: BatchConfig::default(),
            poll_interval_ms: 10,
            max_concurrent_batches: 4,
            request_buffer_size: 1000,
        }
    }
}

impl BatchProcessorConfig {
    pub fn validate(&self) -> EmbeddingResult<()> {
        if self.max_concurrent_batches == 0 {
            return Err(EmbeddingError::ConfigError {
                message: "max_concurrent_batches must be > 0".to_string(),
            });
        }
        if self.request_buffer_size == 0 {
            return Err(EmbeddingError::ConfigError {
                message: "request_buffer_size must be > 0".to_string(),
            });
        }
        Ok(())
    }
}

// ============================================================================
// STATISTICS
// ============================================================================

#[derive(Debug, Default)]
pub struct BatchProcessorStatsInternal {
    pub requests_submitted: AtomicU64,
    pub batches_processed: AtomicU64,
    pub requests_completed: AtomicU64,
    pub requests_failed: AtomicU64,
}

#[derive(Debug, Clone, Default)]
pub struct BatchProcessorStats {
    pub requests_submitted: u64,
    pub batches_processed: u64,
    pub requests_completed: u64,
    pub requests_failed: u64,
    pub current_queue_depth: usize,
    pub active_batches: usize,
}

// ============================================================================
// BATCH PROCESSOR
// ============================================================================

pub struct BatchProcessor {
    registry: Arc<ModelRegistry>,
    queues: Arc<RwLock<HashMap<ModelId, BatchQueue>>>,
    config: BatchProcessorConfig,
    request_tx: mpsc::Sender<BatchRequest>,
    worker_handle: Option<JoinHandle<()>>,
    shutdown_notify: Arc<Notify>,
    is_running: Arc<AtomicBool>,
    stats: Arc<BatchProcessorStatsInternal>,
    batch_semaphore: Arc<Semaphore>,
}

impl BatchProcessor {
    pub async fn new(
        registry: Arc<ModelRegistry>,
        config: BatchProcessorConfig,
    ) -> EmbeddingResult<Self> {
        // Validate config - FAIL FAST
        config.validate()?;

        // Create per-model queues for all 12 models
        let mut queues = HashMap::new();
        for model_id in ModelId::all() {
            queues.insert(*model_id, BatchQueue::new(*model_id, config.batch_config.clone()));
        }
        let queues = Arc::new(RwLock::new(queues));

        // Create channels
        let (request_tx, request_rx) = mpsc::channel(config.request_buffer_size);

        // Create synchronization primitives
        let shutdown_notify = Arc::new(Notify::new());
        let is_running = Arc::new(AtomicBool::new(true));
        let stats = Arc::new(BatchProcessorStatsInternal::default());
        let batch_semaphore = Arc::new(Semaphore::new(config.max_concurrent_batches));

        // Clone for worker
        let worker_queues = queues.clone();
        let worker_registry = registry.clone();
        let worker_shutdown = shutdown_notify.clone();
        let worker_running = is_running.clone();
        let worker_stats = stats.clone();
        let worker_semaphore = batch_semaphore.clone();
        let poll_interval = Duration::from_millis(config.poll_interval_ms);

        // Spawn worker task
        let worker_handle = tokio::spawn(async move {
            Self::worker_loop(
                worker_queues,
                worker_registry,
                request_rx,
                worker_shutdown,
                worker_running,
                worker_stats,
                worker_semaphore,
                poll_interval,
            ).await;
        });

        Ok(Self {
            registry,
            queues,
            config,
            request_tx,
            worker_handle: Some(worker_handle),
            shutdown_notify,
            is_running,
            stats,
            batch_semaphore,
        })
    }

    /// Main worker loop that processes requests and batches.
    async fn worker_loop(
        queues: Arc<RwLock<HashMap<ModelId, BatchQueue>>>,
        registry: Arc<ModelRegistry>,
        mut request_rx: mpsc::Receiver<BatchRequest>,
        shutdown_notify: Arc<Notify>,
        is_running: Arc<AtomicBool>,
        stats: Arc<BatchProcessorStatsInternal>,
        batch_semaphore: Arc<Semaphore>,
        poll_interval: Duration,
    ) {
        let mut poll_timer = interval(poll_interval);

        loop {
            tokio::select! {
                // Check for shutdown
                _ = shutdown_notify.notified() => {
                    // Process remaining batches before exiting
                    Self::flush_all_queues(&queues, &registry, &stats, &batch_semaphore).await;
                    break;
                }

                // Receive new requests
                Some(request) = request_rx.recv() => {
                    let model_id = request.model_id;

                    // Add to appropriate queue
                    {
                        let mut queues_guard = queues.write().await;
                        if let Some(queue) = queues_guard.get_mut(&model_id) {
                            queue.push(request);
                        }
                    }

                    // Check if this queue should flush
                    Self::check_and_process_queue(
                        &queues,
                        &registry,
                        model_id,
                        &stats,
                        &batch_semaphore,
                    ).await;
                }

                // Poll for timeouts
                _ = poll_timer.tick() => {
                    if !is_running.load(Ordering::Relaxed) {
                        break;
                    }

                    // Check all queues for timeout-triggered flushes
                    for model_id in ModelId::all() {
                        Self::check_and_process_queue(
                            &queues,
                            &registry,
                            *model_id,
                            &stats,
                            &batch_semaphore,
                        ).await;
                    }
                }
            }
        }
    }

    /// Check if a queue should flush and process the batch.
    async fn check_and_process_queue(
        queues: &Arc<RwLock<HashMap<ModelId, BatchQueue>>>,
        registry: &Arc<ModelRegistry>,
        model_id: ModelId,
        stats: &Arc<BatchProcessorStatsInternal>,
        batch_semaphore: &Arc<Semaphore>,
    ) {
        // Check if should flush (read lock)
        let should_flush = {
            let queues_guard = queues.read().await;
            queues_guard.get(&model_id)
                .map(|q| q.should_flush())
                .unwrap_or(false)
        };

        if !should_flush {
            return;
        }

        // Acquire semaphore permit for concurrent batch limiting
        let permit = match batch_semaphore.try_acquire() {
            Ok(permit) => permit,
            Err(_) => return, // Max concurrent batches reached, try next poll
        };

        // Extract batch (write lock)
        let batch = {
            let mut queues_guard = queues.write().await;
            queues_guard.get_mut(&model_id)
                .and_then(|q| q.drain_batch())
        };

        if let Some(batch) = batch {
            // Process batch asynchronously
            let registry = registry.clone();
            let stats = stats.clone();

            tokio::spawn(async move {
                let _permit = permit; // Hold permit until batch completes
                Self::process_batch(batch, &registry, &stats).await;
            });
        }
    }

    /// Process a single batch through the model.
    async fn process_batch(
        batch: Batch,
        registry: &Arc<ModelRegistry>,
        stats: &Arc<BatchProcessorStatsInternal>,
    ) {
        let batch_size = batch.len();
        let model_id = batch.model_id;

        // Get model from registry
        let model = match registry.get_model(model_id).await {
            Ok(model) => model,
            Err(e) => {
                // Fail entire batch
                batch.fail(format!("Failed to get model {:?}: {}", model_id, e));
                stats.requests_failed.fetch_add(batch_size as u64, Ordering::Relaxed);
                return;
            }
        };

        // Process batch
        let results = model.embed_batch(&batch.inputs).await;

        match results {
            Ok(embeddings) => {
                // Wrap in Ok for complete()
                let results: Vec<EmbeddingResult<ModelEmbedding>> = embeddings
                    .into_iter()
                    .map(Ok)
                    .collect();
                batch.complete(results);
                stats.requests_completed.fetch_add(batch_size as u64, Ordering::Relaxed);
            }
            Err(e) => {
                // Fail entire batch
                batch.fail(format!("Batch embedding failed: {}", e));
                stats.requests_failed.fetch_add(batch_size as u64, Ordering::Relaxed);
            }
        }

        stats.batches_processed.fetch_add(1, Ordering::Relaxed);
    }

    /// Flush all queues (used during shutdown).
    async fn flush_all_queues(
        queues: &Arc<RwLock<HashMap<ModelId, BatchQueue>>>,
        registry: &Arc<ModelRegistry>,
        stats: &Arc<BatchProcessorStatsInternal>,
        batch_semaphore: &Arc<Semaphore>,
    ) {
        for model_id in ModelId::all() {
            loop {
                // Check if queue has items
                let has_items = {
                    let queues_guard = queues.read().await;
                    queues_guard.get(model_id)
                        .map(|q| !q.is_empty())
                        .unwrap_or(false)
                };

                if !has_items {
                    break;
                }

                // Acquire permit (blocking during shutdown is OK)
                let permit = batch_semaphore.acquire().await.unwrap();

                // Extract and process batch
                let batch = {
                    let mut queues_guard = queues.write().await;
                    queues_guard.get_mut(model_id)
                        .and_then(|q| q.drain_batch())
                };

                if let Some(batch) = batch {
                    Self::process_batch(batch, registry, stats).await;
                }

                drop(permit);
            }
        }
    }

    // ========================================================================
    // PUBLIC API
    // ========================================================================

    pub async fn submit(
        &self,
        model_id: ModelId,
        input: ModelInput,
    ) -> EmbeddingResult<ModelEmbedding> {
        if !self.is_running.load(Ordering::Relaxed) {
            return Err(EmbeddingError::BatchError {
                message: "BatchProcessor is shutting down".to_string(),
            });
        }

        let (request, rx) = BatchRequest::new(input, model_id);
        self.stats.requests_submitted.fetch_add(1, Ordering::Relaxed);

        // Send to worker
        self.request_tx.send(request).await
            .map_err(|_| EmbeddingError::BatchError {
                message: "Failed to submit request: channel closed".to_string(),
            })?;

        // Wait for result
        rx.await
            .map_err(|_| EmbeddingError::BatchError {
                message: "Request was dropped before completion".to_string(),
            })?
    }

    pub async fn submit_batch(
        &self,
        model_id: ModelId,
        inputs: Vec<ModelInput>,
    ) -> EmbeddingResult<Vec<ModelEmbedding>> {
        if inputs.is_empty() {
            return Ok(Vec::new());
        }

        if !self.is_running.load(Ordering::Relaxed) {
            return Err(EmbeddingError::BatchError {
                message: "BatchProcessor is shutting down".to_string(),
            });
        }

        // Create requests and collect receivers
        let mut receivers = Vec::with_capacity(inputs.len());

        for input in inputs {
            let (request, rx) = BatchRequest::new(input, model_id);
            self.stats.requests_submitted.fetch_add(1, Ordering::Relaxed);

            self.request_tx.send(request).await
                .map_err(|_| EmbeddingError::BatchError {
                    message: "Failed to submit request: channel closed".to_string(),
                })?;

            receivers.push(rx);
        }

        // Collect all results
        let mut results = Vec::with_capacity(receivers.len());
        for rx in receivers {
            let result = rx.await
                .map_err(|_| EmbeddingError::BatchError {
                    message: "Request was dropped before completion".to_string(),
                })??;
            results.push(result);
        }

        Ok(results)
    }

    pub async fn submit_with_priority(
        &self,
        model_id: ModelId,
        input: ModelInput,
        priority: u8,
    ) -> EmbeddingResult<ModelEmbedding> {
        if !self.is_running.load(Ordering::Relaxed) {
            return Err(EmbeddingError::BatchError {
                message: "BatchProcessor is shutting down".to_string(),
            });
        }

        let (request, rx) = BatchRequest::with_priority(input, model_id, priority);
        self.stats.requests_submitted.fetch_add(1, Ordering::Relaxed);

        self.request_tx.send(request).await
            .map_err(|_| EmbeddingError::BatchError {
                message: "Failed to submit request: channel closed".to_string(),
            })?;

        rx.await
            .map_err(|_| EmbeddingError::BatchError {
                message: "Request was dropped before completion".to_string(),
            })?
    }

    pub async fn queue_depth(&self, model_id: ModelId) -> usize {
        let queues_guard = self.queues.read().await;
        queues_guard.get(&model_id)
            .map(|q| q.len())
            .unwrap_or(0)
    }

    pub async fn total_queue_depth(&self) -> usize {
        let queues_guard = self.queues.read().await;
        queues_guard.values().map(|q| q.len()).sum()
    }

    pub async fn stats(&self) -> BatchProcessorStats {
        let queue_depth = self.total_queue_depth().await;
        let active = self.config.max_concurrent_batches - self.batch_semaphore.available_permits();

        BatchProcessorStats {
            requests_submitted: self.stats.requests_submitted.load(Ordering::Relaxed),
            batches_processed: self.stats.batches_processed.load(Ordering::Relaxed),
            requests_completed: self.stats.requests_completed.load(Ordering::Relaxed),
            requests_failed: self.stats.requests_failed.load(Ordering::Relaxed),
            current_queue_depth: queue_depth,
            active_batches: active,
        }
    }

    pub fn is_running(&self) -> bool {
        self.is_running.load(Ordering::Relaxed)
    }

    pub async fn shutdown(&mut self) {
        // Signal shutdown
        self.is_running.store(false, Ordering::Relaxed);
        self.shutdown_notify.notify_one();

        // Wait for worker to finish
        if let Some(handle) = self.worker_handle.take() {
            let _ = handle.await;
        }
    }
}

// SAFETY: All internal state protected by Arc<RwLock<>> or channels
unsafe impl Send for BatchProcessor {}
unsafe impl Sync for BatchProcessor {}

impl Drop for BatchProcessor {
    fn drop(&mut self) {
        self.is_running.store(false, Ordering::Relaxed);
        self.shutdown_notify.notify_one();
    }
}
```
</implementation_pattern>

<validation_criteria>
  <criterion>cargo check passes with no errors</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>cargo test batch --lib passes all tests</criterion>
  <criterion>BatchProcessor::new() creates and starts worker</criterion>
  <criterion>submit() queues request and returns embedding</criterion>
  <criterion>submit_batch() processes multiple inputs correctly</criterion>
  <criterion>Batch triggers at max_batch_size OR timeout</criterion>
  <criterion>max_concurrent_batches limits GPU pressure</criterion>
  <criterion>shutdown() completes all in-flight batches</criterion>
  <criterion>queue_depth() returns accurate counts</criterion>
  <criterion>stats() returns accurate metrics</criterion>
  <criterion>Integration test: 100 concurrent requests all complete</criterion>
  <criterion>Throughput: >100 items/sec at batch 32</criterion>
</validation_criteria>

<full_state_verification>
  <source_of_truth>
    <file path="crates/context-graph-embeddings/src/batch/types.rs">
      BatchQueue with should_flush() logic at lines 415-433
      Batch.complete() for result distribution at lines 658-671
      BatchRequest with oneshot channels
    </file>
    <file path="crates/context-graph-embeddings/src/models/registry.rs">
      ModelRegistry.get_model() for lazy loading
    </file>
    <file path="crates/context-graph-embeddings/src/config.rs">
      BatchConfig: max_batch_size=32, max_wait_ms=50, sort_by_length=true
    </file>
    <file path="docs2/constitution.yaml">
      perf.throughput.embed_batch: ">1000/sec"
      perf.latency.batch_embed_64: "<50ms"
    </file>
  </source_of_truth>

  <edge_case_audits>
    <test name="test_edge_case_1_empty_batch">
      BEFORE: Create processor, call submit_batch([])
      OPERATION: submit_batch(ModelId::Semantic, vec![])
      AFTER: Returns Ok(vec![]) immediately - no queue interaction
      VERIFY: No panic, returns empty vec
    </test>
    <test name="test_edge_case_2_shutdown_with_pending">
      BEFORE: Submit 50 requests (batch_size=32)
      OPERATION: Immediately call shutdown()
      AFTER: All 50 requests complete (first batch + remaining)
      VERIFY: stats().requests_completed == 50
    </test>
    <test name="test_edge_case_3_concurrent_models">
      BEFORE: Submit 10 requests each to 4 different models
      OPERATION: Parallel submit to Semantic, Code, Graph, Entity
      AFTER: All 40 complete, each model processed independently
      VERIFY: All 4 models' queues processed
    </test>
    <test name="test_edge_case_4_timeout_trigger">
      BEFORE: Configure max_wait_ms=10, submit 1 request (below batch_size)
      OPERATION: Wait 15ms
      AFTER: Single request processed via timeout trigger
      VERIFY: Request completes despite not reaching batch_size
    </test>
    <test name="test_edge_case_5_max_concurrent_batches">
      BEFORE: Configure max_concurrent_batches=2
      OPERATION: Submit 4 batches worth of requests simultaneously
      AFTER: Only 2 batches process at once
      VERIFY: stats().active_batches never exceeds 2
    </test>
    <test name="test_edge_case_6_channel_closed">
      BEFORE: Create processor, shutdown
      OPERATION: Try submit() after shutdown
      AFTER: Returns Err(BatchError) with "shutting down" message
      VERIFY: Error message is clear and actionable
    </test>
    <test name="test_edge_case_7_model_load_failure">
      BEFORE: Registry cannot load model (e.g., missing path)
      OPERATION: submit() to that model
      AFTER: Returns error with model context
      VERIFY: Error propagates correctly, other models unaffected
    </test>
    <test name="test_edge_case_8_high_priority_request">
      BEFORE: Submit 10 low-priority requests
      OPERATION: Submit 1 high-priority (priority=255) request
      AFTER: High-priority processed in same batch
      VERIFY: Priority preserved through queue
    </test>
  </edge_case_audits>

  <evidence_of_success>
    <evidence>cargo check: PASS</evidence>
    <evidence>cargo clippy: PASS (no warnings)</evidence>
    <evidence>cargo test batch --lib: All tests PASS</evidence>
    <evidence>processor.rs created at correct path</evidence>
    <evidence>batch/mod.rs exports BatchProcessor, BatchProcessorConfig, BatchProcessorStats</evidence>
    <evidence>submit() returns valid ModelEmbedding</evidence>
    <evidence>submit_batch() returns Vec in correct order</evidence>
    <evidence>stats().requests_submitted matches actual submissions</evidence>
    <evidence>stats().batches_processed > 0 after processing</evidence>
    <evidence>stats().requests_completed equals successful completions</evidence>
    <evidence>shutdown() waits for all in-flight batches</evidence>
    <evidence>total_queue_depth() returns 0 after all processing</evidence>
  </evidence_of_success>

  <physical_verification>
    <file_must_exist>crates/context-graph-embeddings/src/batch/processor.rs</file_must_exist>
    <export_must_exist module="batch/mod.rs">BatchProcessor</export_must_exist>
    <export_must_exist module="batch/mod.rs">BatchProcessorConfig</export_must_exist>
    <export_must_exist module="batch/mod.rs">BatchProcessorStats</export_must_exist>
    <test_must_pass>cargo test processor --lib</test_must_pass>
    <build_must_pass>cargo check -p context-graph-embeddings</build_must_pass>
    <clippy_must_pass>cargo clippy -p context-graph-embeddings</clippy_must_pass>
  </physical_verification>

  <manual_verification_protocol>
    TRIGGER EVENT: Call BatchProcessor::submit() with valid ModelInput
    PROCESS X: Request queued → batch assembled → model.embed_batch() → results distributed
    OUTCOME Y: Caller receives ModelEmbedding via oneshot channel

    VERIFICATION STEPS:
    1. After calling submit(), check stats().requests_submitted increased by 1
    2. After batch processes, check stats().batches_processed increased
    3. After result received, check stats().requests_completed increased
    4. Check embedding.vector.len() matches expected dimension (e.g., 1024 for Semantic)
    5. Check embedding.model_id matches requested model_id
    6. Check total_queue_depth() returns 0 after all processing complete
  </manual_verification_protocol>
</full_state_verification>

<test_implementation_pattern>
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Instant;
    use tempfile::TempDir;
    use crate::config::GpuConfig;
    use crate::models::DefaultModelFactory;

    async fn create_test_processor() -> (BatchProcessor, TempDir, Arc<ModelRegistry>) {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");

        // Create subdirectories for pretrained models
        for model_id in ModelId::pretrained() {
            let subdir = temp_dir.path().join(model_id.directory_name());
            std::fs::create_dir_all(&subdir).expect("Failed to create dir");
            std::fs::write(subdir.join("config.json"), "{}").expect("Failed to write config");
        }

        let factory = DefaultModelFactory::new(temp_dir.path(), GpuConfig::default())
            .expect("Failed to create factory");
        let registry = Arc::new(
            ModelRegistry::new(
                crate::models::ModelRegistryConfig::default(),
                Arc::new(factory),
            ).await.expect("Failed to create registry")
        );

        let mut config = BatchProcessorConfig::default();
        config.batch_config.max_batch_size = 4; // Small for testing
        config.batch_config.max_wait_ms = 50;

        let processor = BatchProcessor::new(registry.clone(), config)
            .await
            .expect("Failed to create processor");

        (processor, temp_dir, registry)
    }

    #[tokio::test]
    async fn test_evidence_of_success() {
        println!("\n========================================");
        println!("M03-L17 EVIDENCE OF SUCCESS");
        println!("========================================\n");

        let (processor, _temp, _) = create_test_processor().await;

        println!("1. PROCESSOR STATE:");
        println!("   is_running = {}", processor.is_running());

        // Submit requests to multiple models
        println!("\n2. MULTI-MODEL SUBMISSION:");
        for model_id in [ModelId::Semantic, ModelId::Code, ModelId::Entity] {
            let input = ModelInput::text("Test input").unwrap();
            let result = processor.submit(model_id, input).await;
            println!("   {:?}: {:?}", model_id, result.is_ok());
        }

        // Submit batch
        println!("\n3. BATCH SUBMISSION:");
        let inputs: Vec<_> = (0..10)
            .map(|i| ModelInput::text(format!("Batch item {}", i)).unwrap())
            .collect();
        let results = processor.submit_batch(ModelId::Semantic, inputs).await;
        println!("   batch results: {} items", results.as_ref().map(|r| r.len()).unwrap_or(0));

        // Statistics
        let stats = processor.stats().await;
        println!("\n4. FINAL STATISTICS:");
        println!("   requests_submitted = {}", stats.requests_submitted);
        println!("   batches_processed = {}", stats.batches_processed);
        println!("   requests_completed = {}", stats.requests_completed);
        println!("   requests_failed = {}", stats.requests_failed);

        println!("\n========================================");
        println!("ALL CHECKS PASSED");
        println!("========================================\n");

        assert!(processor.is_running());
        assert_eq!(stats.requests_completed, 13); // 3 + 10
        assert_eq!(stats.requests_failed, 0);
    }
}
```
</test_implementation_pattern>

<sherlock_verification>
After implementation is complete, spawn sherlock-holmes subagent to verify:

1. **File Existence**: processor.rs exists at crates/context-graph-embeddings/src/batch/processor.rs
2. **Module Export**: batch/mod.rs exports BatchProcessor, BatchProcessorConfig, BatchProcessorStats
3. **Worker Task**: Verify worker loop handles requests, timeouts, and shutdown
4. **Queue Management**: Verify 12 per-model queues are created
5. **Batching Logic**: Verify should_flush() triggers on max_batch_size OR timeout
6. **Concurrency Limit**: Verify max_concurrent_batches is enforced via semaphore
7. **Shutdown Behavior**: Verify shutdown() processes all in-flight batches
8. **Statistics**: Verify all counters update correctly
9. **Error Propagation**: Verify NO FALLBACKS - all errors propagate immediately
10. **Thread Safety**: Verify Send + Sync with Arc<RwLock<>> pattern
11. **Build Success**: cargo check, cargo test batch --lib, cargo clippy all pass
12. **Integration**: Verify works with real ModelRegistry (no mock data)

Sherlock MUST:
- Run `cargo test processor --lib -- --nocapture` to see evidence logs
- Verify all edge case tests print BEFORE/AFTER state
- Check stats counters match actual operations
- Confirm throughput test shows >100 requests/sec
- Verify no panics during shutdown with pending requests
</sherlock_verification>
</task_spec>
```

---

## Implementation Dependencies

### REQUIRED - Already Complete:
| Task | File | Status |
|------|------|--------|
| M03-L16 | `batch/types.rs` | ✅ COMPLETE (1270 lines) |
| M03-L15 | `models/factory.rs` | ✅ COMPLETE (1021 lines) |
| M03-F13 | `models/registry.rs` | ✅ COMPLETE |

### Key Imports Required:
```rust
// From batch/types.rs
use super::{Batch, BatchQueue, BatchRequest, BatchQueueStats};

// From models
use crate::models::ModelRegistry;

// From types
use crate::types::{ModelEmbedding, ModelId, ModelInput};

// From config
use crate::config::BatchConfig;

// From error
use crate::error::{EmbeddingError, EmbeddingResult};
```

---

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────┐
│                       BatchProcessor                                │
│  ┌───────────────────────────────────────────────────────────────┐  │
│  │                      Worker Task                               │  │
│  │  ┌─────────────────────────────────────────────────────────┐  │  │
│  │  │ request_rx ─────────────────────────────────────────────│──┼──┼─► Clients
│  │  │     │                                                    │  │  │   submit()
│  │  │     ▼                                                    │  │  │
│  │  │ ┌───────────┬───────────┬───────────┬──────────────────┐│  │  │
│  │  │ │ Queue:    │ Queue:    │ Queue:    │ ... (12 total)   ││  │  │
│  │  │ │ Semantic  │ Code      │ Graph     │                  ││  │  │
│  │  │ └─────┬─────┴─────┬─────┴─────┬─────┴──────────────────┘│  │  │
│  │  │       │           │           │                          │  │  │
│  │  │       ▼           ▼           ▼                          │  │  │
│  │  │ ┌─────────────────────────────────────────────────────┐ │  │  │
│  │  │ │           should_flush() checks                      │ │  │  │
│  │  │ │           (max_batch_size OR timeout)               │ │  │  │
│  │  │ └─────────────────────────────────────────────────────┘ │  │  │
│  │  │       │           │           │                          │  │  │
│  │  │       ▼           ▼           ▼                          │  │  │
│  │  │ ┌─────────────────────────────────────────────────────┐ │  │  │
│  │  │ │    drain_batch() → Batch (sorted by length)         │ │  │  │
│  │  │ └─────────────────────────────────────────────────────┘ │  │  │
│  │  │       │                                                  │  │  │
│  │  │       ▼                                                  │  │  │
│  │  │ ┌─────────────────────────────────────────────────────┐ │  │  │
│  │  │ │         Semaphore (max_concurrent_batches)          │ │  │  │
│  │  │ └─────────────────────────────────────────────────────┘ │  │  │
│  │  │       │                                                  │  │  │
│  │  │       ▼                                                  │  │  │
│  │  │ ┌─────────────────────────────────────────────────────┐ │  │  │
│  │  │ │         process_batch(batch, registry)              │ │  │  │
│  │  │ │         → model.embed_batch(inputs)                 │ │  │  │
│  │  │ │         → batch.complete(results)                   │ │  │  │
│  │  │ └─────────────────────────────────────────────────────┘ │  │  │
│  │  └─────────────────────────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────────┘  │
│                                                                     │
│  ModelRegistry ◄─── get_model(model_id) ◄─── lazy load on demand   │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Performance Targets

| Metric | Target | Source |
|--------|--------|--------|
| Throughput | >1000/sec | Constitution perf.throughput.embed_batch |
| Batch latency (64) | <50ms | Constitution perf.latency.batch_embed_64 |
| Single embed | <10ms | Constitution perf.latency.single_embed |
| Queue poll interval | 10ms | Default config |
| Max concurrent batches | 4 | Default config (GPU memory safety) |

---

## Completion Checklist

- [x] `processor.rs` created at `crates/context-graph-embeddings/src/batch/processor.rs`
- [x] `BatchProcessorConfig` struct with validation
- [x] `BatchProcessorStats` struct with atomic counters
- [x] `BatchProcessor::new()` creates worker task and 12 queues
- [x] Worker loop handles: requests, timeouts, shutdown
- [x] `submit()` queues request and awaits result
- [x] `submit_batch()` handles multiple inputs efficiently
- [x] `shutdown()` completes all in-flight batches
- [x] Semaphore enforces max_concurrent_batches
- [x] `batch/mod.rs` exports all new types
- [x] All 8 edge case tests pass with BEFORE/AFTER output
- [x] `test_evidence_of_success` logs all metrics
- [x] `cargo check` passes
- [x] `cargo test batch --lib` passes
- [x] `cargo clippy` passes with no warnings
- [x] sherlock-holmes verification complete - VERDICT: INNOCENT
