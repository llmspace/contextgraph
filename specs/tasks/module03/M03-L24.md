<?xml version="1.0" encoding="UTF-8"?>
<task_spec id="M03-L24" version="1.0">
<metadata>
  <title>CAME-AB Cross-Attention Bridge Layer</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>24</sequence>
  <implements>PRD-EMB-005: Optional Modality Enhancement</implements>
  <depends_on>M03-L23</depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Implement the CAME-AB (Cross-Attention Modality Encoding with Adaptive Blending)
bridge layer as an optional enhancement to FuseMoE. This layer applies cross-attention
between different modality pairs (e.g., semantic-code, semantic-graph) to capture
inter-modality relationships that may be missed by independent expert processing.

The CAME-AB module:
1. Groups embeddings by modality type
2. Applies multi-head cross-attention between pairs
3. Uses learned adaptive blending weights to combine outputs
4. Produces enhanced embedding that can replace or augment FuseMoE output

This is an OPTIONAL enhancement - the pipeline works without it, but CAME-AB
can improve quality for multimodal inputs.
</context>

<definition_of_done>
  <signatures>
```rust
pub struct CameAb {
    cross_attention: MultiHeadAttention,
    adaptive_blend: Linear,
    modality_embeddings: HashMap<ModalityType, Tensor>,
    num_heads: usize,
    bridge_dim: usize,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum ModalityType {
    Semantic,
    Temporal,
    Structural,
    Code,
    Multimodal,
}

pub struct MultiHeadAttention {
    query_proj: Linear,
    key_proj: Linear,
    value_proj: Linear,
    output_proj: Linear,
    num_heads: usize,
    head_dim: usize,
    dropout: f32,
}

impl CameAb {
    pub fn new(input_dim: usize, num_heads: usize) -> EmbeddingResult<Self>;

    /// Apply CAME-AB enhancement to concatenated embeddings
    /// Returns enhanced embedding that can be added to or replace FuseMoE output
    pub fn enhance(
        &self,
        concatenated: &ConcatenatedEmbedding
    ) -> EmbeddingResult<Vec<f32>>;

    /// Apply to FusedEmbedding, blending with original
    pub fn blend_with_fused(
        &self,
        concatenated: &ConcatenatedEmbedding,
        fused: &FusedEmbedding,
        blend_weight: f32,
    ) -> EmbeddingResult<FusedEmbedding>;

    /// Get modality groupings for the 12 models
    pub fn modality_groups() -> HashMap<ModelId, ModalityType>;
}

impl MultiHeadAttention {
    pub fn new(
        embed_dim: usize,
        num_heads: usize,
        dropout: f32
    ) -> EmbeddingResult<Self>;

    /// Standard multi-head attention
    /// query, key, value: [batch, seq_len, embed_dim]
    pub fn forward(
        &self,
        query: &Tensor,
        key: &Tensor,
        value: &Tensor,
        mask: Option<&Tensor>,
    ) -> EmbeddingResult<Tensor>;
}
```
  </signatures>

  <constraints>
    <constraint>Optional layer - pipeline works without it</constraint>
    <constraint>Cross-attention between modality pairs</constraint>
    <constraint>Learnable blending weights</constraint>
    <constraint>Multi-head attention with configurable heads (default: 8)</constraint>
    <constraint>Modality grouping maps 12 models to 5 modality types</constraint>
    <constraint>Bridge dimension matches FuseMoE output (1536)</constraint>
    <constraint>Dropout for regularization during training</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/fusion/came_ab.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes without warnings</criterion>
  <criterion>Unit test: MultiHeadAttention output shape correct</criterion>
  <criterion>Unit test: enhance() produces valid embedding</criterion>
  <criterion>Unit test: blend_with_fused() produces valid FusedEmbedding</criterion>
  <criterion>Unit test: modality_groups() returns all 12 models</criterion>
  <criterion>Unit test: blend_weight=0.0 returns original fused</criterion>
  <criterion>Unit test: blend_weight=1.0 returns enhanced only</criterion>
</validation_criteria>

<implementation_notes>
Modality groupings for 12 models:
```rust
fn modality_groups() -> HashMap<ModelId, ModalityType> {
    [
        (ModelId::Semantic, ModalityType::Semantic),
        (ModelId::TemporalRecent, ModalityType::Temporal),
        (ModelId::TemporalPeriodic, ModalityType::Temporal),
        (ModelId::TemporalPositional, ModalityType::Temporal),
        (ModelId::Causal, ModalityType::Semantic),
        (ModelId::Sparse, ModalityType::Semantic),
        (ModelId::Code, ModalityType::Code),
        (ModelId::Graph, ModalityType::Structural),
        (ModelId::Hdc, ModalityType::Structural),
        (ModelId::Multimodal, ModalityType::Multimodal),
        (ModelId::Entity, ModalityType::Structural),
        (ModelId::LateInteraction, ModalityType::Semantic),
    ].into_iter().collect()
}
```

Cross-attention between modalities:
- Semantic attends to Code (understands code descriptions)
- Semantic attends to Structural (understands entity relationships)
- Temporal attends to Semantic (time-aware meaning)
- All pairs contribute to final enhanced embedding

Adaptive blending:
```rust
// blend_weight learned per-modality pair
enhanced = sum_over_pairs(attention_output * blend_weight)
final = (1 - alpha) * fused + alpha * enhanced
```
</implementation_notes>
</task_spec>
