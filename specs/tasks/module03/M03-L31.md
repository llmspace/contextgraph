# Task Specification: M03-L31

```xml
<task_spec id="M03-L31" version="1.0">
<metadata>
  <title>FuseMoE Weight Registry and Initialization</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>31</sequence>
  <implements>
    - Architecture: FuseMoE weight management
    - PRD: Deterministic embedding outputs
    - Constitution: embeddings.fusion.weights
  </implements>
  <depends_on>M03-L23, M03-L21, M03-S13</depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
While M03-L23 mentions `load_weights` for the FuseMoE module, it doesn't specify
**where** those weights come from. Unlike E1-E12 (downloaded from HuggingFace),
FuseMoE weights are custom to this system.

This task implements the WeightRegistry for FuseMoE that can:
1. Load pre-trained "Golden Checkpoint" from project artifacts
2. Perform "Zero-Shot Initialization" for fresh deployments
3. Handle incremental weight updates during fine-tuning
4. Manage weight versioning and rollback
5. Ensure deterministic initialization for reproducibility

Without proper initialization, the FuseMoE experts start with random weights
and the pipeline returns garbage embeddings on day one. The WeightRegistry
ensures consistent, deterministic, and production-ready weight management.

Key features:
- Multiple initialization strategies (checkpoint, kaiming, xavier, orthogonal)
- Version tracking for model weights
- Checksum validation for weight integrity
- Graceful fallback chain (checkpoint → backup → initialization)
- Thread-safe concurrent access
</context>

<definition_of_done>
  <signatures>
```rust
use std::path::{Path, PathBuf};
use std::sync::Arc;

/// Weight initialization strategy
#[derive(Debug, Clone)]
pub enum InitializationStrategy {
    /// Load from checkpoint file
    Checkpoint {
        path: PathBuf,
        strict: bool,
    },
    /// Kaiming (He) uniform initialization
    KaimingUniform {
        mode: KaimingMode,
        nonlinearity: Nonlinearity,
    },
    /// Kaiming (He) normal initialization
    KaimingNormal {
        mode: KaimingMode,
        nonlinearity: Nonlinearity,
    },
    /// Xavier (Glorot) uniform initialization
    XavierUniform,
    /// Xavier (Glorot) normal initialization
    XavierNormal,
    /// Orthogonal initialization
    Orthogonal { gain: f32 },
    /// Zero initialization (for biases)
    Zeros,
    /// Ones initialization
    Ones,
    /// Custom initialization function
    Custom(Arc<dyn Fn(&[usize]) -> Tensor + Send + Sync>),
}

/// Kaiming initialization mode
#[derive(Debug, Clone, Copy)]
pub enum KaimingMode {
    FanIn,
    FanOut,
    FanAvg,
}

/// Nonlinearity for gain calculation
#[derive(Debug, Clone, Copy)]
pub enum Nonlinearity {
    Linear,
    ReLU,
    LeakyReLU(f32),
    Tanh,
    Sigmoid,
    GELU,
    SiLU,
}

/// Weight versioning metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WeightVersion {
    /// Semantic version (e.g., "1.2.3")
    pub version: String,
    /// Git commit or training run ID
    pub source_id: String,
    /// Training date
    pub created_at: DateTime<Utc>,
    /// SHA256 checksum of weights
    pub checksum: String,
    /// Training metrics at checkpoint
    pub metrics: Option<TrainingMetrics>,
}

/// Training metrics at checkpoint time
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingMetrics {
    pub epoch: usize,
    pub loss: f64,
    pub validation_loss: Option<f64>,
    pub learning_rate: f64,
}

/// Checkpoint format for FuseMoE weights
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FuseMoECheckpoint {
    /// Version metadata
    pub version: WeightVersion,
    /// Gating network weights
    pub gating_weights: CheckpointTensor,
    /// Expert network weights (all 8 experts)
    pub expert_weights: Vec<ExpertCheckpoint>,
    /// Output projection weights
    pub output_projection: CheckpointTensor,
    /// Configuration used during training
    pub config: FusionConfig,
}

/// Expert-specific checkpoint data
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExpertCheckpoint {
    pub expert_id: usize,
    pub input_projection: CheckpointTensor,
    pub hidden_layers: Vec<CheckpointTensor>,
    pub output_projection: CheckpointTensor,
    pub bias: Option<CheckpointTensor>,
}

/// Serializable tensor format
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CheckpointTensor {
    pub shape: Vec<usize>,
    pub dtype: DType,
    pub data: Vec<u8>,
}

/// FuseMoE weight registry and manager
pub struct WeightRegistry {
    /// Artifact base directory
    artifact_dir: PathBuf,
    /// Loaded weights cache
    loaded_weights: RwLock<Option<LoadedWeights>>,
    /// Version history
    versions: Vec<WeightVersion>,
    /// Device for weight allocation
    device: Device,
    /// Initialization config
    init_config: InitializationConfig,
}

/// Configuration for weight initialization
#[derive(Debug, Clone)]
pub struct InitializationConfig {
    /// Primary strategy (usually Checkpoint)
    pub primary: InitializationStrategy,
    /// Fallback if primary fails
    pub fallback: InitializationStrategy,
    /// Seed for deterministic initialization
    pub seed: u64,
    /// Verify checksum on load
    pub verify_checksum: bool,
}

impl Default for InitializationConfig {
    fn default() -> Self {
        Self {
            primary: InitializationStrategy::Checkpoint {
                path: PathBuf::from("artifacts/fusemoe/golden.ckpt"),
                strict: true,
            },
            fallback: InitializationStrategy::KaimingUniform {
                mode: KaimingMode::FanIn,
                nonlinearity: Nonlinearity::ReLU,
            },
            seed: 42,
            verify_checksum: true,
        }
    }
}

/// Loaded weights in GPU memory
pub struct LoadedWeights {
    pub gating: GatingNetworkWeights,
    pub experts: Vec<ExpertWeights>,
    pub output: Tensor,
    pub version: WeightVersion,
}

impl WeightRegistry {
    /// Create registry with artifact directory
    pub fn new(artifact_dir: impl AsRef<Path>, device: Device) -> Self;

    /// Create with custom initialization config
    pub fn with_config(
        artifact_dir: impl AsRef<Path>,
        device: Device,
        config: InitializationConfig,
    ) -> Self;

    /// Load weights (tries primary, falls back if needed)
    pub async fn load_weights(&self) -> EmbeddingResult<Arc<LoadedWeights>>;

    /// Load specific checkpoint version
    pub async fn load_checkpoint(&self, version: &str) -> EmbeddingResult<Arc<LoadedWeights>>;

    /// Initialize weights from scratch
    pub fn initialize_weights(&self, strategy: &InitializationStrategy) -> EmbeddingResult<LoadedWeights>;

    /// Save current weights as checkpoint
    pub async fn save_checkpoint(
        &self,
        weights: &LoadedWeights,
        metrics: Option<TrainingMetrics>,
    ) -> EmbeddingResult<WeightVersion>;

    /// List available checkpoint versions
    pub fn list_versions(&self) -> &[WeightVersion];

    /// Get current loaded version
    pub fn current_version(&self) -> Option<&WeightVersion>;

    /// Verify checkpoint integrity
    pub fn verify_checkpoint(&self, path: impl AsRef<Path>) -> EmbeddingResult<bool>;

    /// Rollback to previous version
    pub async fn rollback(&self, version: &str) -> EmbeddingResult<Arc<LoadedWeights>>;

    /// Hot-swap weights without downtime
    pub async fn hot_swap(&self, new_weights: LoadedWeights) -> EmbeddingResult<()>;
}

/// Weight initializer implementing various strategies
pub struct WeightInitializer {
    seed: u64,
    device: Device,
}

impl WeightInitializer {
    pub fn new(seed: u64, device: Device) -> Self;

    /// Initialize tensor with strategy
    pub fn initialize(
        &self,
        shape: &[usize],
        strategy: &InitializationStrategy,
    ) -> CudaResult<Tensor>;

    /// Initialize FuseMoE gating network
    pub fn init_gating(&self, config: &FusionConfig) -> CudaResult<GatingNetworkWeights>;

    /// Initialize single expert network
    pub fn init_expert(&self, expert_id: usize, config: &FusionConfig) -> CudaResult<ExpertWeights>;

    /// Initialize all experts
    pub fn init_all_experts(&self, config: &FusionConfig) -> CudaResult<Vec<ExpertWeights>>;

    /// Initialize output projection
    pub fn init_output_projection(&self, config: &FusionConfig) -> CudaResult<Tensor>;
}
```
  </signatures>

  <constraints>
    <constraint>Golden checkpoint must be validated before production use</constraint>
    <constraint>Initialization must be deterministic given same seed</constraint>
    <constraint>Checksum verification prevents corrupted weight loading</constraint>
    <constraint>Fallback chain must complete without human intervention</constraint>
    <constraint>Hot-swap must be atomic (no partial weight states)</constraint>
    <constraint>Version history persisted to disk</constraint>
    <constraint>Thread-safe for concurrent read access</constraint>
    <constraint>GPU memory properly freed on weight swap</constraint>
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/fusion/weights/mod.rs</file>
  <file>crates/context-graph-embeddings/src/fusion/weights/registry.rs</file>
  <file>crates/context-graph-embeddings/src/fusion/weights/initializer.rs</file>
  <file>crates/context-graph-embeddings/src/fusion/weights/checkpoint.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>Checkpoint loading from valid file succeeds</criterion>
  <criterion>Checksum mismatch returns error</criterion>
  <criterion>Fallback initialization produces valid weights</criterion>
  <criterion>Kaiming initialization has correct variance</criterion>
  <criterion>Same seed produces identical weights</criterion>
  <criterion>Version tracking persists across restarts</criterion>
  <criterion>Hot-swap is atomic and thread-safe</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Checkpoint File Format
```
fusemoe_v1.2.0.ckpt (safetensors format)
├── metadata.json
│   ├── version: "1.2.0"
│   ├── source_id: "training-run-2024-001"
│   ├── created_at: "2024-01-15T10:30:00Z"
│   ├── checksum: "sha256:abc123..."
│   └── config: { ... FusionConfig ... }
├── gating/
│   ├── linear.weight: [8320, 8]
│   ├── linear.bias: [8]
│   └── temperature: scalar
├── experts/
│   ├── expert_0/
│   │   ├── input_proj.weight: [8320, 1024]
│   │   ├── hidden.weight: [1024, 1024]
│   │   └── output_proj.weight: [1024, 192]
│   ├── expert_1/ ...
│   └── expert_7/ ...
└── output/
    └── projection.weight: [1536, 1536]
```

### Loading with Fallback Chain
```rust
impl WeightRegistry {
    pub async fn load_weights(&self) -> EmbeddingResult<Arc<LoadedWeights>> {
        // Check cache first
        {
            let cached = self.loaded_weights.read().await;
            if let Some(ref weights) = *cached {
                return Ok(Arc::new(weights.clone()));
            }
        }

        // Try primary strategy
        let weights = match &self.init_config.primary {
            InitializationStrategy::Checkpoint { path, strict } => {
                match self.load_from_checkpoint(path, *strict).await {
                    Ok(w) => {
                        info!("Loaded weights from checkpoint: {}", path.display());
                        w
                    }
                    Err(e) => {
                        warn!("Checkpoint load failed: {}. Trying fallback.", e);
                        self.apply_fallback()?
                    }
                }
            }
            other => {
                let initializer = WeightInitializer::new(
                    self.init_config.seed,
                    self.device.clone(),
                );
                self.initialize_with_strategy(&initializer, other)?
            }
        };

        // Cache and return
        {
            let mut cached = self.loaded_weights.write().await;
            *cached = Some(weights.clone());
        }

        Ok(Arc::new(weights))
    }

    fn apply_fallback(&self) -> EmbeddingResult<LoadedWeights> {
        warn!("Applying fallback initialization strategy");

        let initializer = WeightInitializer::new(
            self.init_config.seed,
            self.device.clone(),
        );

        let config = FusionConfig::default();

        let gating = initializer.init_gating(&config)?;
        let experts = initializer.init_all_experts(&config)?;
        let output = initializer.init_output_projection(&config)?;

        Ok(LoadedWeights {
            gating,
            experts,
            output,
            version: WeightVersion {
                version: "0.0.0-init".to_string(),
                source_id: format!("zero-shot-seed-{}", self.init_config.seed),
                created_at: Utc::now(),
                checksum: "none".to_string(),
                metrics: None,
            },
        })
    }
}
```

### Kaiming Initialization
```rust
impl WeightInitializer {
    pub fn initialize(
        &self,
        shape: &[usize],
        strategy: &InitializationStrategy,
    ) -> CudaResult<Tensor> {
        let mut rng = StdRng::seed_from_u64(self.seed);

        match strategy {
            InitializationStrategy::KaimingUniform { mode, nonlinearity } => {
                let (fan_in, fan_out) = calculate_fan(shape);
                let fan = match mode {
                    KaimingMode::FanIn => fan_in,
                    KaimingMode::FanOut => fan_out,
                    KaimingMode::FanAvg => (fan_in + fan_out) / 2,
                };

                let gain = nonlinearity.gain();
                let std = gain / (fan as f32).sqrt();
                let bound = (3.0_f32).sqrt() * std;

                let numel: usize = shape.iter().product();
                let data: Vec<f32> = (0..numel)
                    .map(|_| rng.gen_range(-bound..bound))
                    .collect();

                Tensor::from_vec(data, shape, &self.device)
            }

            InitializationStrategy::XavierUniform => {
                let (fan_in, fan_out) = calculate_fan(shape);
                let bound = (6.0 / (fan_in + fan_out) as f32).sqrt();

                let numel: usize = shape.iter().product();
                let data: Vec<f32> = (0..numel)
                    .map(|_| rng.gen_range(-bound..bound))
                    .collect();

                Tensor::from_vec(data, shape, &self.device)
            }

            InitializationStrategy::Zeros => {
                Tensor::zeros(shape, &self.device)
            }

            InitializationStrategy::Checkpoint { path, .. } => {
                // Delegate to checkpoint loader
                Err(CudaError::InvalidOperation(
                    "Use load_from_checkpoint for checkpoint strategy".to_string()
                ))
            }

            // ... other strategies
        }
    }
}

fn calculate_fan(shape: &[usize]) -> (usize, usize) {
    match shape.len() {
        0 | 1 => (1, 1),
        2 => (shape[0], shape[1]),
        _ => {
            let receptive_field: usize = shape[2..].iter().product();
            (shape[1] * receptive_field, shape[0] * receptive_field)
        }
    }
}

impl Nonlinearity {
    pub fn gain(&self) -> f32 {
        match self {
            Self::Linear | Self::Sigmoid | Self::Tanh => 1.0,
            Self::ReLU => (2.0_f32).sqrt(),
            Self::LeakyReLU(negative_slope) => {
                (2.0 / (1.0 + negative_slope * negative_slope)).sqrt()
            }
            Self::GELU | Self::SiLU => 1.0, // Approximation
        }
    }
}
```

### Checksum Verification
```rust
use sha2::{Sha256, Digest};

impl WeightRegistry {
    pub fn verify_checkpoint(&self, path: impl AsRef<Path>) -> EmbeddingResult<bool> {
        let path = path.as_ref();

        // Load checkpoint metadata
        let metadata_path = path.join("metadata.json");
        let metadata: serde_json::Value = serde_json::from_reader(
            std::fs::File::open(&metadata_path)?
        )?;

        let expected_checksum = metadata["checksum"]
            .as_str()
            .ok_or_else(|| EmbeddingError::CheckpointCorrupted("missing checksum".to_string()))?;

        // Compute actual checksum of weight files
        let mut hasher = Sha256::new();

        for entry in walkdir::WalkDir::new(path).into_iter().filter_map(|e| e.ok()) {
            if entry.path().extension().map_or(false, |e| e == "safetensors") {
                let data = std::fs::read(entry.path())?;
                hasher.update(&data);
            }
        }

        let computed = format!("sha256:{:x}", hasher.finalize());

        Ok(computed == expected_checksum)
    }
}
```

### Hot-Swap Implementation
```rust
impl WeightRegistry {
    pub async fn hot_swap(&self, new_weights: LoadedWeights) -> EmbeddingResult<()> {
        // Validate new weights first
        self.validate_weights(&new_weights)?;

        // Atomic swap
        {
            let mut cached = self.loaded_weights.write().await;

            // Old weights dropped here, freeing GPU memory
            let old_version = cached.as_ref().map(|w| w.version.version.clone());

            *cached = Some(new_weights);

            if let Some(old) = old_version {
                info!("Hot-swapped weights from {} to {}",
                    old, cached.as_ref().unwrap().version.version);
            }
        }

        // Trigger CUDA synchronization to ensure clean handoff
        cuda_runtime::device_synchronize()?;

        Ok(())
    }

    fn validate_weights(&self, weights: &LoadedWeights) -> EmbeddingResult<()> {
        // Check gating dimensions
        if weights.gating.linear_weight.shape() != &[8320, 8] {
            return Err(EmbeddingError::WeightDimensionMismatch {
                expected: vec![8320, 8],
                got: weights.gating.linear_weight.shape().to_vec(),
                layer: "gating".to_string(),
            });
        }

        // Check expert count
        if weights.experts.len() != 8 {
            return Err(EmbeddingError::InvalidExpertCount {
                expected: 8,
                got: weights.experts.len(),
            });
        }

        // Check output projection
        if weights.output.shape() != &[1536, 1536] {
            return Err(EmbeddingError::WeightDimensionMismatch {
                expected: vec![1536, 1536],
                got: weights.output.shape().to_vec(),
                layer: "output_projection".to_string(),
            });
        }

        Ok(())
    }
}
```

### Integration with FuseMoE
```rust
impl FuseMoE {
    pub async fn new(
        config: FusionConfig,
        weight_registry: Arc<WeightRegistry>,
    ) -> EmbeddingResult<Self> {
        // Load weights from registry
        let weights = weight_registry.load_weights().await?;

        let gating = GatingNetwork::from_weights(config.clone(), weights.gating.clone())?;
        let experts = ExpertPool::from_weights(config.clone(), weights.experts.clone())?;
        let output_proj = weights.output.clone();

        info!(
            "FuseMoE initialized with weights version: {}",
            weights.version.version
        );

        Ok(Self {
            config,
            gating,
            experts,
            output_proj,
            weight_registry,
            metrics: FuseMoEMetrics::default(),
        })
    }
}
```

---
*Task ID: M03-L31*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
